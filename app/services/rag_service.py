"""
RAG (Retrieval-Augmented Generation) Service

Implements the RAG pipeline following the architecture diagram:
1. User asks Question/Interview request
2. Query VectorDB for relevant context (Resume/Result)
3. Send Question + Context to LLM
4. Generate and stream Answer
"""

import logging
from typing import List, Dict, Any, Optional, AsyncIterator, Union
from .llm_service import LLMService
from .vllm_service import VLLMService
from .vectordb_service import VectorDBService
from app.prompts import (
    SYSTEM_GENERAL_CHAT,
    SYSTEM_RAG_CHAT,
    SYSTEM_FOLLOWUP,
    create_rag_prompt,
    create_followup_prompt,
)

logger = logging.getLogger(__name__)


class RAGService:
    """RAG Service for chatbot with VectorDB context retrieval"""

    def __init__(
        self, 
        llm_service: LLMService, 
        vectordb_service: VectorDBService,
        vllm_service: Optional[VLLMService] = None
    ):
        """
        Initialize RAG Service

        Args:
            llm_service: LLM service instance (Gemini)
            vectordb_service: VectorDB service instance
            vllm_service: vLLM service instance (optional)
        """
        self.llm = llm_service
        self.vllm = vllm_service
        self.vectordb = vectordb_service
        logger.info("RAG Service initialized")

    async def retrieve_all_documents(
        self,
        user_id: str,
        context_types: List[str] = ["resume", "job_posting"]
    ) -> str:
        """
        Retrieve ALL documents for a user (for analysis mode)

        Args:
            user_id: User ID
            context_types: List of collection types to retrieve

        Returns:
            Formatted context string with all documents (truncated if needed)
        """
        try:
            all_results = []

            # Get all documents from each collection type
            for collection_type in context_types:
                # Portfolio ì»¬ë ‰ì…˜ì€ user_id í•„í„° ì—†ì´ ê²€ìƒ‰í•˜ì§€ ì•ŠìŒ (ë¶„ì„ ì‹œì—ëŠ” ì‚¬ìš©ì ë°ì´í„°ë§Œ)
                if collection_type == "portfolio":
                    continue

                docs = await self.vectordb.get_all_documents_by_user(
                    user_id=user_id,
                    collection_type=collection_type
                )
                all_results.extend([(collection_type, doc) for doc in docs])

            # Format context
            if not all_results:
                return ""

            context_parts = []
            total_length = 0
            max_context_length = 4000  # ~1000 tokens (4 chars â‰ˆ 1 token)
            
            for collection_type, doc in all_results:
                source = {
                    "resume": "ì´ë ¥ì„œ",
                    "job_posting": "ì±„ìš©ê³µê³ ",
                    "portfolio": "í¬íŠ¸í´ë¦¬ì˜¤"
                }.get(collection_type, collection_type)

                doc_text = doc['text']
                
                # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (vLLM 8192 í† í° ì œí•œ ê³ ë ¤)
                if total_length + len(doc_text) > max_context_length:
                    remaining = max_context_length - total_length
                    if remaining > 100:  # ìµœì†Œ 100ìëŠ” í¬í•¨
                        doc_text = doc_text[:remaining] + "... (ìƒëµ)"
                        context_parts.append(f"[ì¶œì²˜: {source}]\n{doc_text}")
                    break
                
                context_parts.append(f"[ì¶œì²˜: {source}]\n{doc_text}")
                total_length += len(doc_text)

            result = "\n\n".join(context_parts)
            logger.info(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result)} ë¬¸ì (~{len(result)//4} í† í°)")
            return result

        except Exception as e:
            logger.error(f"Error retrieving all documents: {e}")
            return ""

    async def retrieve_context(
        self,
        query: str,
        user_id: str,
        context_types: List[str] = ["resume", "job_posting"],
        n_results: int = 3
    ) -> str:
        """
        Retrieve relevant context from VectorDB

        Args:
            query: User's query
            user_id: User ID for filtering
            context_types: List of collection types to search
            n_results: Number of results per collection

        Returns:
            Formatted context string
        """
        try:
            all_results = []

            # Query each collection type
            for collection_type in context_types:
                # Portfolio (ë©´ì ‘ ì§ˆë¬¸) ì»¬ë ‰ì…˜ì€ user_id í•„í„° ì—†ì´ ê²€ìƒ‰ (ê³µí†µ ë°ì´í„°)
                where_filter = None
                if collection_type != "portfolio" and user_id:
                    where_filter = {"user_id": user_id}
                
                results = await self.vectordb.query(
                    query_text=query,
                    collection_type=collection_type,
                    n_results=n_results,
                    where=where_filter
                )
                all_results.extend([(collection_type, r) for r in results])

            # Format context
            if not all_results:
                return ""

            context_parts = []
            for collection_type, result in all_results:
                source = {
                    "resume": "ì´ë ¥ì„œ",
                    "job_posting": "ì±„ìš©ê³µê³ ",
                    "portfolio": "í¬íŠ¸í´ë¦¬ì˜¤"
                }.get(collection_type, collection_type)

                context_parts.append(f"[ì¶œì²˜: {source}]\n{result['text']}")

            return "\n\n".join(context_parts)

        except Exception as e:
            logger.error(f"Error retrieving context: {e}")
            return ""

    async def chat_with_rag(
        self,
        user_message: str,
        user_id: str,
        history: Optional[List[Dict[str, str]]] = None,
        use_rag: bool = True,
        context_types: List[str] = ["resume", "job_posting"],
        model: str = "gemini",
        n_results: int = 1  # ê¸°ë³¸ê°’ì„ 1ë¡œ ì„¤ì •í•˜ì—¬ ì†ë„ ê°œì„ 
    ) -> AsyncIterator[str]:
        """
        Chat with RAG context retrieval

        Args:
            user_message: User's message
            user_id: User ID
            history: Chat history
            use_rag: Whether to use RAG (retrieve context)
            context_types: Collection types to search
            model: Model to use ("gemini" or "vllm")

        Yields:
            Response chunks
        """
        try:
            context = None

            # Retrieve context if RAG is enabled
            if use_rag:
                logger.info(f"Retrieving RAG context for user {user_id}")
                context = await self.retrieve_context(
                    query=user_message,
                    user_id=user_id,
                    context_types=context_types,
                    n_results=n_results
                )

                if context:
                    logger.info(f"Retrieved context length: {len(context)} characters")
                else:
                    logger.info("No context found, using general knowledge")

            # System prompt for job search assistant (from prompts module)
            system_prompt = SYSTEM_RAG_CHAT if context else SYSTEM_GENERAL_CHAT

            # Select model
            if model == "vllm" and self.vllm:
                logger.info("Using vLLM model")
                async for chunk in self.vllm.generate_response(
                    user_message=user_message,
                    context=context,
                    history=history,
                    system_prompt=system_prompt
                ):
                    yield chunk
            else:
                logger.info("Using Gemini model")
                async for chunk in self.llm.generate_response(
                    user_message=user_message,
                    context=context,
                    history=history,
                    system_prompt=system_prompt,
                    user_id=user_id,
                ):
                    yield chunk

        except Exception as e:
            logger.error(f"Error in RAG chat: {e}")
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def analyze_resume_and_posting(
        self,
        user_id: str,
        resume_id: Optional[str] = None,
        posting_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze resume and job posting match

        Args:
            user_id: User ID
            resume_id: Resume document ID (optional, will search if not provided)
            posting_id: Posting document ID (optional, will search if not provided)

        Returns:
            Analysis result
        """
        try:
            # Get resume text
            if resume_id:
                resume_doc = await self.vectordb.get_document(resume_id, "resume")
                resume_text = resume_doc['text'] if resume_doc else ""
            else:
                # Search for user's resume
                resume_results = await self.vectordb.query(
                    query_text="ì´ë ¥ì„œ ì „ì²´ ë‚´ìš©",
                    collection_type="resume",
                    n_results=1,
                    where={"user_id": user_id}
                )
                resume_text = resume_results[0]['text'] if resume_results else ""

            # Get posting text
            if posting_id:
                posting_doc = await self.vectordb.get_document(posting_id, "job_posting")
                posting_text = posting_doc['text'] if posting_doc else ""
            else:
                # Search for recent posting
                posting_results = await self.vectordb.query(
                    query_text="ì±„ìš©ê³µê³  ì „ì²´ ë‚´ìš©",
                    collection_type="job_posting",
                    n_results=1,
                    where={"user_id": user_id}
                )
                posting_text = posting_results[0]['text'] if posting_results else ""

            if not resume_text or not posting_text:
                raise ValueError("ì´ë ¥ì„œ ë˜ëŠ” ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # Generate analysis
            analysis = await self.llm.generate_analysis(resume_text, posting_text, user_id=user_id)
            return analysis

        except Exception as e:
            logger.error(f"Error analyzing resume and posting: {e}")
            raise

    async def generate_interview_question(
        self,
        user_id: str,
        interview_type: str = "technical"
    ) -> Dict[str, Any]:
        """
        Generate interview question based on user's resume and job posting

        Args:
            user_id: User ID
            interview_type: "technical" or "personality"

        Returns:
            Interview question
        """
        try:
            # Get resume
            resume_results = await self.vectordb.query(
                query_text="ì´ë ¥ì„œ ì „ì²´ ë‚´ìš©",
                collection_type="resume",
                n_results=1,
                where={"user_id": user_id}
            )
            resume_text = resume_results[0]['text'] if resume_results else ""

            # Get posting
            posting_results = await self.vectordb.query(
                query_text="ì±„ìš©ê³µê³  ì „ì²´ ë‚´ìš©",
                collection_type="job_posting",
                n_results=1,
                where={"user_id": user_id}
            )
            posting_text = posting_results[0]['text'] if posting_results else ""

            if not resume_text:
                resume_text = "ì •ë³´ ì—†ìŒ"

            if not posting_text:
                posting_text = "ì •ë³´ ì—†ìŒ"

            # Generate question
            question = await self.llm.generate_interview_question(
                resume_text, posting_text, interview_type, user_id=user_id
            )
            return question

        except Exception as e:
            logger.error(f"Error generating interview question: {e}")
            raise

    async def evaluate_interview_answer(
        self,
        question: str,
        answer: str,
        history: Optional[List[Dict[str, str]]] = None
    ) -> AsyncIterator[str]:
        """
        Evaluate interview answer and provide feedback

        Args:
            question: Interview question
            answer: User's answer
            history: Previous Q&A history

        Yields:
            Evaluation and feedback chunks
        """
        try:
            prompt = f"""ë©´ì ‘ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í‰ê°€í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€: {answer}

ë‹¤ìŒ í•­ëª©ì— ëŒ€í•´ í”¼ë“œë°±í•´ì£¼ì„¸ìš”:
1. ì¢‹ì€ ì  (good_points)
2. ê°œì„ í•  ì  (improvements)
3. ëª¨ë²” ë‹µì•ˆ ì˜ˆì‹œ (example_answer)

ì¹œì ˆí•˜ê³  ê±´ì„¤ì ìœ¼ë¡œ í”¼ë“œë°±í•´ì£¼ì„¸ìš”."""

            system_prompt = "ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹µë³€ì„ ë¶„ì„í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”."

            async for chunk in self.llm.generate_response(
                user_message=prompt,
                context=None,
                history=history,
                system_prompt=system_prompt
            ):
                yield chunk

        except Exception as e:
            logger.error(f"Error evaluating interview answer: {e}")
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def generate_followup_question(
        self,
        original_question: str,
        candidate_answer: str,
        star_analysis: Optional[Dict[str, str]] = None,
        model: str = "gemini",
        user_id: Optional[str] = None
    ) -> AsyncIterator[str]:
        """
        ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± (STAR ë¶„ì„ ê¸°ë°˜)
        
        Args:
            original_question: ì›ë³¸ ë©´ì ‘ ì§ˆë¬¸
            candidate_answer: ì§€ì›ì ë‹µë³€
            star_analysis: STAR ë¶„ì„ ê²°ê³¼ (Optional)
            model: ì‚¬ìš©í•  ëª¨ë¸ ("gemini" ë˜ëŠ” "vllm")
            user_id: ì‚¬ìš©ì ID (Gemini ì‚¬ìš© ì‹œ)
        
        Yields:
            ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ìŠ¤íŠ¸ë¦¬ë° ì²­í¬
        """
        try:
            # STAR ë¶„ì„ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            if star_analysis is None:
                star_analysis = {
                    "situation": "unknown",
                    "task": "unknown",
                    "action": "unknown",
                    "result": "unknown"
                }
            
            # ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± í”„ë¡¬í”„íŠ¸ (prompts ëª¨ë“ˆ ì‚¬ìš©)
            followup_prompt = create_followup_prompt(
                original_question=original_question,
                candidate_answer=candidate_answer,
                star_analysis=star_analysis
            )
            
            logger.info(f"ğŸ” [ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±] ì‹œì‘")
            logger.info(f"   ì›ë³¸ ì§ˆë¬¸: {original_question[:50]}...")
            logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(candidate_answer)}ì")
            logger.info(f"   ëª¨ë¸: {model}")
            
            # vLLM ë˜ëŠ” Gemini ì„ íƒ
            if model == "vllm" and self.vllm:
                logger.info(f"ğŸ’¬ [vLLM] ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì‹œì‘")
                async for chunk in self.vllm.generate_response(
                    user_message=followup_prompt,
                    context=None,
                    history=[],
                    system_prompt=SYSTEM_FOLLOWUP
                ):
                    yield chunk
            else:
                logger.info(f"ğŸ’¬ [Gemini] ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì‹œì‘")
                async for chunk in self.llm.generate_response(
                    user_message=followup_prompt,
                    context=None,
                    history=[],
                    system_prompt=SYSTEM_FOLLOWUP,
                    user_id=user_id,
                ):
                    yield chunk
                    
        except Exception as e:
            logger.error(f"Error generating followup question: {e}")
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
