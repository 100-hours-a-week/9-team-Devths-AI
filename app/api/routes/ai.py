import asyncio
import json
import logging
import os
import uuid
from datetime import datetime

from fastapi import APIRouter, Depends, Header, HTTPException, status
from fastapi.responses import StreamingResponse

from app.schemas.calendar import CalendarParseRequest, CalendarParseResponse
from app.schemas.chat import (
    AnalysisResult,
    ChatMode,
    ChatRequest,
    InterviewReport,
    MatchingResult,
    PostingAnalysis,
    QAEvaluation,
    ResumeAnalysis,
)
from app.schemas.common import (
    AsyncTaskResponse,
    ErrorCode,
    TaskStatus,
    TaskStatusResponse,
)
from app.schemas.text_extract import PageText, TextExtractRequest, TextExtractResult
from app.services.llm_service import LLMService
from app.services.rag_service import RAGService
from app.services.vectordb_service import VectorDBService
from app.services.vllm_service import VLLMService

logger = logging.getLogger(__name__)


def sanitize_for_log(value: str, max_length: int = 100) -> str:
    """
    Sanitize user input for safe logging to prevent log injection attacks.

    Args:
        value: The string value to sanitize
        max_length: Maximum length to truncate to

    Returns:
        Sanitized string safe for logging
    """
    if not value:
        return ""

    # Remove control characters and newlines that could be used for log injection
    sanitized = "".join(
        char if char.isprintable() and char not in "\n\r" else " " for char in value
    )

    # Truncate if too long
    if len(sanitized) > max_length:
        sanitized = sanitized[:max_length] + "..."

    return sanitized


router = APIRouter(
    prefix="/ai",
    tags=["AI APIs (v3.0)"],
    responses={404: {"description": "Not found"}},
)

# ì„ì‹œ ì‘ì—… ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” Redis ë“± ì‚¬ìš©)
tasks_db = {}

# Initialize services
llm_service = None
vllm_service = None
vectordb_service = None
rag_service = None


def get_services():
    """Get or initialize AI services"""
    global llm_service, vllm_service, vectordb_service, rag_service

    if llm_service is None:
        api_key = os.getenv("GOOGLE_API_KEY")
        llm_service = LLMService(api_key=api_key)
        vectordb_service = VectorDBService(api_key=api_key)

        # Initialize vLLM service (GCP GPU server)
        gcp_vllm_url = os.getenv("GCP_VLLM_BASE_URL")

        try:
            if gcp_vllm_url:
                logger.info(f"ğŸŒ GCP vLLM ì„œë²„ ì—°ê²°: {gcp_vllm_url}")
                vllm_service = VLLMService()
                logger.info("âœ… vLLM service initialized (GCP GPU server)")
            else:
                # GCP URL ì—†ìœ¼ë©´ OCR ì „ìš© ëª¨ë“œ
                logger.info("ğŸ’° GCP URL ì—†ìŒ - OCR ì „ìš© ëª¨ë“œë¡œ ì´ˆê¸°í™”")
                vllm_service = VLLMService(ocr_only=True)
                logger.info("âœ… vLLM service initialized (OCR-only mode)")
        except Exception as e:
            logger.warning(f"vLLM service initialization failed: {e}")
            vllm_service = None

        rag_service = RAGService(llm_service, vectordb_service, vllm_service)

    return rag_service


async def verify_api_key(x_api_key: str | None = Header(None)):
    """API í‚¤ ê²€ì¦"""
    # ì‹¤ì œë¡œëŠ” í™˜ê²½ë³€ìˆ˜ë‚˜ DBì—ì„œ í™•ì¸
    if x_api_key != "your-api-key-here":
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid API key")
    return x_api_key


# ============================================================================
# API 1: í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© (í†µí•©) (ë¹„ë™ê¸°)
# ============================================================================


@router.post(
    "/text/extract",
    response_model=AsyncTaskResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© ì €ì¥ (í†µí•©)",
    description="""
    íŒŒì¼ ë˜ëŠ” í…ìŠ¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë‚´ë¶€ì—ì„œ ì„ë² ë”©ê¹Œì§€ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    **v3.0 ë³€ê²½ì‚¬í•­:**
    - `/ai/ocr/extract` + `/ai/file/embed` í†µí•©
    - VectorDB ì •ë³´ ì œê±° (AI ì„œë²„ ë‚´ë¶€ì—ì„œë§Œ ì‚¬ìš©)
    - ë°±ì—”ë“œì—ëŠ” extracted_textë§Œ ë°˜í™˜

    **ì²˜ë¦¬ ë°©ì‹:** ë¹„ë™ê¸° (task_id ë°˜í™˜ â†’ í´ë§ í•„ìš”)

    **ë‚´ë¶€ ì²˜ë¦¬ íë¦„:**
    1. íŒŒì¼ ì…ë ¥ ì‹œ: OCR/VLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    2. í…ìŠ¤íŠ¸ ì…ë ¥ ì‹œ: ê·¸ëŒ€ë¡œ ì‚¬ìš©
    3. í…ìŠ¤íŠ¸ ì²­í‚¹ (500 tokens, 50 overlap)
    4. Gemini Embedding ìƒì„±
    5. VectorDBì— ì €ì¥ (typeì— ë”°ë¥¸ ì»¬ë ‰ì…˜)
    6. Backendì— extracted_textë§Œ ë°˜í™˜
    """,
    dependencies=[Depends(verify_api_key)],
)
async def text_extract(request: TextExtractRequest):
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© ì €ì¥ (í†µí•©)"""
    task_id = f"task_{uuid.uuid4().hex[:12]}"

    # ë¹„ë™ê¸° ì‘ì—… ì‹œì‘
    tasks_db[task_id] = {
        "status": TaskStatus.PROCESSING,
        "created_at": datetime.now(),
        "request": request.model_dump(),
    }

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
    async def process_text_extract():
        try:
            rag = get_services()

            # ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ê°’: gemini)
            model = request.model if hasattr(request, "model") and request.model else "gemini"
            logger.info("")
            logger.info(f"{'='*80}")
            logger.info("=== ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘ (íŒŒì¼ ì—…ë¡œë“œ) ===")
            logger.info(f"{'='*80}")
            logger.info(f"ğŸ“Œ ìš”ì²­ ëª¨ë¸: {sanitize_for_log(model, 20).upper()}")
            logger.info(f"ğŸ“Œ ë¬¸ì„œ íƒ€ì…: {sanitize_for_log(request.type, 50)}")
            logger.info(f"ğŸ“Œ ì‚¬ìš©ì ID: {sanitize_for_log(request.user_id, 50)}")
            logger.info(f"ğŸ“Œ ë¬¸ì„œ ID: {sanitize_for_log(str(request.document_id), 50)}")
            logger.info(f"ğŸ“Œ vLLM ì„œë¹„ìŠ¤: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if rag.vllm else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")

            # íŒŒì¼ URLì´ ìˆìœ¼ë©´ OCR ì²˜ë¦¬
            if request.file_url:
                file_type = request.file_type if hasattr(request, "file_type") else "pdf"
                logger.info(f"ğŸ“Œ íŒŒì¼ íƒ€ì…: {file_type}")
                logger.info("")

                # vLLM ëª¨ë“œ: pytesseract OCR ì‚¬ìš© (ê°€ì„±ë¹„)
                if model == "vllm" and rag.vllm:
                    logger.info("ğŸ’° [vLLM ê°€ì„±ë¹„ ëª¨ë“œ] pytesseract OCR ì‹œì‘")
                    logger.info("   â†’ ë¹„ìš© ì ˆê°ì„ ìœ„í•´ pytesseract ì‚¬ìš© (Gemini Vision API ëŒ€ì‹ )")
                    ocr_result = await rag.vllm.extract_text_from_file(
                        file_url=str(request.file_url), file_type=file_type
                    )
                    extracted_text = ocr_result["extracted_text"]
                    pages = [PageText(**page) for page in ocr_result["pages"]]
                    logger.info(
                        f"âœ… [vLLM OCR] ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text)}ì (í˜ì´ì§€: {len(pages)})"
                    )

                # Gemini ëª¨ë“œ: Gemini Vision API ì‚¬ìš© (ê³ ì„±ëŠ¥)
                else:
                    if model == "vllm" and not rag.vllm:
                        logger.warning("âš ï¸ vLLM ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€ â†’ Geminië¡œ ìë™ ë³€ê²½")
                    logger.info("ğŸš€ [Gemini ê³ ì„±ëŠ¥ ëª¨ë“œ] Gemini Vision API OCR ì‹œì‘")
                    logger.info("   â†’ ê³ í’ˆì§ˆ OCRì„ ìœ„í•´ Gemini Vision API ì‚¬ìš©")
                    ocr_result = await rag.llm.extract_text_from_file(
                        file_url=str(request.file_url), file_type=file_type
                    )
                    extracted_text = ocr_result["extracted_text"]
                    pages = [PageText(**page) for page in ocr_result["pages"]]
                    logger.info(
                        f"âœ… [Gemini OCR] ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text)}ì (í˜ì´ì§€: {len(pages)})"
                    )

            # í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥
            else:
                extracted_text = request.text or ""
                pages = None
                logger.info(f"í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥: {len(extracted_text)} characters")

            # VectorDBì— ì„ë² ë”© ì €ì¥
            if extracted_text:
                document_id = request.document_id or f"doc_{uuid.uuid4().hex[:12]}"

                await rag.vectordb.add_document(
                    document_id=document_id,
                    text=extracted_text,
                    collection_type=request.type,
                    metadata={"user_id": request.user_id, "created_at": datetime.now().isoformat()},
                )

                tasks_db[task_id]["status"] = TaskStatus.COMPLETED
                tasks_db[task_id]["result"] = TextExtractResult(
                    success=True, extracted_text=extracted_text, pages=pages
                ).model_dump()
            else:
                raise ValueError("No text extracted")

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            tasks_db[task_id]["status"] = TaskStatus.FAILED
            tasks_db[task_id]["error"] = {"code": ErrorCode.PROCESSING_ERROR, "message": str(e)}

    asyncio.create_task(process_text_extract())

    return AsyncTaskResponse(task_id=task_id, status=TaskStatus.PROCESSING)


# ============================================================================
# ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ
# ============================================================================


@router.get(
    "/task/{task_id}",
    response_model=TaskStatusResponse,
    summary="ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ",
    description="ë¹„ë™ê¸° ì²˜ë¦¬ ì‘ì—…ì˜ ìƒíƒœë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
    dependencies=[Depends(verify_api_key)],
)
async def get_task_status(task_id: str):
    """ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    if task_id not in tasks_db:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail={"code": ErrorCode.TASK_NOT_FOUND, "message": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
        )

    task = tasks_db[task_id]

    return TaskStatusResponse(
        task_id=task_id,
        status=task["status"],
        progress=task.get("progress"),
        message=task.get("message"),
        result=task.get("result"),
        error=task.get("error"),
    )


# ============================================================================
# API 2: ì±„íŒ… (í†µí•©: ëŒ€í™”/ë¶„ì„/ë©´ì ‘) (ìŠ¤íŠ¸ë¦¬ë°)
# ============================================================================


async def generate_chat_stream(request: ChatRequest):
    """ì±„íŒ… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""

    mode = request.context.mode
    rag = get_services()
    newline = "\n"
    sse_end = "\n\n"

    # ëª¨ë¸ ì„ íƒ (gemini ë˜ëŠ” vllm)
    model = request.model.value if hasattr(request.model, "value") else str(request.model)
    logger.info("")
    logger.info(f"{'='*80}")
    logger.info("=== ğŸ’¬ ì±„íŒ… ìš”ì²­ ì‹œì‘ ===")
    logger.info(f"{'='*80}")
    logger.info(f"ğŸ“Œ ìš”ì²­ ëª¨ë¸: {sanitize_for_log(model, 20).upper()}")
    logger.info(f"ğŸ“Œ ì±„íŒ… ëª¨ë“œ: {sanitize_for_log(mode, 50)}")
    logger.info(f"ğŸ“Œ ì‚¬ìš©ì ID: {sanitize_for_log(request.user_id, 50)}")
    logger.info(f"ğŸ“Œ ì±„íŒ…ë°© ID: {sanitize_for_log(str(request.room_id), 50)}")
    logger.info(f"ğŸ“Œ vLLM ì„œë¹„ìŠ¤: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if rag.vllm else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
    logger.info("")

    # 1. ì¼ë°˜ ëŒ€í™” (RAG í™œìš©)
    if mode == ChatMode.GENERAL:
        full_response = ""

        try:
            # Convert ChatMessage list to dict list for service compatibility
            history_dict = [
                {
                    "role": msg.role.value if hasattr(msg.role, "value") else str(msg.role),
                    "content": msg.content,
                }
                for msg in request.history
            ]

            # Determine if this is an analysis request
            user_message = request.message or ""
            is_analysis = any(
                keyword in user_message for keyword in ["ë¶„ì„", "ë§¤ì¹­", "ì í•©", "í‰ê°€", "ë¹„êµ"]
            )

            if is_analysis:
                # ===================================================================
                # ë¶„ì„ ìš”ì²­: vLLMê³¼ Gemini ì™„ì „ ë¶„ë¦¬
                # ===================================================================
                logger.info(f"ğŸ” ë¶„ì„ ìš”ì²­ ê°ì§€: '{sanitize_for_log(user_message, 50)}'")
                logger.info("")

                # ---------------------------------------------------------------
                # vLLM ëª¨ë“œ (ê°€ì„±ë¹„): OCR(pytesseract) â†’ VectorDB â†’ Llama ë¶„ì„
                # ---------------------------------------------------------------
                if model == "vllm" and rag.vllm:
                    logger.info("ğŸ’° [vLLM ê°€ì„±ë¹„ ëª¨ë“œ] ë¶„ì„ ì‹œì‘")
                    logger.info(
                        "   í”„ë¡œì„¸ìŠ¤: pytesseract OCR â†’ VectorDB ì €ì¥ â†’ VectorDB ì¡°íšŒ â†’ Llama ë¶„ì„"
                    )
                    logger.info("")

                    # 1. VectorDBì—ì„œ OCRë¡œ ì¶”ì¶œëœ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                    logger.info("ğŸ“‚ [1/3] VectorDBì—ì„œ ì—…ë¡œë“œëœ ë¬¸ì„œ ì¡°íšŒ ì¤‘...")
                    full_context = await rag.retrieve_all_documents(
                        user_id=request.user_id, context_types=["resume", "job_posting"]
                    )

                    if not full_context:
                        error_msg = "âŒ ì—…ë¡œë“œëœ ì´ë ¥ì„œ ë˜ëŠ” ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\në¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                        logger.error(
                            f"âš ï¸ VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (user_id: {sanitize_for_log(request.user_id, 50)})"
                        )
                        yield f"data: {json.dumps({'type': 'chunk', 'content': error_msg}, ensure_ascii=False)}{sse_end}"
                        full_response = error_msg
                    else:
                        logger.info(f"âœ… [1/3] VectorDB ì¡°íšŒ ì™„ë£Œ: {len(full_context)}ì")
                        logger.info("")

                        # 2. Llama ëª¨ë¸ë¡œ ë¶„ì„
                        logger.info("ğŸ¤– [2/3] Llama ëª¨ë¸ ë¶„ì„ ì‹œì‘...")
                        analysis_prompt = f"""ë‹¤ìŒ ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ë¥¼ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:

{full_context}

ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. **ì í•©ë„ í‰ê°€**: ì§€ì›ìê°€ ì±„ìš©ê³µê³  ìš”êµ¬ì‚¬í•­ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ì§€
2. **ê°•ì **: ì§€ì›ìì˜ ë›°ì–´ë‚œ ì—­ëŸ‰ê³¼ ê²½í—˜
3. **ì•½ì **: ë¶€ì¡±í•œ ë¶€ë¶„ì´ë‚˜ ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­
4. **ì˜ˆìƒ ë©´ì ‘ ì§ˆë¬¸ (ì¸ì„± 3ê°œ, ê¸°ìˆ  3ê°œ)**

ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”."""

                        async for chunk in rag.vllm.generate_response(
                            user_message=analysis_prompt,
                            context=None,
                            history=[],
                            system_prompt="ë‹¹ì‹ ì€ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ë¥¼ ë¶„ì„í•˜ì—¬ ëª…í™•í•œ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”.",
                        ):
                            full_response += chunk
                            yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"

                        logger.info(f"âœ… [3/3] Llama ë¶„ì„ ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")

                # ---------------------------------------------------------------
                # Gemini ëª¨ë“œ (ê³ ì„±ëŠ¥): Gemini Vision APIë¡œ ì§ì ‘ íŒŒì¼ ì½ê³  ë¶„ì„
                # ---------------------------------------------------------------
                else:
                    if model == "vllm" and not rag.vllm:
                        logger.warning("âš ï¸ vLLM ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€ â†’ Geminië¡œ ìë™ ë³€ê²½")

                    logger.info("ğŸš€ [Gemini ê³ ì„±ëŠ¥ ëª¨ë“œ] ë¶„ì„ ì‹œì‘")
                    logger.info("   í”„ë¡œì„¸ìŠ¤: RAG ê²€ìƒ‰ â†’ Gemini ë¶„ì„ (ì›ë˜ ë°©ì‹)")
                    logger.info("")

                    # RAGë¡œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰í•˜ì—¬ Geminië¡œ ë¶„ì„
                    logger.info("ğŸ“‚ [1/2] RAG ê²€ìƒ‰ ì¤‘...")
                    async for chunk in rag.chat_with_rag(
                        user_message=user_message,
                        user_id=request.user_id,
                        history=history_dict,
                        use_rag=True,
                        context_types=["resume", "job_posting"],
                        model="gemini",
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"

                    logger.info(f"âœ… [2/2] Gemini ë¶„ì„ ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")
            else:
                # ===================================================================
                # ì¼ë°˜ ëŒ€í™”: RAG ê²€ìƒ‰ ì‚¬ìš©
                # ===================================================================
                logger.info("ğŸ’¬ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ")
                logger.info("")

                if (
                    "ë©´ì ‘ ì§ˆë¬¸" in user_message
                    or "ë©´ì ‘ì§ˆë¬¸" in user_message
                    or "ë©´ì ‘" in user_message
                ):
                    # ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­ ì‹œ portfolio(ë©´ì ‘ ì§ˆë¬¸ ë°ì´í„°)ë§Œ ê²€ìƒ‰
                    context_types = ["portfolio"]
                    logger.info("ğŸ¯ ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­ ê°ì§€ â†’ portfolio ì»¬ë ‰ì…˜ë§Œ ê²€ìƒ‰")
                else:
                    # ì¼ë°˜ ìš”ì²­ ì‹œ ëª¨ë“  ì»¬ë ‰ì…˜ ê²€ìƒ‰
                    context_types = ["resume", "job_posting", "portfolio"]
                    logger.info("ğŸ“š ì¼ë°˜ ëŒ€í™” â†’ ëª¨ë“  ì»¬ë ‰ì…˜ ê²€ìƒ‰")

                logger.info("")

                # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±
                logger.info(
                    f"ğŸ” [{sanitize_for_log(model, 20).upper()}] RAG ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì‹œì‘..."
                )
                async for chunk in rag.chat_with_rag(
                    user_message=user_message,
                    user_id=request.user_id,
                    history=history_dict,
                    use_rag=True,  # RAG í™œì„±í™”
                    context_types=context_types,
                    model=model,
                ):
                    full_response += chunk
                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"

                logger.info(
                    f"âœ… [{sanitize_for_log(model, 20).upper()}] ì¼ë°˜ ëŒ€í™” ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)"
                )

        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            yield f"data: {json.dumps({'type': 'chunk', 'content': error_msg}, ensure_ascii=False)}{sse_end}"
            full_response = error_msg

        result = {
            "success": True,
            "mode": "general",
            "response": full_response,
            "tool_used": {"tool": "RAG", "description": "VectorDB ê²€ìƒ‰ í›„ LLM ì‘ë‹µ ìƒì„±"},
        }
        yield f"data: {json.dumps({'type': 'complete', 'data': result}, ensure_ascii=False)}{sse_end}"

    # 2. ë¶„ì„ ëª¨ë“œ (RAG ì‚¬ìš©)
    elif mode == ChatMode.ANALYSIS:
        try:
            content1 = f"ì´ë ¥ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...{newline}"
            yield f"data: {json.dumps({'type': 'chunk', 'content': content1}, ensure_ascii=False)}{sse_end}"
            await asyncio.sleep(0.3)

            content2 = f"ì±„ìš©ê³µê³ ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...{newline}"
            yield f"data: {json.dumps({'type': 'chunk', 'content': content2}, ensure_ascii=False)}{sse_end}"
            await asyncio.sleep(0.3)

            content3 = f"ë§¤ì¹­ë„ë¥¼ ê³„ì‚° ì¤‘ì…ë‹ˆë‹¤...{newline}"
            yield f"data: {json.dumps({'type': 'chunk', 'content': content3}, ensure_ascii=False)}{sse_end}"

            # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰
            analysis_result = await rag.analyze_resume_and_posting(
                user_id=request.user_id,
                resume_id=request.context.resume_id,
                posting_id=request.context.posting_id,
            )

            # Convert to Pydantic models
            analysis = AnalysisResult(
                resume_analysis=ResumeAnalysis(**analysis_result.get("resume_analysis", {})),
                posting_analysis=PostingAnalysis(**analysis_result.get("posting_analysis", {})),
                matching=MatchingResult(**analysis_result.get("matching", {})),
            )

            result = {"success": True, "mode": "analysis", "analysis": analysis.model_dump()}
            yield f"data: {json.dumps({'type': 'complete', 'data': result}, ensure_ascii=False)}{sse_end}"

        except Exception as e:
            error_result = {"success": False, "mode": "analysis", "error": str(e)}
            yield f"data: {json.dumps({'type': 'complete', 'data': error_result}, ensure_ascii=False)}{sse_end}"

    # 3. ë©´ì ‘ ëª¨ë“œ - ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„± ë° ëŒ€í™”
    elif mode == ChatMode.INTERVIEW_QUESTION:
        try:
            # ë©´ì ‘ íƒ€ì…ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì¡°ì •
            interview_type = request.context.interview_type or "technical"
            interview_type_kr = "ê¸°ìˆ " if interview_type == "technical" else "ì¸ì„±"

            content = f"{interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...{newline}"
            yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}{sse_end}"

            # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ë§ì¶¤ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
            # resume, portfolio, job_posting ì»¬ë ‰ì…˜ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            context = await rag.retrieve_context(
                query=f"{interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ì„ ìœ„í•œ ì‚¬ìš©ì ì •ë³´",
                user_id=request.user_id,
                context_types=["resume", "portfolio", "job_posting"],
                n_results=1,  # ì†ë„ ê°œì„ ì„ ìœ„í•´ 1ê°œë§Œ ê²€ìƒ‰
            )

            # ë©´ì ‘ ì§ˆë¬¸ ìƒì„± í”„ë¡¬í”„íŠ¸ (ê°„ì†Œí™”)
            if context:
                question_prompt = f"{interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ 1ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\n\nì°¸ê³ ì •ë³´:\n{context}\n\nì§ˆë¬¸ë§Œ ê°„ë‹¨íˆ ì¶œë ¥í•˜ì„¸ìš”:"
            else:
                question_prompt = f"ì¼ë°˜ì ì¸ {interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ 1ê°œë¥¼ ì§§ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”:"

            full_question = ""
            async for chunk in rag.llm.generate_response(
                user_message=question_prompt,
                context=None,
                history=[],
                system_prompt="ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì§ˆë¬¸ë§Œ ìƒì„±í•˜ì„¸ìš”.",
            ):
                full_question += chunk
                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"

            result = {
                "success": True,
                "mode": "interview_question",
                "response": full_question.strip(),
                "interview_type": interview_type,
            }
            yield f"data: {json.dumps({'type': 'complete', 'data': result}, ensure_ascii=False)}{sse_end}"

        except Exception as e:
            logger.error(f"Interview question generation error: {e}")
            error_result = {"success": False, "mode": "interview_question", "error": str(e)}
            yield f"data: {json.dumps({'type': 'complete', 'data': error_result}, ensure_ascii=False)}{sse_end}"

    # 4. ë©´ì ‘ ë¦¬í¬íŠ¸ (Pydantic AI ì‚¬ìš©)
    elif mode == ChatMode.INTERVIEW_REPORT:
        chunks = [
            f"ë©´ì ‘ ë‹µë³€ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...{newline}",
            f"ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤...{newline}",
        ]

        for chunk in chunks:
            yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"
            await asyncio.sleep(0.5)

        # Pydantic AI ëª¨ë¸ë¡œ êµ¬ì¡°í™”ëœ ë¦¬í¬íŠ¸
        report = InterviewReport(
            evaluations=[
                QAEvaluation(
                    question="Reactì˜ Virtual DOMì´ ë¬´ì—‡ì¸ê°€ìš”?",
                    answer="ì‹¤ì œ DOMê³¼ ë¹„êµí•´ì„œ ë³€ê²½ëœ ë¶€ë¶„ë§Œ ì—…ë°ì´íŠ¸í•˜ëŠ” ê±°ì˜ˆìš”",
                    good_points=["Virtual DOMì˜ ê¸°ë³¸ ê°œë…ì„ ì˜ ì´í•´í•˜ê³  ìˆìŒ"],
                    improvements=["Reconciliation ì•Œê³ ë¦¬ì¦˜ ì„¤ëª… ì¶”ê°€í•˜ë©´ ì¢‹ìŒ"],
                )
            ],
            strength_patterns=["ê¸°ìˆ  ê°œë…ì— ëŒ€í•œ ì´í•´ë„ê°€ ë†’ìŒ"],
            weakness_patterns=["ì‹¬í™” ê°œë… ì„¤ëª…ì´ ë¶€ì¡±í•¨"],
            learning_guide=["React ì‹¬í™” ê°œë… í•™ìŠµ (Fiber, Concurrent Mode)"],
        )

        result = {"success": True, "mode": "interview_report", "report": report.model_dump()}
        yield f"data: {json.dumps({'type': 'complete', 'data': result}, ensure_ascii=False)}{sse_end}"


@router.post(
    "/chat",
    summary="ì±„íŒ… ì²˜ë¦¬ (í†µí•©: ëŒ€í™”/ë¶„ì„/ë©´ì ‘)",
    description="""
    ëª¨ë“  LLM ì‘ë‹µì„ í†µí•© ì²˜ë¦¬í•©ë‹ˆë‹¤.

    **v3.0 ë³€ê²½ì‚¬í•­:**
    - `/ai/analyze`, `/ai/interview/*` í†µí•©
    - context.modeë¡œ ê¸°ëŠ¥ êµ¬ë¶„
    - Pydantic AIë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥

    **ì²˜ë¦¬ ë°©ì‹:** ìŠ¤íŠ¸ë¦¬ë° (SSE)

    **ëª¨ë“œ:**
    - general: ì¼ë°˜ ëŒ€í™”
    - analysis: ì´ë ¥ì„œ/ì±„ìš©ê³µê³  ë¶„ì„
    - interview_question: ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
    - interview_report: ë©´ì ‘ ë¦¬í¬íŠ¸

    **Pydantic AI ì‚¬ìš©:**
    - analysis: AnalysisResult
    - interview_question: InterviewQuestion
    - interview_report: InterviewReport
    """,
    dependencies=[Depends(verify_api_key)],
)
async def chat(request: ChatRequest):
    """ì±„íŒ… ì²˜ë¦¬ (í†µí•©)"""
    return StreamingResponse(generate_chat_stream(request), media_type="text/event-stream")


# ============================================================================
# API 3: ìº˜ë¦°ë” ì¼ì • íŒŒì‹± (ë™ê¸°)
# ============================================================================


@router.post(
    "/calendar/parse",
    response_model=CalendarParseResponse,
    summary="ìº˜ë¦°ë” ì¼ì • ì •ë³´ íŒŒì‹±",
    description="""
    ìº˜ë¦°ë” ëª¨ë‹¬ì—ì„œ íŒŒì¼/í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (í¼ ìë™ ì±„ìš°ê¸°ìš©).

    **ì²˜ë¦¬ ë°©ì‹:** ë™ê¸° - ê°„ë‹¨í•œ íŒŒì‹± ì‘ì—…

    **Pydantic AI ì‚¬ìš©:** CalendarParseResult

    **ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
    - ëª¨ë‹¬ì—ì„œ ì±„ìš©ê³µê³  íŒŒì¼/í…ìŠ¤íŠ¸ ì²¨ë¶€ â†’ ì¼ì • ì •ë³´ ì¶”ì¶œ
    - Frontendê°€ ëª¨ë‹¬ í¼ì— ìë™ ì±„ì›Œë„£ìŒ
    - ì‚¬ìš©ì í™•ì¸/ìˆ˜ì • â†’ ì €ì¥ â†’ Backendê°€ Google Calendarì— ì¶”ê°€
    """,
    dependencies=[Depends(verify_api_key)],
)
async def calendar_parse(request: CalendarParseRequest):
    """ìº˜ë¦°ë” ì¼ì • íŒŒì‹±"""
    if not request.file_url and not request.text:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "code": ErrorCode.INVALID_REQUEST,
                "message": "file_url ë˜ëŠ” text ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.",
            },
        )

    # Pydantic AIë¡œ êµ¬ì¡°í™”ëœ ê²°ê³¼ ë°˜í™˜
    return CalendarParseResponse(
        success=True,
        company="ì¹´ì¹´ì˜¤",
        position="ë°±ì—”ë“œ ê°œë°œì",
        schedules=[
            {"stage": "ì„œë¥˜ ë§ˆê°", "date": "2026-01-15", "time": None},
            {"stage": "ì½”ë”©í…ŒìŠ¤íŠ¸", "date": "2026-01-20", "time": "14:00"},
            {"stage": "1ì°¨ ë©´ì ‘", "date": "2026-01-25", "time": None},
        ],
        hashtags=["#ì¹´ì¹´ì˜¤", "#ë°±ì—”ë“œ", "#ì‹ ì…"],
    )


# ============================================================================
# API 4: ê²Œì‹œíŒ ì²¨ë¶€íŒŒì¼ ë§ˆìŠ¤í‚¹ (ë¹„ë™ê¸°)
# ============================================================================
# ì´ APIëŠ” app/api/routes/masking.pyë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.
# masking.pyì—ì„œ íŒŒì¼ ê¸°ë°˜ ì €ì¥ì†Œì™€ ì‹¤ì œ Gemini APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
