import asyncio
import json
import logging
import os
import uuid
from datetime import datetime

from fastapi import APIRouter, Header, HTTPException, status
from fastapi.responses import StreamingResponse

from app.prompts import (
    SYSTEM_INTERVIEW,
    create_interview_question_prompt,
)
from app.schemas.calendar import CalendarParseRequest, CalendarParseResponse
from app.schemas.chat import (
    AnalysisResult,
    ChatMode,
    ChatRequest,
    MatchingResult,
    PostingAnalysis,
    ResumeAnalysis,
)
from app.schemas.common import AsyncTaskResponse, ErrorCode, TaskStatus, TaskStatusResponse
from app.schemas.text_extract import (
    DocumentExtractResult,
    DocumentInput,
    PageText,
    TextExtractRequest,
    TextExtractResult,
)
from app.services.llm_service import LLMService
from app.services.rag_service import RAGService
from app.services.vectordb_service import VectorDBService
from app.services.vllm_service import VLLMService
from app.utils.log_sanitizer import sanitize_log_input
from app.utils.task_store import get_task_store

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/ai",
    tags=["AI APIs (v3.0)"],
    responses={404: {"description": "Not found"}},
)

# í†µí•© ì‘ì—… ì €ì¥ì†Œ (íŒŒì¼ ê¸°ë°˜, ì„œë²„ ì¬ì‹œì‘ ì‹œì—ë„ ìœ ì§€)
# ë°±ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ task_idë¥¼ í‚¤ë¡œ ì‚¬ìš©
task_store = get_task_store()


# Initialize services
llm_service = None
vllm_service = None
vectordb_service = None
rag_service = None


def get_services():
    """Get or initialize AI services"""
    global llm_service, vllm_service, vectordb_service, rag_service

    if llm_service is None:
        api_key = os.getenv("GOOGLE_API_KEY")
        llm_service = LLMService(api_key=api_key)
        vectordb_service = VectorDBService(api_key=api_key)

        # Initialize vLLM service (GCP GPU server)
        gcp_vllm_url = os.getenv("GCP_VLLM_BASE_URL")

        try:
            if gcp_vllm_url:
                logger.info(f"ğŸŒ GCP vLLM ì„œë²„ ì—°ê²°: {gcp_vllm_url}")
                vllm_service = VLLMService()
                logger.info("âœ… vLLM service initialized (GCP GPU server)")
            else:
                # GCP URL ì—†ìœ¼ë©´ OCR ì „ìš© ëª¨ë“œ
                logger.info("ğŸ’° GCP URL ì—†ìŒ - OCR ì „ìš© ëª¨ë“œë¡œ ì´ˆê¸°í™”")
                vllm_service = VLLMService(ocr_only=True)
                logger.info("âœ… vLLM service initialized (OCR-only mode)")
        except Exception as e:
            logger.warning(f"vLLM service initialization failed: {e}")
            vllm_service = None

        rag_service = RAGService(llm_service, vectordb_service, vllm_service)

    return rag_service


async def verify_api_key(x_api_key: str | None = Header(None)):
    """API í‚¤ ê²€ì¦"""
    # ì‹¤ì œë¡œëŠ” í™˜ê²½ë³€ìˆ˜ë‚˜ DBì—ì„œ í™•ì¸
    if x_api_key != "your-api-key-here":
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid API key")
    return x_api_key


# ============================================================================
# API 1: í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© (í†µí•©) (ë¹„ë™ê¸°)
# ============================================================================


@router.post(
    "/text/extract",
    response_model=AsyncTaskResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© ì €ì¥ (ì´ë ¥ì„œ + ì±„ìš©ê³µê³ )",
    description="""
    ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë‚´ë¶€ì—ì„œ ì„ë² ë”©ê¹Œì§€ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    **ìš”ì²­ êµ¬ì¡°:**
    - `resume`: ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤ ì…ë ¥ (í•„ìˆ˜)
    - `job_posting`: ì±„ìš©ê³µê³  ì…ë ¥ (í•„ìˆ˜)
    - ê° ë¬¸ì„œëŠ” íŒŒì¼ ì—…ë¡œë“œ(`s3_key` + `file_type`) ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥(`text`) ì¤‘ í•˜ë‚˜ ì„ íƒ

    **ì²˜ë¦¬ ë°©ì‹:** ë¹„ë™ê¸° (task_id ë°˜í™˜ â†’ í´ë§ í•„ìš”)

    **ë‚´ë¶€ ì²˜ë¦¬ íë¦„:**
    1. ì´ë ¥ì„œ ì²˜ë¦¬: íŒŒì¼ì´ë©´ OCR/VLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ, í…ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    2. ì±„ìš©ê³µê³  ì²˜ë¦¬: íŒŒì¼ì´ë©´ OCR/VLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ, í…ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    3. ê° í…ìŠ¤íŠ¸ ì²­í‚¹ (500 tokens, 50 overlap)
    4. Gemini Embedding ìƒì„±
    5. VectorDBì— ì €ì¥ (resume, job_posting ì»¬ë ‰ì…˜)
    6. ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (ì´ë ¥ì„œ/ì±„ìš©ê³µê³  ë¶„ì„)
    7. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ + ë¶„ì„ ê²°ê³¼ ë°˜í™˜
    """,
    responses={
        400: {
            "description": "Bad Request",
            "content": {
                "application/json": {
                    "examples": {
                        "invalid_request": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_REQUEST",
                                    "message": "resumeê³¼ job_posting ëŠ” í•„ìˆ˜ ì…ë ¥í•´ì•¼í•©ë‹ˆë‹¤",
                                }
                            }
                        },
                        "invalid_file_type": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_FILE_TYPE",
                                    "message": "file_typeì€ pdf ë˜ëŠ” imageë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                                    "field": "resume.file_type",
                                }
                            }
                        },
                        "invalid_document": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_DOCUMENT",
                                    "message": "s3_key ë˜ëŠ” text ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤",
                                    "field": "resume",
                                }
                            }
                        },
                    }
                }
            },
        },
        401: {
            "description": "Unauthorized",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {"code": "UNAUTHORIZED", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤"}
                    }
                }
            },
        },
        404: {
            "description": "Not Found",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "FILE_NOT_FOUND",
                            "message": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: users/12/resume/abc123.pdf",
                        }
                    }
                }
            },
        },
        422: {
            "description": "Unprocessable Entity",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "OCR_FAILED",
                            "message": "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        429: {
            "description": "Too Many Requests",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "RATE_LIMIT_EXCEEDED",
                            "message": "ìš”ì²­ í•œë„ ì´ˆê³¼. 1ë¶„ í›„ ì¬ì‹œë„í•˜ì„¸ìš”",
                        }
                    }
                }
            },
        },
        500: {
            "description": "Internal Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "INTERNAL_ERROR",
                            "message": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        503: {
            "description": "Service Unavailable",
            "content": {
                "application/json": {
                    "examples": {
                        "llm_unavailable": {
                            "value": {
                                "detail": {
                                    "code": "LLM_UNAVAILABLE",
                                    "message": "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                                }
                            }
                        },
                        "s3_unavailable": {
                            "value": {
                                "detail": {
                                    "code": "S3_UNAVAILABLE",
                                    "message": "íŒŒì¼ ìŠ¤í† ë¦¬ì§€ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                                }
                            }
                        },
                    }
                }
            },
        },
    },
)
async def text_extract(request: TextExtractRequest):
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© ì €ì¥ (í†µí•©) - ì´ë ¥ì„œ + ì±„ìš©ê³µê³ """
    task_id = request.task_id  # ë°±ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ task_id ì‚¬ìš©

    # ë¹„ë™ê¸° ì‘ì—… ì‹œì‘ (í†µí•© task_store ì‚¬ìš©)
    task_key = str(task_id)  # íŒŒì¼ ê¸°ë°˜ ì €ì¥ì†ŒëŠ” ë¬¸ìì—´ í‚¤ ì‚¬ìš©
    task_store.save(
        task_key,
        {
            "type": "text_extract",
            "status": TaskStatus.PROCESSING,
            "created_at": datetime.now(),
            "room_id": request.room_id,
            "request": request.model_dump(),
        },
    )

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
    async def process_text_extract():
        try:
            rag = get_services()

            # ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ê°’: gemini)
            model = request.model if hasattr(request, "model") and request.model else "gemini"
            logger.info("")
            logger.info(f"{'='*80}")
            logger.info("=== ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘ (ì´ë ¥ì„œ + ì±„ìš©ê³µê³ ) ===")
            logger.info(f"{'='*80}")
            logger.info(f"ğŸ“Œ ìš”ì²­ ëª¨ë¸: {model.upper()}")
            # ì‚¬ìš©ì IDëŠ” ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
            logger.info("ğŸ“Œ ì‚¬ìš©ì ID: [REDACTED]")
            logger.info(f"ğŸ“Œ vLLM ì„œë¹„ìŠ¤: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if rag.vllm else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
            logger.info("")

            async def extract_document(
                doc_input: DocumentInput, doc_type: str
            ) -> DocumentExtractResult:
                """ë¬¸ì„œ ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜"""
                logger.info(f"ğŸ“„ [{doc_type.upper()}] ì²˜ë¦¬ ì‹œì‘")

                # íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹
                if doc_input.s3_key:
                    # MIME íƒ€ì…ì„ ë‹¨ìˆœ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (pdf/image)
                    file_type = doc_input.get_file_type_simple() or "pdf"
                    logger.info(f"   â†’ íŒŒì¼ íƒ€ì… (MIME): {doc_input.file_type}")
                    logger.info(f"   â†’ íŒŒì¼ íƒ€ì… (ë‹¨ìˆœ): {file_type}")
                    safe_s3_key = sanitize_log_input(doc_input.s3_key)
                    logger.info("   â†’ S3 í‚¤: %s", safe_s3_key)

                    # vLLM ëª¨ë“œ: EasyOCR ì‚¬ìš© (ê°€ì„±ë¹„)
                    if model == "vllm" and rag.vllm:
                        logger.info("   ğŸ’° [vLLM ê°€ì„±ë¹„ ëª¨ë“œ] EasyOCR ì‹œì‘")
                        ocr_result = await rag.vllm.extract_text_from_file(
                            file_url=str(doc_input.s3_key),
                            file_type=file_type,
                            user_id=str(request.user_id),
                        )
                        extracted_text = ocr_result["extracted_text"]
                        pages = [PageText(**page) for page in ocr_result["pages"]]
                        logger.info(
                            f"   âœ… [vLLM OCR] ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text)}ì (í˜ì´ì§€: {len(pages)})"
                        )

                    # Gemini ëª¨ë“œ: Gemini Vision API ì‚¬ìš© (ê³ ì„±ëŠ¥)
                    else:
                        if model == "vllm" and not rag.vllm:
                            logger.warning("   âš ï¸ vLLM ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€ â†’ Geminië¡œ ìë™ ë³€ê²½")
                        logger.info("   ğŸš€ [Gemini ê³ ì„±ëŠ¥ ëª¨ë“œ] Gemini Vision API OCR ì‹œì‘")
                        ocr_result = await rag.llm.extract_text_from_file(
                            file_url=str(doc_input.s3_key), file_type=file_type
                        )
                        extracted_text = ocr_result["extracted_text"]
                        pages = [PageText(**page) for page in ocr_result["pages"]]
                        logger.info(
                            f"   âœ… [Gemini OCR] ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text)}ì (í˜ì´ì§€: {len(pages)})"
                        )

                # í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥
                else:
                    extracted_text = doc_input.text or ""
                    pages = None
                    logger.info(f"   â†’ í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥: {len(extracted_text)} characters")

                # VectorDBì— ì„ë² ë”© ì €ì¥
                if extracted_text:
                    document_id = f"{doc_type}_{uuid.uuid4().hex[:12]}"
                    await rag.vectordb.add_document(
                        document_id=document_id,
                        text=extracted_text,
                        collection_type=doc_type,
                        metadata={
                            "user_id": request.user_id,
                            "file_id": doc_input.file_id,
                            "created_at": datetime.now().isoformat(),
                        },
                    )
                    safe_document_id = sanitize_log_input(document_id)
                    logger.info("   âœ… VectorDB ì €ì¥ ì™„ë£Œ: %s", safe_document_id)

                return DocumentExtractResult(
                    file_id=doc_input.file_id, extracted_text=extracted_text, pages=pages
                )

            # ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³  ê°ê° ì²˜ë¦¬
            resume_result = await extract_document(request.resume, "resume")
            job_posting_result = await extract_document(request.job_posting, "job_posting")

            # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (ëª…ì„¸ì„œ ìš”êµ¬ì‚¬í•­)
            logger.info("")
            logger.info("ğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
            try:
                analysis_result = await rag.llm.generate_analysis(
                    resume_text=resume_result.extracted_text,
                    posting_text=job_posting_result.extracted_text,
                    user_id=str(request.user_id),
                )
                logger.info("âœ… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e} (OCR í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜)")
                analysis_result = {
                    "resume_analysis": {"strengths": [], "weaknesses": [], "suggestions": []},
                    "posting_analysis": {
                        "company": "ì•Œ ìˆ˜ ì—†ìŒ",
                        "position": "ì•Œ ìˆ˜ ì—†ìŒ",
                        "required_skills": [],
                        "preferred_skills": [],
                    },
                }

            # ê²°ê³¼ ì €ì¥ (ëª…ì„¸ì„œì— ë”°ë¥¸ ì‘ë‹µ êµ¬ì¡°)
            task_data = task_store.get(task_key) or {}
            task_data["status"] = TaskStatus.COMPLETED
            task_data["result"] = TextExtractResult(
                success=True,
                resume_ocr=resume_result.extracted_text,
                job_posting_ocr=job_posting_result.extracted_text,
                resume_analysis=analysis_result.get("resume_analysis"),
                posting_analysis=analysis_result.get("posting_analysis"),
            ).model_dump()
            task_store.save(task_key, task_data)

            logger.info("")
            logger.info("âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë¶„ì„ ì™„ë£Œ!")
            logger.info(f"   â†’ ì´ë ¥ì„œ OCR: {len(resume_result.extracted_text)}ì")
            logger.info(f"   â†’ ì±„ìš©ê³µê³  OCR: {len(job_posting_result.extracted_text)}ì")

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            task_data = task_store.get(task_key) or {}
            task_data["status"] = TaskStatus.FAILED
            task_data["error"] = {"code": ErrorCode.PROCESSING_ERROR, "message": str(e)}
            task_store.save(task_key, task_data)

    asyncio.create_task(process_text_extract())

    return AsyncTaskResponse(task_id=task_id, status=TaskStatus.PROCESSING)


# ============================================================================
# ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ
# ============================================================================


@router.get(
    "/task/{task_id}",
    response_model=TaskStatusResponse,
    summary="ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ",
    description="ë¹„ë™ê¸° ì²˜ë¦¬ ì‘ì—…ì˜ ìƒíƒœë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
    responses={
        401: {
            "description": "Unauthorized",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {"code": "UNAUTHORIZED", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤"}
                    }
                }
            },
        },
        404: {
            "description": "Not Found",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "TASK_NOT_FOUND",
                            "message": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: 12",
                        }
                    }
                }
            },
        },
    },
)
async def get_task_status(task_id: str):
    """í†µí•© ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ (text_extract, masking ë“±)"""
    task_key = str(task_id)  # ë¬¸ìì—´ í‚¤ë¡œ í†µì¼
    task = task_store.get(task_key)

    if task is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail={
                "code": ErrorCode.TASK_NOT_FOUND.value,
                "message": f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {task_id}",
            },
        )

    # room_idë¥¼ resultì— í¬í•¨
    result = task.get("result")
    if result and "room_id" not in result:
        result["room_id"] = task.get("room_id")

    return TaskStatusResponse(
        task_id=task_id,
        status=task["status"],
        progress=task.get("progress"),
        message=task.get("message"),
        result=result,
        error=task.get("error"),
    )


# ============================================================================
# API 2: ì±„íŒ… (í†µí•©: ëŒ€í™”/ë¶„ì„/ë©´ì ‘) (ìŠ¤íŠ¸ë¦¬ë°)
# ============================================================================


async def generate_chat_stream(request: ChatRequest):
    """ì±„íŒ… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""

    # contextì—ì„œ ëª¨ë“œ ê²°ì • (normal ë˜ëŠ” interview)
    mode = request.context.mode if request.context else ChatMode.NORMAL

    rag = get_services()
    newline = "\n"
    sse_end = "\n\n"

    # ëª¨ë¸ ì„ íƒ (gemini ë˜ëŠ” vllm)
    model = request.model.value if hasattr(request.model, "value") else str(request.model)
    logger.info("")
    logger.info(f"{'='*80}")
    logger.info("=== ğŸ’¬ ì±„íŒ… ìš”ì²­ ì‹œì‘ ===")
    logger.info(f"{'='*80}")
    # ëª¨ë¸ëª…ê³¼ ëª¨ë“œëŠ” ì‚¬ìš©ì ì…ë ¥ì´ë¯€ë¡œ ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
    logger.info("ğŸ“Œ ìš”ì²­ ëª¨ë¸: [REDACTED]")
    logger.info("ğŸ“Œ ì±„íŒ… ëª¨ë“œ: [REDACTED]")
    # ì‚¬ìš©ì IDì™€ ì±„íŒ…ë°© IDëŠ” ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
    logger.info("ğŸ“Œ ì‚¬ìš©ì ID: [REDACTED]")
    logger.info("ğŸ“Œ ì±„íŒ…ë°© ID: [REDACTED]")
    logger.info(f"ğŸ“Œ vLLM ì„œë¹„ìŠ¤: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if rag.vllm else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
    logger.info("")

    # 1. ì¼ë°˜ ëŒ€í™” (RAG í™œìš©)
    if mode == ChatMode.NORMAL:
        full_response = ""

        try:
            # íˆìŠ¤í† ë¦¬ ì—†ì´ ë‹¨ì¼ ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬ (ëª…ì„¸ì„œ ê¸°ì¤€)
            history_dict = []

            # Determine if this is an analysis request
            user_message = request.message or ""
            is_analysis = any(
                keyword in user_message for keyword in ["ë¶„ì„", "ë§¤ì¹­", "ì í•©", "í‰ê°€", "ë¹„êµ"]
            )

            # ë©´ì ‘ ëª¨ë“œ ì—¬ë¶€ í™•ì¸
            is_followup = request.interview_id is not None and request.context.mode == ChatMode.INTERVIEW

            if is_analysis:
                # ===================================================================
                # ë¶„ì„ ìš”ì²­: vLLMê³¼ Gemini ì™„ì „ ë¶„ë¦¬
                # ===================================================================
                # ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                logger.info("ğŸ” ë¶„ì„ ìš”ì²­ ê°ì§€")
                logger.info("")

                # ---------------------------------------------------------------
                # vLLM ëª¨ë“œ (ê°€ì„±ë¹„): EasyOCR â†’ VectorDB â†’ Llama ë¶„ì„
                # ---------------------------------------------------------------
                if model == "vllm" and rag.vllm:
                    logger.info("ğŸ’° [vLLM ê°€ì„±ë¹„ ëª¨ë“œ] ë¶„ì„ ì‹œì‘")
                    logger.info("   í”„ë¡œì„¸ìŠ¤: EasyOCR â†’ VectorDB ì €ì¥ â†’ VectorDB ì¡°íšŒ â†’ Llama ë¶„ì„")
                    logger.info("")

                    # 1. VectorDBì—ì„œ OCRë¡œ ì¶”ì¶œëœ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                    logger.info("ğŸ“‚ [1/3] VectorDBì—ì„œ ì—…ë¡œë“œëœ ë¬¸ì„œ ì¡°íšŒ ì¤‘...")
                    full_context = await rag.retrieve_all_documents(
                        user_id=request.user_id, context_types=["resume", "job_posting"]
                    )

                    if not full_context:
                        error_msg = "âŒ ì—…ë¡œë“œëœ ì´ë ¥ì„œ ë˜ëŠ” ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\në¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                        # ì‚¬ìš©ì IDëŠ” ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                        logger.error("âš ï¸ VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                        yield f"data: {json.dumps({'type': 'chunk', 'content': error_msg}, ensure_ascii=False)}{sse_end}"
                        full_response = error_msg
                    else:
                        logger.info(f"âœ… [1/3] VectorDB ì¡°íšŒ ì™„ë£Œ: {len(full_context)}ì")
                        logger.info("")

                        # 2. Llama ëª¨ë¸ë¡œ ë¶„ì„
                        logger.info("ğŸ¤– [2/3] Llama ëª¨ë¸ ë¶„ì„ ì‹œì‘...")
                        analysis_prompt = f"""ë‹¤ìŒ ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ë¥¼ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:

{full_context}

ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. **ì í•©ë„ í‰ê°€**: ì§€ì›ìê°€ ì±„ìš©ê³µê³  ìš”êµ¬ì‚¬í•­ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ì§€
2. **ê°•ì **: ì§€ì›ìì˜ ë›°ì–´ë‚œ ì—­ëŸ‰ê³¼ ê²½í—˜
3. **ì•½ì **: ë¶€ì¡±í•œ ë¶€ë¶„ì´ë‚˜ ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­
4. **ì˜ˆìƒ ë©´ì ‘ ì§ˆë¬¸ (ì¸ì„± 3ê°œ, ê¸°ìˆ  3ê°œ)**

ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”."""

                        async for chunk in rag.vllm.generate_response(
                            user_message=analysis_prompt,
                            context=None,
                            history=[],
                            system_prompt="ë‹¹ì‹ ì€ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ë¥¼ ë¶„ì„í•˜ì—¬ ëª…í™•í•œ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”.",
                        ):
                            full_response += chunk
                            yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"

                        logger.info(f"âœ… [3/3] Llama ë¶„ì„ ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")

                # ---------------------------------------------------------------
                # Gemini ëª¨ë“œ (ê³ ì„±ëŠ¥): Gemini Vision APIë¡œ ì§ì ‘ íŒŒì¼ ì½ê³  ë¶„ì„
                # ---------------------------------------------------------------
                else:
                    if model == "vllm" and not rag.vllm:
                        logger.warning("âš ï¸ vLLM ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€ â†’ Geminië¡œ ìë™ ë³€ê²½")

                    logger.info("ğŸš€ [Gemini ê³ ì„±ëŠ¥ ëª¨ë“œ] ë¶„ì„ ì‹œì‘")
                    logger.info("   í”„ë¡œì„¸ìŠ¤: RAG ê²€ìƒ‰ â†’ Gemini ë¶„ì„ (ì›ë˜ ë°©ì‹)")
                    logger.info("")

                    # RAGë¡œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰í•˜ì—¬ Geminië¡œ ë¶„ì„
                    logger.info("ğŸ“‚ [1/2] RAG ê²€ìƒ‰ ì¤‘...")
                    async for chunk in rag.chat_with_rag(
                        user_message=user_message,
                        user_id=request.user_id,
                        history=history_dict,
                        use_rag=True,
                        context_types=["resume", "job_posting"],
                        model="gemini",
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"

                    logger.info(f"âœ… [2/2] Gemini ë¶„ì„ ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")
            else:
                # ===================================================================
                # ì¼ë°˜ ëŒ€í™”: RAG ê²€ìƒ‰ ì‚¬ìš©
                # ===================================================================
                logger.info("ğŸ’¬ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ")
                logger.info("")

                # ë©´ì ‘ ì„¸ì…˜ì—ì„œ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± (interview_idê°€ ìˆê³ , ì´ì „ ì§ˆë¬¸-ë‹µë³€ ìŒì´ ìˆëŠ” ê²½ìš°)
                if is_followup:
                    original_question = history_dict[-2].get("content", "")
                    candidate_answer = history_dict[-1].get("content", "")

                    logger.info("ğŸ” [ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±] ê°ì§€")
                    # ì‚¬ìš©ì ì…ë ¥ì€ ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                    logger.info("   ì›ë³¸ ì§ˆë¬¸: [REDACTED]")
                    logger.info("   ë‹µë³€: [REDACTED]")
                    logger.info("")

                    # ê°„ë‹¨í•œ STAR ë¶„ì„ (ì‹¤ì œë¡œëŠ” LLMìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
                    star_analysis = {
                        "situation": "unknown",
                        "task": "unknown",
                        "action": "unknown",
                        "result": "unknown",
                    }

                    # ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±
                    async for chunk in rag.generate_followup_question(
                        original_question=original_question,
                        candidate_answer=candidate_answer,
                        star_analysis=star_analysis,
                        model=model,
                        user_id=request.user_id,
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"

                    logger.info(f"âœ… [ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±] ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")

                else:
                    # ì¼ë°˜ ëŒ€í™” ë˜ëŠ” ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­
                    if (
                        "ë©´ì ‘ ì§ˆë¬¸" in user_message
                        or "ë©´ì ‘ì§ˆë¬¸" in user_message
                        or "ë©´ì ‘" in user_message
                    ):
                        # ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­ ì‹œ portfolio(ë©´ì ‘ ì§ˆë¬¸ ë°ì´í„°)ë§Œ ê²€ìƒ‰
                        context_types = ["portfolio"]
                        logger.info("ğŸ¯ ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­ ê°ì§€ â†’ portfolio ì»¬ë ‰ì…˜ë§Œ ê²€ìƒ‰")
                    else:
                        # ì¼ë°˜ ìš”ì²­ ì‹œ ëª¨ë“  ì»¬ë ‰ì…˜ ê²€ìƒ‰
                        context_types = ["resume", "job_posting", "portfolio"]
                        logger.info("ğŸ“š ì¼ë°˜ ëŒ€í™” â†’ ëª¨ë“  ì»¬ë ‰ì…˜ ê²€ìƒ‰")

                    logger.info("")

                    # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±
                    # ëª¨ë¸ëª…ì€ ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                    logger.info("ğŸ” RAG ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì‹œì‘...")
                    async for chunk in rag.chat_with_rag(
                        user_message=user_message,
                        user_id=request.user_id,
                        history=history_dict,
                        use_rag=True,  # RAG í™œì„±í™”
                        context_types=context_types,
                        model=model,
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"

                    # ëª¨ë¸ëª…ì€ ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                    logger.info("âœ… ì¼ë°˜ ëŒ€í™” ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: %dì)", len(full_response))

        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            yield f"data: {json.dumps({'type': 'chunk', 'content': error_msg}, ensure_ascii=False)}{sse_end}"
            full_response = error_msg

        result = {
            "success": True,
            "mode": "general",
            "response": full_response,
            "tool_used": {"tool": "RAG", "description": "VectorDB ê²€ìƒ‰ í›„ LLM ì‘ë‹µ ìƒì„±"},
        }
        yield f"data: {json.dumps({'type': 'complete', 'data': result}, ensure_ascii=False)}{sse_end}"

    # 2. ë©´ì ‘ ëª¨ë“œ - ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„± ë° ëŒ€í™”
    elif mode == ChatMode.INTERVIEW:
        try:
            # ë©´ì ‘ íƒ€ì…ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì¡°ì • (behavior: ì¸ì„±, tech: ê¸°ìˆ )
            interview_type = request.context.interview_type or "tech"
            interview_type_kr = "ê¸°ìˆ " if interview_type == "tech" else "ì¸ì„±"

            content = f"{interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...{newline}"
            yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}{sse_end}"

            # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ë§ì¶¤ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
            # resume, portfolio, job_posting ì»¬ë ‰ì…˜ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            context = await rag.retrieve_context(
                query=f"{interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ì„ ìœ„í•œ ì‚¬ìš©ì ì •ë³´",
                user_id=request.user_id,
                context_types=["resume", "portfolio", "job_posting"],
                n_results=1,  # ì†ë„ ê°œì„ ì„ ìœ„í•´ 1ê°œë§Œ ê²€ìƒ‰
            )

            # ë©´ì ‘ ì§ˆë¬¸ ìƒì„± í”„ë¡¬í”„íŠ¸ (prompts ëª¨ë“ˆ ì‚¬ìš©)
            # contextì—ì„œ resume_ocr, job_posting_ocr ì‚¬ìš©
            resume_ocr = request.context.resume_ocr if request.context else None
            job_posting_ocr = request.context.job_posting_ocr if request.context else None

            if resume_ocr or job_posting_ocr or context:
                # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±
                question_prompt = create_interview_question_prompt(
                    resume_text=resume_ocr or context or "ì •ë³´ ì—†ìŒ",
                    job_posting_text=job_posting_ocr or "ì •ë³´ ì—†ìŒ",
                    interview_type=interview_type,
                )
            else:
                question_prompt = f"ì¼ë°˜ì ì¸ {interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ 1ê°œë¥¼ ì§§ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”:"

            full_question = ""

            # vLLM ë˜ëŠ” Gemini ì„ íƒ
            model_choice = (
                request.model.value if hasattr(request.model, "value") else str(request.model)
            )

            if model_choice == "vllm" and rag.vllm:
                logger.info("ğŸ’¬ [vLLM] ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.vllm.generate_response(
                    user_message=question_prompt,
                    context=None,
                    history=[],
                    system_prompt=SYSTEM_INTERVIEW,
                ):
                    full_question += chunk
                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"
            else:
                logger.info("ğŸ’¬ [Gemini] ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.llm.generate_response(
                    user_message=question_prompt,
                    context=None,
                    history=[],
                    system_prompt=SYSTEM_INTERVIEW,
                    user_id=request.user_id,
                ):
                    full_question += chunk
                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"

            result = {
                "success": True,
                "mode": "interview_question",
                "response": full_question.strip(),
                "interview_type": interview_type,
            }
            yield f"data: {json.dumps({'type': 'complete', 'data': result}, ensure_ascii=False)}{sse_end}"

        except Exception as e:
            logger.error(f"Interview question generation error: {e}")
            error_result = {"success": False, "mode": "interview", "error": str(e)}
            yield f"data: {json.dumps({'type': 'complete', 'data': error_result}, ensure_ascii=False)}{sse_end}"

    # 3. ë¦¬í¬íŠ¸ ëª¨ë“œ - ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
    elif mode == ChatMode.REPORT:
        try:
            interview_type = request.context.interview_type or "tech"
            interview_type_kr = "ê¸°ìˆ " if interview_type == "tech" else "ì¸ì„±"
            qa_list = request.context.qa_list or []

            if not qa_list:
                error_result = {"success": False, "mode": "report", "error": "Q&A ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
                yield f"data: {json.dumps({'type': 'complete', 'data': error_result}, ensure_ascii=False)}{sse_end}"
                return

            content = f"{interview_type_kr} ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...{newline}"
            yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}{sse_end}"

            # Q&A ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            qa_text = ""
            for i, qa in enumerate(qa_list, 1):
                q = qa.get("question", "")
                a = qa.get("answer", "")
                qa_text += f"ì§ˆë¬¸ {i}: {q}\në‹µë³€ {i}: {a}\n\n"

            # í‰ê°€ ë¦¬í¬íŠ¸ í”„ë¡¬í”„íŠ¸
            report_prompt = f"""
ë‹¤ìŒì€ {interview_type_kr} ë©´ì ‘ Q&A ê¸°ë¡ì…ë‹ˆë‹¤:

{qa_text}

ìœ„ ë©´ì ‘ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ê° ë‹µë³€ì— ëŒ€í•œ ê°œë³„ í‰ê°€ (ì˜í•œ ì , ê°œì„ ì )
2. ì „ì²´ì ì¸ ê°•ì  íŒ¨í„´
3. ì „ì²´ì ì¸ ì•½ì  íŒ¨í„´
4. í–¥í›„ í•™ìŠµ ê°€ì´ë“œ
"""

            full_report = ""

            # vLLM ë˜ëŠ” Gemini ì„ íƒ
            model_choice = (
                request.model.value if hasattr(request.model, "value") else str(request.model)
            )

            if model_choice == "vllm" and rag.vllm:
                logger.info("ğŸ“Š [vLLM] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.vllm.generate_response(
                    user_message=report_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
                ):
                    full_report += chunk
                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"
            else:
                logger.info("ğŸ“Š [Gemini] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.llm.generate_response(
                    user_message=report_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
                    user_id=request.user_id,
                ):
                    full_report += chunk
                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}{sse_end}"

            result = {
                "success": True,
                "mode": "report",
                "response": full_report.strip(),
                "interview_type": interview_type,
                "interview_id": request.interview_id,
            }
            yield f"data: {json.dumps({'type': 'complete', 'data': result}, ensure_ascii=False)}{sse_end}"

        except Exception as e:
            logger.error(f"Interview report generation error: {e}")
            error_result = {"success": False, "mode": "report", "error": str(e)}
            yield f"data: {json.dumps({'type': 'complete', 'data': error_result}, ensure_ascii=False)}{sse_end}"


@router.post(
    "/chat",
    summary="ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° (ì¼ë°˜/ë©´ì ‘/ë¦¬í¬íŠ¸)",
    description="""
    ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    **ì²˜ë¦¬ ë°©ì‹:** ìŠ¤íŠ¸ë¦¬ë° (SSE)

    **ëª¨ë“œ:**
    - normal: ì¼ë°˜ ëŒ€í™”
    - interview: ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
    - report: ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±

    **ë©´ì ‘ íƒ€ì… (context.interview_type):**
    - behavior: ì¸ì„± ë©´ì ‘
    - tech: ê¸°ìˆ  ë©´ì ‘
    """,
    responses={
        400: {
            "description": "Bad Request",
            "content": {
                "application/json": {
                    "examples": {
                        "invalid_request": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_REQUEST",
                                    "message": "room_idëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤",
                                    "field": "room_id",
                                }
                            }
                        },
                        "invalid_mode": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_MODE",
                                    "message": "modeëŠ” normal ë˜ëŠ” interview ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤",
                                    "field": "context.mode",
                                }
                            }
                        },
                        "invalid_interview_type": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_INTERVIEW_TYPE",
                                    "message": "interview_typeì€ behavior ë˜ëŠ” techë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                                    "field": "context.interview_type",
                                }
                            }
                        },
                    }
                }
            },
        },
        401: {
            "description": "Unauthorized",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {"code": "UNAUTHORIZED", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤"}
                    }
                }
            },
        },
        422: {
            "description": "Unprocessable Entity",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "MISSING_CONTEXT",
                            "message": "ë©´ì ‘ ëª¨ë“œ ì‹œ context.resume_ocr ë˜ëŠ” context.job_posting_ocrì´ í•„ìš”í•©ë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        429: {
            "description": "Too Many Requests",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "RATE_LIMIT_EXCEEDED",
                            "message": "ìš”ì²­ í•œë„ ì´ˆê³¼. 1ë¶„ í›„ ì¬ì‹œë„í•˜ì„¸ìš”",
                        }
                    }
                }
            },
        },
        500: {
            "description": "Internal Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "INTERNAL_ERROR",
                            "message": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        503: {
            "description": "Service Unavailable",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "LLM_UNAVAILABLE",
                            "message": "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
    },
)
async def chat(request: ChatRequest):
    """ì±„íŒ… ì²˜ë¦¬ (ì¼ë°˜/ë©´ì ‘)"""
    return StreamingResponse(generate_chat_stream(request), media_type="text/event-stream")


# ============================================================================
# API 3: ìº˜ë¦°ë” ì¼ì • íŒŒì‹± (ë™ê¸°)
# ============================================================================


@router.post(
    "/calendar/parse",
    response_model=CalendarParseResponse,
    summary="ìº˜ë¦°ë” ì¼ì • ì •ë³´ íŒŒì‹±",
    description="""
    ìº˜ë¦°ë” ëª¨ë‹¬ì—ì„œ íŒŒì¼/í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (í¼ ìë™ ì±„ìš°ê¸°ìš©).

    **ì²˜ë¦¬ ë°©ì‹:** ë™ê¸° - ê°„ë‹¨í•œ íŒŒì‹± ì‘ì—…

    **Pydantic AI ì‚¬ìš©:** CalendarParseResult

    **ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
    - ëª¨ë‹¬ì—ì„œ ì±„ìš©ê³µê³  íŒŒì¼/í…ìŠ¤íŠ¸ ì²¨ë¶€ â†’ ì¼ì • ì •ë³´ ì¶”ì¶œ
    - Frontendê°€ ëª¨ë‹¬ í¼ì— ìë™ ì±„ì›Œë„£ìŒ
    - ì‚¬ìš©ì í™•ì¸/ìˆ˜ì • â†’ ì €ì¥ â†’ Backendê°€ Google Calendarì— ì¶”ê°€
    """,
)
async def calendar_parse(request: CalendarParseRequest):  # noqa: ARG001
    """ìº˜ë¦°ë” ì¼ì • íŒŒì‹±"""
    # validatorì—ì„œ ì´ë¯¸ ê²€ì¦ë¨

    # Pydantic AIë¡œ êµ¬ì¡°í™”ëœ ê²°ê³¼ ë°˜í™˜
    return CalendarParseResponse(
        success=True,
        company="ì¹´ì¹´ì˜¤",
        position="ë°±ì—”ë“œ ê°œë°œì",
        schedules=[
            {"stage": "ì„œë¥˜ ë§ˆê°", "date": "2026-01-15", "time": None},
            {"stage": "ì½”ë”©í…ŒìŠ¤íŠ¸", "date": "2026-01-20", "time": "14:00"},
            {"stage": "1ì°¨ ë©´ì ‘", "date": "2026-01-25", "time": None},
        ],
        hashtags=["#ì¹´ì¹´ì˜¤", "#ë°±ì—”ë“œ", "#ì‹ ì…"],
    )


# ============================================================================
# API 4: ê²Œì‹œíŒ ì²¨ë¶€íŒŒì¼ ë§ˆìŠ¤í‚¹ (ë¹„ë™ê¸°)
# ============================================================================
# ì´ APIëŠ” app/api/routes/masking.pyë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.
# masking.pyì—ì„œ íŒŒì¼ ê¸°ë°˜ ì €ì¥ì†Œì™€ ì‹¤ì œ Gemini APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
