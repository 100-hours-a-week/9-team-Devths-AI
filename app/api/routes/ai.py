import asyncio
import json
import logging
import os
import uuid
from datetime import datetime

from fastapi import APIRouter, Header, HTTPException, status
from fastapi.responses import StreamingResponse

from app.prompts import (
    SYSTEM_INTERVIEW,
    create_interview_question_prompt,
    get_extract_title_prompt,
)
from app.schemas.calendar import CalendarParseRequest, CalendarParseResponse
from app.schemas.chat import (
    ChatMode,
    ChatRequest,
)
from app.schemas.common import AsyncTaskResponse, ErrorCode, TaskStatus, TaskStatusResponse
from app.schemas.text_extract import (
    DocumentExtractResult,
    DocumentInput,
    PageText,
    TextExtractRequest,
    TextExtractResult,
)
from app.services.llm_service import LLMService
from app.services.rag_service import RAGService
from app.services.vectordb_service import VectorDBService
from app.services.vllm_service import VLLMService
from app.utils.log_sanitizer import sanitize_log_input
from app.utils.task_store import get_task_store

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/ai",
    tags=["AI APIs (v3.0)"],
    responses={404: {"description": "Not found"}},
)

# í†µí•© ì‘ì—… ì €ì¥ì†Œ (íŒŒì¼ ê¸°ë°˜, ì„œë²„ ì¬ì‹œì‘ ì‹œì—ë„ ìœ ì§€)
# ë°±ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ task_idë¥¼ í‚¤ë¡œ ì‚¬ìš©
task_store = get_task_store()


# Initialize services
llm_service = None
vllm_service = None
vectordb_service = None
rag_service = None


def get_services():
    """Get or initialize AI services"""
    global llm_service, vllm_service, vectordb_service, rag_service

    if llm_service is None:
        api_key = os.getenv("GOOGLE_API_KEY")
        llm_service = LLMService(api_key=api_key)
        vectordb_service = VectorDBService(api_key=api_key)

        # Initialize vLLM service (GCP GPU server)
        gcp_vllm_url = os.getenv("GCP_VLLM_BASE_URL")

        try:
            if gcp_vllm_url:
                logger.info(f"ğŸŒ GCP vLLM ì„œë²„ ì—°ê²°: {gcp_vllm_url}")
                vllm_service = VLLMService()
                logger.info("âœ… vLLM service initialized (GCP GPU server)")
            else:
                # GCP URL ì—†ìœ¼ë©´ OCR ì „ìš© ëª¨ë“œ
                logger.info("ğŸ’° GCP URL ì—†ìŒ - OCR ì „ìš© ëª¨ë“œë¡œ ì´ˆê¸°í™”")
                vllm_service = VLLMService(ocr_only=True)
                logger.info("âœ… vLLM service initialized (OCR-only mode)")
        except Exception as e:
            logger.warning(f"vLLM service initialization failed: {e}")
            vllm_service = None

        rag_service = RAGService(llm_service, vectordb_service, vllm_service)

    return rag_service


def format_analysis_text(
    resume_analysis: dict | None,
    posting_analysis: dict | None,
    summary: str | None,
) -> str:
    """ë¶„ì„ ê²°ê³¼ë¥¼ plain textë¡œ í¬ë§·íŒ… (ë§ˆí¬ë‹¤ìš´ ì—†ì´)

    ë°±ì—”ë“œì—ì„œ ë°”ë¡œ í™”ë©´ì— í‘œì‹œí•  ìˆ˜ ìˆë„ë¡ í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    lines = []

    # íšŒì‚¬/ì§ë¬´
    if summary:
        lines.append(f"ì§€ì› íšŒì‚¬ ë° ì§ë¬´ : {summary}")
        lines.append("")

    # ì´ë ¥ì„œ ë¶„ì„
    if resume_analysis:
        lines.append("ì´ë ¥ì„œ ë¶„ì„")
        lines.append("")
        lines.append("ì¥ì ")
        strengths = resume_analysis.get("strengths", [])
        for i, strength in enumerate(strengths[:5], 1):
            lines.append(f"{i}. {strength}")
        lines.append("")
        lines.append("ë‹¨ì ")
        weaknesses = resume_analysis.get("weaknesses", [])
        for i, weakness in enumerate(weaknesses[:5], 1):
            lines.append(f"{i}. {weakness}")
        lines.append("")

    # ì±„ìš©ê³µê³  ë¶„ì„
    if posting_analysis:
        lines.append("ì±„ìš© ê³µê³  ë¶„ì„")
        lines.append("")
        company = posting_analysis.get("company", "")
        position = posting_analysis.get("position", "")
        lines.append("ê¸°ì—… / í¬ì§€ì…˜")
        lines.append(f"{company} / {position}")
        lines.append("")
        lines.append("í•„ìˆ˜ ì—­ëŸ‰")
        for skill in posting_analysis.get("required_skills", [])[:5]:
            lines.append(f"- {skill}")
        lines.append("")
        lines.append("ìš°ëŒ€ ì‚¬í•­")
        for skill in posting_analysis.get("preferred_skills", [])[:5]:
            lines.append(f"- {skill}")
        lines.append("")

    # ë§¤ì¹­ë„
    if resume_analysis and posting_analysis:
        lines.append("ë§¤ì¹­ë„")
        lines.append("")
        lines.append("ë‚˜ì™€ ì§€ì› ì§ë¬´ì— ë§ëŠ” ì ")
        # matches í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ strengthsì—ì„œ ê°€ì ¸ì˜´
        matches = resume_analysis.get("matches", resume_analysis.get("strengths", [])[:3])
        for match in matches[:3] if matches else []:
            lines.append(f"- {match}")
        lines.append("")
        lines.append("ë‚˜ì™€ ì§€ì› ì§ë¬´ì— ë§ì§€ ì•ŠëŠ” ì ")
        # gaps í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ weaknessesì—ì„œ ê°€ì ¸ì˜´
        gaps = resume_analysis.get("gaps", resume_analysis.get("weaknesses", [])[:3])
        for gap in gaps[:3] if gaps else []:
            lines.append(f"- {gap}")

    return "\n".join(lines)


async def verify_api_key(x_api_key: str | None = Header(None)):
    """API í‚¤ ê²€ì¦"""
    # ì‹¤ì œë¡œëŠ” í™˜ê²½ë³€ìˆ˜ë‚˜ DBì—ì„œ í™•ì¸
    if x_api_key != "your-api-key-here":
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid API key")
    return x_api_key


# ============================================================================
# API 1: í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© (í†µí•©) (ë¹„ë™ê¸°)
# ============================================================================


@router.post(
    "/text/extract",
    response_model=AsyncTaskResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© ì €ì¥ (ì´ë ¥ì„œ + ì±„ìš©ê³µê³ )",
    description="""
    ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë‚´ë¶€ì—ì„œ ì„ë² ë”©ê¹Œì§€ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    **ìš”ì²­ êµ¬ì¡°:**
    - `resume`: ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤ ì…ë ¥ (í•„ìˆ˜)
    - `job_posting`: ì±„ìš©ê³µê³  ì…ë ¥ (í•„ìˆ˜)
    - ê° ë¬¸ì„œëŠ” íŒŒì¼ ì—…ë¡œë“œ(`s3_key` + `file_type`) ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥(`text`) ì¤‘ í•˜ë‚˜ ì„ íƒ

    **ì²˜ë¦¬ ë°©ì‹:** ë¹„ë™ê¸° (task_id ë°˜í™˜ â†’ í´ë§ í•„ìš”)

    **ë‚´ë¶€ ì²˜ë¦¬ íë¦„:**
    1. ì´ë ¥ì„œ ì²˜ë¦¬: íŒŒì¼ì´ë©´ OCR/VLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ, í…ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    2. ì±„ìš©ê³µê³  ì²˜ë¦¬: íŒŒì¼ì´ë©´ OCR/VLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ, í…ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    3. ê° í…ìŠ¤íŠ¸ ì²­í‚¹ (500 tokens, 50 overlap)
    4. Gemini Embedding ìƒì„±
    5. VectorDBì— ì €ì¥ (resume, job_posting ì»¬ë ‰ì…˜)
    6. ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (ì´ë ¥ì„œ/ì±„ìš©ê³µê³  ë¶„ì„)
    7. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ + ë¶„ì„ ê²°ê³¼ ë°˜í™˜
    """,
    responses={
        400: {
            "description": "Bad Request",
            "content": {
                "application/json": {
                    "examples": {
                        "invalid_request": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_REQUEST",
                                    "message": "resumeê³¼ job_posting ëŠ” í•„ìˆ˜ ì…ë ¥í•´ì•¼í•©ë‹ˆë‹¤",
                                }
                            }
                        },
                        "invalid_file_type": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_FILE_TYPE",
                                    "message": "file_typeì€ pdf ë˜ëŠ” imageë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                                    "field": "resume.file_type",
                                }
                            }
                        },
                        "invalid_document": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_DOCUMENT",
                                    "message": "s3_key ë˜ëŠ” text ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤",
                                    "field": "resume",
                                }
                            }
                        },
                    }
                }
            },
        },
        401: {
            "description": "Unauthorized",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {"code": "UNAUTHORIZED", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤"}
                    }
                }
            },
        },
        404: {
            "description": "Not Found",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "FILE_NOT_FOUND",
                            "message": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: users/12/resume/abc123.pdf",
                        }
                    }
                }
            },
        },
        422: {
            "description": "Unprocessable Entity",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "OCR_FAILED",
                            "message": "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        429: {
            "description": "Too Many Requests",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "RATE_LIMIT_EXCEEDED",
                            "message": "ìš”ì²­ í•œë„ ì´ˆê³¼. 1ë¶„ í›„ ì¬ì‹œë„í•˜ì„¸ìš”",
                        }
                    }
                }
            },
        },
        500: {
            "description": "Internal Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "INTERNAL_ERROR",
                            "message": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        503: {
            "description": "Service Unavailable",
            "content": {
                "application/json": {
                    "examples": {
                        "llm_unavailable": {
                            "value": {
                                "detail": {
                                    "code": "LLM_UNAVAILABLE",
                                    "message": "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                                }
                            }
                        },
                        "s3_unavailable": {
                            "value": {
                                "detail": {
                                    "code": "S3_UNAVAILABLE",
                                    "message": "íŒŒì¼ ìŠ¤í† ë¦¬ì§€ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                                }
                            }
                        },
                    }
                }
            },
        },
    },
)
async def text_extract(request: TextExtractRequest):
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© ì €ì¥ (í†µí•©) - ì´ë ¥ì„œ + ì±„ìš©ê³µê³ """
    task_id = request.task_id  # ë°±ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ task_id ì‚¬ìš©

    # ë¹„ë™ê¸° ì‘ì—… ì‹œì‘ (í†µí•© task_store ì‚¬ìš©)
    task_key = str(task_id)  # íŒŒì¼ ê¸°ë°˜ ì €ì¥ì†ŒëŠ” ë¬¸ìì—´ í‚¤ ì‚¬ìš©
    task_store.save(
        task_key,
        {
            "type": "text_extract",
            "status": TaskStatus.PROCESSING,
            "created_at": datetime.now(),
            "room_id": request.room_id,
            "request": request.model_dump(),
        },
    )

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
    async def process_text_extract():
        try:
            rag = get_services()

            # ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ê°’: gemini)
            model = request.model if hasattr(request, "model") and request.model else "gemini"
            logger.info("")
            logger.info(f"{'='*80}")
            logger.info("=== ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘ (ì´ë ¥ì„œ + ì±„ìš©ê³µê³ ) ===")
            logger.info(f"{'='*80}")
            logger.info(f"ğŸ“Œ ìš”ì²­ ëª¨ë¸: {model.upper()}")
            logger.info(f"ğŸ“Œ ì‚¬ìš©ì ID: {request.user_id}")
            logger.info(f"ğŸ“Œ vLLM ì„œë¹„ìŠ¤: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if rag.vllm else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
            logger.info("")

            async def extract_document(
                doc_input: DocumentInput, doc_type: str
            ) -> DocumentExtractResult:
                """ë¬¸ì„œ ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜"""
                logger.info(f"ğŸ“„ [{doc_type.upper()}] ì²˜ë¦¬ ì‹œì‘")

                # íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹
                if doc_input.s3_key:
                    # MIME íƒ€ì…ì„ ë‹¨ìˆœ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (pdf/image)
                    file_type = doc_input.get_file_type_simple() or "pdf"
                    logger.info(f"   â†’ íŒŒì¼ íƒ€ì… (MIME): {doc_input.file_type}")
                    logger.info(f"   â†’ íŒŒì¼ íƒ€ì… (ë‹¨ìˆœ): {file_type}")
                    safe_s3_key = sanitize_log_input(doc_input.s3_key)
                    logger.info("   â†’ S3 í‚¤: %s", safe_s3_key)

                    # vLLM ëª¨ë“œ: EasyOCR ì‚¬ìš© (ê°€ì„±ë¹„)
                    if model == "vllm" and rag.vllm:
                        logger.info("   ğŸ’° [vLLM ê°€ì„±ë¹„ ëª¨ë“œ] EasyOCR ì‹œì‘")
                        ocr_result = await rag.vllm.extract_text_from_file(
                            file_url=str(doc_input.s3_key),
                            file_type=file_type,
                            user_id=str(request.user_id),
                        )
                        extracted_text = ocr_result["extracted_text"]
                        pages = [PageText(**page) for page in ocr_result["pages"]]
                        logger.info(
                            f"   âœ… [vLLM OCR] ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text)}ì (í˜ì´ì§€: {len(pages)})"
                        )

                    # Gemini ëª¨ë“œ: Gemini Vision API ì‚¬ìš© (ê³ ì„±ëŠ¥)
                    else:
                        if model == "vllm" and not rag.vllm:
                            logger.warning("   âš ï¸ vLLM ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€ â†’ Geminië¡œ ìë™ ë³€ê²½")
                        logger.info("   ğŸš€ [Gemini ê³ ì„±ëŠ¥ ëª¨ë“œ] Gemini Vision API OCR ì‹œì‘")
                        ocr_result = await rag.llm.extract_text_from_file(
                            file_url=str(doc_input.s3_key), file_type=file_type
                        )
                        extracted_text = ocr_result["extracted_text"]
                        pages = [PageText(**page) for page in ocr_result["pages"]]
                        logger.info(
                            f"   âœ… [Gemini OCR] ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text)}ì (í˜ì´ì§€: {len(pages)})"
                        )

                # í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥
                else:
                    extracted_text = doc_input.text or ""
                    pages = None
                    logger.info(f"   â†’ í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥: {len(extracted_text)} characters")

                # VectorDBì— ì„ë² ë”© ì €ì¥
                if extracted_text:
                    document_id = f"{doc_type}_{uuid.uuid4().hex[:12]}"
                    await rag.vectordb.add_document(
                        document_id=document_id,
                        text=extracted_text,
                        collection_type=doc_type,
                        metadata={
                            "user_id": request.user_id,
                            "file_id": doc_input.file_id,
                            "created_at": datetime.now().isoformat(),
                        },
                    )
                    safe_document_id = sanitize_log_input(document_id)
                    logger.info("   âœ… VectorDB ì €ì¥ ì™„ë£Œ: %s", safe_document_id)

                return DocumentExtractResult(
                    file_id=doc_input.file_id, extracted_text=extracted_text, pages=pages
                )

            # ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³  ê°ê° ì²˜ë¦¬
            resume_result = await extract_document(request.resume, "resume")
            job_posting_result = await extract_document(request.job_posting, "job_posting")

            # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (ëª…ì„¸ì„œ ìš”êµ¬ì‚¬í•­)
            logger.info("")
            logger.info("ğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
            try:
                analysis_result = await rag.llm.generate_analysis(
                    resume_text=resume_result.extracted_text,
                    posting_text=job_posting_result.extracted_text,
                    user_id=str(request.user_id),
                )
                logger.info("âœ… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e} (OCR í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜)")
                analysis_result = {
                    "resume_analysis": {"strengths": [], "weaknesses": [], "suggestions": []},
                    "posting_analysis": {
                        "company": "ì•Œ ìˆ˜ ì—†ìŒ",
                        "position": "ì•Œ ìˆ˜ ì—†ìŒ",
                        "required_skills": [],
                        "preferred_skills": [],
                    },
                }

            # ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ (íšŒì‚¬ëª…/ì±„ìš©ì§ë¬´)
            chat_title = ""
            try:
                logger.info("ğŸ“ ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì¤‘...")
                # ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸ (ì• 1000ìë§Œ)
                posting_text = job_posting_result.extracted_text[:1000]
                title_prompt = f"""{get_extract_title_prompt()}

## ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸
{posting_text}
"""
                # Geminië¡œ ì œëª© ì¶”ì¶œ
                title_response = ""
                async for chunk in rag.llm.generate_response(
                    user_message=title_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ì±„ìš©ê³µê³ ì—ì„œ íšŒì‚¬ëª…ê³¼ ì§ë¬´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤.",
                ):
                    title_response += chunk

                chat_title = title_response.strip()
                logger.info(f"âœ… ì±„íŒ…ë°© ì œëª©: {chat_title}")
            except Exception as e:
                logger.error(f"âŒ ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            logger.info("")

            # ê²°ê³¼ ì €ì¥ (ëª…ì„¸ì„œì— ë”°ë¥¸ ì‘ë‹µ êµ¬ì¡°)
            task_data = task_store.get(task_key) or {}
            task_data["status"] = TaskStatus.COMPLETED

            # formatted_text ìƒì„± (ë°±ì—”ë“œì—ì„œ ë°”ë¡œ í‘œì‹œìš©)
            formatted_text = format_analysis_text(
                resume_analysis=analysis_result.get("resume_analysis"),
                posting_analysis=analysis_result.get("posting_analysis"),
                summary=chat_title,
            )

            task_data["result"] = TextExtractResult(
                success=True,
                summary=chat_title or None,
                resume_ocr=resume_result.extracted_text,
                job_posting_ocr=job_posting_result.extracted_text,
                resume_analysis=analysis_result.get("resume_analysis"),
                posting_analysis=analysis_result.get("posting_analysis"),
                formatted_text=formatted_text,
            ).model_dump()
            task_store.save(task_key, task_data)

            logger.info("")
            logger.info("âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë¶„ì„ ì™„ë£Œ!")
            logger.info(f"   â†’ ì´ë ¥ì„œ OCR: {len(resume_result.extracted_text)}ì")
            logger.info(f"   â†’ ì±„ìš©ê³µê³  OCR: {len(job_posting_result.extracted_text)}ì")

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            task_data = task_store.get(task_key) or {}
            task_data["status"] = TaskStatus.FAILED
            task_data["error"] = {"code": ErrorCode.PROCESSING_ERROR, "message": str(e)}
            task_store.save(task_key, task_data)

    asyncio.create_task(process_text_extract())

    return AsyncTaskResponse(task_id=task_id, status=TaskStatus.PROCESSING)


# ============================================================================
# ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ
# ============================================================================


@router.get(
    "/task/{task_id}",
    response_model=TaskStatusResponse,
    summary="ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ",
    description="ë¹„ë™ê¸° ì²˜ë¦¬ ì‘ì—…ì˜ ìƒíƒœë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
    responses={
        200: {
            "description": "ì„±ê³µ",
            "content": {
                "application/json": {
                    "examples": {
                        "processing": {
                            "summary": "ì²˜ë¦¬ ì¤‘",
                            "value": {
                                "task_id": 32,
                                "status": "processing",
                                "progress": None,
                                "message": None,
                                "result": None,
                                "error": None,
                            },
                        },
                        "completed": {
                            "summary": "ì™„ë£Œ (text_extract)",
                            "value": {
                                "task_id": 32,
                                "status": "completed",
                                "progress": 100,
                                "message": None,
                                "result": {
                                    "success": True,
                                    "summary": "ì¹´ì¹´ì˜¤ | ë°±ì—”ë“œ ê°œë°œì",
                                    "resume_ocr": "ì´ë¦„: í™ê¸¸ë™\nê²½ë ¥: 3ë…„...",
                                    "job_posting_ocr": "ì¹´ì¹´ì˜¤ ë°±ì—”ë“œ ì±„ìš©\nìê²©ìš”ê±´: Java...",
                                    "resume_analysis": {
                                        "strengths": ["Java/Spring ìˆ™ë ¨ë„", "í”„ë¡œì íŠ¸ ê²½í—˜"],
                                        "weaknesses": ["í´ë¼ìš°ë“œ ê²½í—˜ ë¶€ì¡±"],
                                        "suggestions": ["AWS í•™ìŠµ ê¶Œì¥"],
                                    },
                                    "posting_analysis": {
                                        "company": "ì¹´ì¹´ì˜¤",
                                        "position": "ë°±ì—”ë“œ ê°œë°œì",
                                        "required_skills": ["Java", "Spring", "MySQL"],
                                        "preferred_skills": ["Docker", "Kubernetes"],
                                    },
                                    "formatted_text": "ì§€ì› íšŒì‚¬ ë° ì§ë¬´ : ì¹´ì¹´ì˜¤ | ë°±ì—”ë“œ ê°œë°œì\n\nì´ë ¥ì„œ ë¶„ì„\n\nì¥ì \n1. Java/Spring ìˆ™ë ¨ë„\n2. í”„ë¡œì íŠ¸ ê²½í—˜\n\në‹¨ì \n1. í´ë¼ìš°ë“œ ê²½í—˜ ë¶€ì¡±\n\nì±„ìš© ê³µê³  ë¶„ì„\n\nê¸°ì—… / í¬ì§€ì…˜\nì¹´ì¹´ì˜¤ / ë°±ì—”ë“œ ê°œë°œì\n\ní•„ìˆ˜ ì—­ëŸ‰\n- Java\n- Spring\n- MySQL\n\nìš°ëŒ€ ì‚¬í•­\n- Docker\n- Kubernetes",
                                    "room_id": 23,
                                },
                                "error": None,
                            },
                        },
                        "failed": {
                            "summary": "ì‹¤íŒ¨",
                            "value": {
                                "task_id": 32,
                                "status": "failed",
                                "progress": None,
                                "message": None,
                                "result": None,
                                "error": {
                                    "code": "OCR_FAILED",
                                    "message": "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                                },
                            },
                        },
                    }
                }
            },
        },
        401: {
            "description": "Unauthorized",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {"code": "UNAUTHORIZED", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤"}
                    }
                }
            },
        },
        404: {
            "description": "Not Found",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "TASK_NOT_FOUND",
                            "message": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: 12",
                        }
                    }
                }
            },
        },
    },
)
async def get_task_status(task_id: str):
    """í†µí•© ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ (text_extract, masking ë“±)"""
    task_key = str(task_id)  # ë¬¸ìì—´ í‚¤ë¡œ í†µì¼
    task = task_store.get(task_key)

    if task is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail={
                "code": ErrorCode.TASK_NOT_FOUND.value,
                "message": f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {task_id}",
            },
        )

    # room_idë¥¼ resultì— í¬í•¨
    result = task.get("result")
    if result and "room_id" not in result:
        result["room_id"] = task.get("room_id")

    return TaskStatusResponse(
        task_id=task_id,
        status=task["status"],
        progress=task.get("progress"),
        message=task.get("message"),
        result=result,
        error=task.get("error"),
    )


# ============================================================================
# API 2: ì±„íŒ… (í†µí•©: ëŒ€í™”/ë¶„ì„/ë©´ì ‘) (ìŠ¤íŠ¸ë¦¬ë°)
# ============================================================================


async def generate_chat_stream(request: ChatRequest):
    """ì±„íŒ… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""

    # contextì—ì„œ ëª¨ë“œ ê²°ì • (normal ë˜ëŠ” interview)
    mode = request.context.mode if request.context else ChatMode.NORMAL

    rag = get_services()
    newline = "\n"
    sse_end = "\n\n"

    # ëª¨ë¸ ì„ íƒ (gemini ë˜ëŠ” vllm)
    model = request.model.value if hasattr(request.model, "value") else str(request.model)
    logger.info("")
    logger.info(f"{'='*80}")
    logger.info("=== ğŸ’¬ ì±„íŒ… ìš”ì²­ ì‹œì‘ ===")
    logger.info(f"{'='*80}")
    logger.info(f"ğŸ“Œ ìš”ì²­ ëª¨ë¸: {model.upper()}")
    logger.info(f"ğŸ“Œ ì±„íŒ… ëª¨ë“œ: {mode}")
    logger.info(f"ğŸ“Œ ì‚¬ìš©ì ID: {request.user_id}")
    logger.info(f"ğŸ“Œ ì±„íŒ…ë°© ID: {request.room_id}")
    logger.info(f"ğŸ“Œ vLLM ì„œë¹„ìŠ¤: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if rag.vllm else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
    logger.info("")

    # 1. ì¼ë°˜ ëŒ€í™” (RAG í™œìš©)
    if mode == ChatMode.NORMAL:
        full_response = ""

        try:
            # íˆìŠ¤í† ë¦¬ ì—†ì´ ë‹¨ì¼ ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬ (ëª…ì„¸ì„œ ê¸°ì¤€)
            history_dict = []

            # Determine if this is an analysis request
            user_message = request.message or ""
            is_analysis = any(
                keyword in user_message for keyword in ["ë¶„ì„", "ë§¤ì¹­", "ì í•©", "í‰ê°€", "ë¹„êµ"]
            )

            # ë©´ì ‘ ëª¨ë“œ ì—¬ë¶€ í™•ì¸
            is_followup = (
                request.interview_id is not None and request.context.mode == ChatMode.INTERVIEW
            )

            if is_analysis:
                # ===================================================================
                # ë¶„ì„ ìš”ì²­: vLLMê³¼ Gemini ì™„ì „ ë¶„ë¦¬
                # ===================================================================
                # ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                logger.info("ğŸ” ë¶„ì„ ìš”ì²­ ê°ì§€")
                logger.info("")

                # ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ (íšŒì‚¬ëª…/ì±„ìš©ì§ë¬´)
                chat_title = ""
                try:
                    logger.info("ğŸ“ [0/3] ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì¤‘...")
                    # VectorDBì—ì„œ ì±„ìš©ê³µê³ ë§Œ ê°€ì ¸ì˜¤ê¸°
                    job_posting_docs = await rag.retrieve_all_documents(
                        user_id=request.user_id, context_types=["job_posting"]
                    )

                    if job_posting_docs:
                        # ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸ (ì• 1000ìë§Œ)
                        posting_text = job_posting_docs[:1000]
                        title_prompt = f"""{get_extract_title_prompt()}

## ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸
{posting_text}
"""
                        # Geminië¡œ ì œëª© ì¶”ì¶œ (ë¹ ë¥´ê³  ì •í™•)
                        title_response = ""
                        async for chunk in rag.llm.generate_response(
                            user_message=title_prompt,
                            context=None,
                            history=[],
                            system_prompt="ë‹¹ì‹ ì€ ì±„ìš©ê³µê³ ì—ì„œ íšŒì‚¬ëª…ê³¼ ì§ë¬´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤.",
                        ):
                            title_response += chunk

                        chat_title = title_response.strip()
                        logger.info(f"âœ… [0/3] ì±„íŒ…ë°© ì œëª©: {chat_title}")

                        # ì±„íŒ…ë°© ì œëª©ì„ SSEë¡œ ì „ì†¡
                        yield f"data: {json.dumps({'summary': chat_title}, ensure_ascii=False)}{sse_end}"
                    else:
                        logger.warning("âš ï¸ ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì œëª© ì¶”ì¶œ ìƒëµ")
                except Exception as e:
                    logger.error(f"âŒ ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                logger.info("")

                # ---------------------------------------------------------------
                # vLLM ëª¨ë“œ (ê°€ì„±ë¹„): EasyOCR â†’ VectorDB â†’ Llama ë¶„ì„
                # ---------------------------------------------------------------
                if model == "vllm" and rag.vllm:
                    logger.info("ğŸ’° [vLLM ê°€ì„±ë¹„ ëª¨ë“œ] ë¶„ì„ ì‹œì‘")
                    logger.info("   í”„ë¡œì„¸ìŠ¤: EasyOCR â†’ VectorDB ì €ì¥ â†’ VectorDB ì¡°íšŒ â†’ Llama ë¶„ì„")
                    logger.info("")

                    # 1. VectorDBì—ì„œ OCRë¡œ ì¶”ì¶œëœ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                    logger.info("ğŸ“‚ [1/3] VectorDBì—ì„œ ì—…ë¡œë“œëœ ë¬¸ì„œ ì¡°íšŒ ì¤‘...")
                    full_context = await rag.retrieve_all_documents(
                        user_id=request.user_id, context_types=["resume", "job_posting"]
                    )

                    if not full_context:
                        error_msg = "âŒ ì—…ë¡œë“œëœ ì´ë ¥ì„œ ë˜ëŠ” ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\në¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                        # ì‚¬ìš©ì IDëŠ” ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                        logger.error("âš ï¸ VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                        yield f"data: {json.dumps({'chunk': error_msg}, ensure_ascii=False)}{sse_end}"
                        full_response = error_msg
                    else:
                        logger.info(f"âœ… [1/3] VectorDB ì¡°íšŒ ì™„ë£Œ: {len(full_context)}ì")
                        logger.info("")

                        # 2. Llama ëª¨ë¸ë¡œ ë¶„ì„
                        logger.info("ğŸ¤– [2/3] Llama ëª¨ë¸ ë¶„ì„ ì‹œì‘...")
                        analysis_prompt = f"""ë‹¤ìŒ ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ë¥¼ ë¶„ì„í•˜ì—¬ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{full_context}

ì•„ë˜ í˜•ì‹ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”:

ì§€ì› íšŒì‚¬ ë° ì§ë¬´ : [íšŒì‚¬ëª…] | [ì§ë¬´ëª…]

ì´ë ¥ì„œ ë¶„ì„

ì¥ì 
1. [êµ¬ì²´ì ì¸ ì¥ì  1]
2. [êµ¬ì²´ì ì¸ ì¥ì  2]
3. [êµ¬ì²´ì ì¸ ì¥ì  3]
4. [êµ¬ì²´ì ì¸ ì¥ì  4]
5. [êµ¬ì²´ì ì¸ ì¥ì  5]

ë‹¨ì 
1. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  1]
2. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  2]
3. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  3]
4. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  4]
5. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  5]

ì±„ìš© ê³µê³  ë¶„ì„

ê¸°ì—… / í¬ì§€ì…˜
[íšŒì‚¬ëª…] / [í¬ì§€ì…˜ëª…]

í•„ìˆ˜ ì—­ëŸ‰
- [í•„ìˆ˜ ì—­ëŸ‰ 1]
- [í•„ìˆ˜ ì—­ëŸ‰ 2]
- [í•„ìˆ˜ ì—­ëŸ‰ 3]

ìš°ëŒ€ ì‚¬í•­
- [ìš°ëŒ€ ì‚¬í•­ 1]
- [ìš°ëŒ€ ì‚¬í•­ 2]
- [ìš°ëŒ€ ì‚¬í•­ 3]

ë§¤ì¹­ë„

ë‚˜ì™€ ì§€ì› ì§ë¬´ì— ë§ëŠ” ì 
- [ë§¤ì¹­ë˜ëŠ” ì—­ëŸ‰/ê²½í—˜ 1]
- [ë§¤ì¹­ë˜ëŠ” ì—­ëŸ‰/ê²½í—˜ 2]
- [ë§¤ì¹­ë˜ëŠ” ì—­ëŸ‰/ê²½í—˜ 3]

ë‚˜ì™€ ì§€ì› ì§ë¬´ì— ë§ì§€ ì•ŠëŠ” ì 
- [ë¶€ì¡±í•˜ê±°ë‚˜ ë³´ì™„ì´ í•„ìš”í•œ ì—­ëŸ‰ 1]
- [ë¶€ì¡±í•˜ê±°ë‚˜ ë³´ì™„ì´ í•„ìš”í•œ ì—­ëŸ‰ 2]
- [ë¶€ì¡±í•˜ê±°ë‚˜ ë³´ì™„ì´ í•„ìš”í•œ ì—­ëŸ‰ 3]

ìœ„ í˜•ì‹ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

ì ˆëŒ€ ê¸ˆì§€:
- # ## ### ì œëª© ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€
- ** __ ë³¼ë“œ/ì´íƒ¤ë¦­ ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€
- ``` ì½”ë“œ ë¸”ë¡ ì‚¬ìš© ê¸ˆì§€
- JSON í˜•ì‹ ì‚¬ìš© ê¸ˆì§€

ê·¸ëƒ¥ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì‘ì„±í•˜ì„¸ìš”."""

                        async for chunk in rag.vllm.generate_response(
                            user_message=analysis_prompt,
                            context=None,
                            history=[],
                            system_prompt="ë‹¹ì‹ ì€ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•(#, ##, **, ```)ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³  ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.",
                        ):
                            full_response += chunk
                            yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                        logger.info(f"âœ… [3/3] Llama ë¶„ì„ ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")

                # ---------------------------------------------------------------
                # Gemini ëª¨ë“œ (ê³ ì„±ëŠ¥): Gemini Vision APIë¡œ ì§ì ‘ íŒŒì¼ ì½ê³  ë¶„ì„
                # ---------------------------------------------------------------
                else:
                    if model == "vllm" and not rag.vllm:
                        logger.warning("âš ï¸ vLLM ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€ â†’ Geminië¡œ ìë™ ë³€ê²½")

                    logger.info("ğŸš€ [Gemini ê³ ì„±ëŠ¥ ëª¨ë“œ] ë¶„ì„ ì‹œì‘")
                    logger.info("   í”„ë¡œì„¸ìŠ¤: RAG ê²€ìƒ‰ â†’ Gemini ë¶„ì„ (ì›ë˜ ë°©ì‹)")
                    logger.info("")

                    # RAGë¡œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰í•˜ì—¬ Geminië¡œ ë¶„ì„
                    logger.info("ğŸ“‚ [1/2] RAG ê²€ìƒ‰ ì¤‘...")
                    async for chunk in rag.chat_with_rag(
                        user_message=user_message,
                        user_id=request.user_id,
                        history=history_dict,
                        use_rag=True,
                        context_types=["resume", "job_posting"],
                        model="gemini",
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                    logger.info(f"âœ… [2/2] Gemini ë¶„ì„ ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")
            else:
                # ===================================================================
                # ì¼ë°˜ ëŒ€í™”: RAG ê²€ìƒ‰ ì‚¬ìš©
                # ===================================================================
                logger.info("ğŸ’¬ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ")
                logger.info("")

                # ë©´ì ‘ ì„¸ì…˜ì—ì„œ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± (interview_idê°€ ìˆê³ , ì´ì „ ì§ˆë¬¸-ë‹µë³€ ìŒì´ ìˆëŠ” ê²½ìš°)
                if is_followup:
                    original_question = history_dict[-2].get("content", "")
                    candidate_answer = history_dict[-1].get("content", "")

                    logger.info("ğŸ” [ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±] ê°ì§€")
                    logger.info(f"   ì›ë³¸ ì§ˆë¬¸: {original_question[:50]}...")
                    logger.info(f"   ë‹µë³€: {candidate_answer[:50]}...")
                    logger.info("")

                    # ê°„ë‹¨í•œ STAR ë¶„ì„ (ì‹¤ì œë¡œëŠ” LLMìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
                    star_analysis = {
                        "situation": "unknown",
                        "task": "unknown",
                        "action": "unknown",
                        "result": "unknown",
                    }

                    # ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±
                    async for chunk in rag.generate_followup_question(
                        original_question=original_question,
                        candidate_answer=candidate_answer,
                        star_analysis=star_analysis,
                        model=model,
                        user_id=request.user_id,
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                    logger.info(f"âœ… [ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±] ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")

                else:
                    # ì¼ë°˜ ëŒ€í™” ë˜ëŠ” ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­
                    if (
                        "ë©´ì ‘ ì§ˆë¬¸" in user_message
                        or "ë©´ì ‘ì§ˆë¬¸" in user_message
                        or "ë©´ì ‘" in user_message
                    ):
                        # ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­ ì‹œ portfolio(ë©´ì ‘ ì§ˆë¬¸ ë°ì´í„°)ë§Œ ê²€ìƒ‰
                        context_types = ["portfolio"]
                        logger.info("ğŸ¯ ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­ ê°ì§€ â†’ portfolio ì»¬ë ‰ì…˜ë§Œ ê²€ìƒ‰")
                    else:
                        # ì¼ë°˜ ìš”ì²­ ì‹œ ëª¨ë“  ì»¬ë ‰ì…˜ ê²€ìƒ‰
                        context_types = ["resume", "job_posting", "portfolio"]
                        logger.info("ğŸ“š ì¼ë°˜ ëŒ€í™” â†’ ëª¨ë“  ì»¬ë ‰ì…˜ ê²€ìƒ‰")

                    logger.info("")

                    # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±
                    # ëª¨ë¸ëª…ì€ ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                    logger.info("ğŸ” RAG ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì‹œì‘...")
                    async for chunk in rag.chat_with_rag(
                        user_message=user_message,
                        user_id=request.user_id,
                        history=history_dict,
                        use_rag=True,  # RAG í™œì„±í™”
                        context_types=context_types,
                        model=model,
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                    # ëª¨ë¸ëª…ì€ ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                    logger.info("âœ… ì¼ë°˜ ëŒ€í™” ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: %dì)", len(full_response))

        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            yield f"data: {json.dumps({'chunk': error_msg}, ensure_ascii=False)}{sse_end}"
            full_response = error_msg

        yield f"data: [DONE]{sse_end}"

    # 2. ë©´ì ‘ ëª¨ë“œ - ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„± ë° ëŒ€í™”
    elif mode == ChatMode.INTERVIEW:
        try:
            # ë©´ì ‘ íƒ€ì…ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì¡°ì • (behavior: ì¸ì„±, tech: ê¸°ìˆ )
            interview_type = request.context.interview_type or "tech"
            interview_type_kr = "ê¸°ìˆ " if interview_type == "tech" else "ì¸ì„±"

            # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ë§ì¶¤ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
            # resume, portfolio, job_posting ì»¬ë ‰ì…˜ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            context = await rag.retrieve_context(
                query=f"{interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ì„ ìœ„í•œ ì‚¬ìš©ì ì •ë³´",
                user_id=request.user_id,
                context_types=["resume", "portfolio", "job_posting"],
                n_results=1,  # ì†ë„ ê°œì„ ì„ ìœ„í•´ 1ê°œë§Œ ê²€ìƒ‰
            )

            # ë©´ì ‘ ì§ˆë¬¸ ìƒì„± í”„ë¡¬í”„íŠ¸ (prompts ëª¨ë“ˆ ì‚¬ìš©)
            # contextì—ì„œ resume_ocr, job_posting_ocr ì‚¬ìš©
            resume_ocr = request.context.resume_ocr if request.context else None
            job_posting_ocr = request.context.job_posting_ocr if request.context else None

            if resume_ocr or job_posting_ocr or context:
                # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±
                question_prompt = create_interview_question_prompt(
                    resume_text=resume_ocr or context or "ì •ë³´ ì—†ìŒ",
                    job_posting_text=job_posting_ocr or "ì •ë³´ ì—†ìŒ",
                    interview_type=interview_type,
                )
            else:
                question_prompt = f"ì¼ë°˜ì ì¸ {interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ 1ê°œë¥¼ ì§§ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”:"

            full_question = ""

            # vLLM ë˜ëŠ” Gemini ì„ íƒ
            model_choice = (
                request.model.value if hasattr(request.model, "value") else str(request.model)
            )

            if model_choice == "vllm" and rag.vllm:
                logger.info("ğŸ’¬ [vLLM] ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.vllm.generate_response(
                    user_message=question_prompt,
                    context=None,
                    history=[],
                    system_prompt=SYSTEM_INTERVIEW,
                ):
                    full_question += chunk
                    yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"
            else:
                logger.info("ğŸ’¬ [Gemini] ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.llm.generate_response(
                    user_message=question_prompt,
                    context=None,
                    history=[],
                    system_prompt=SYSTEM_INTERVIEW,
                    user_id=request.user_id,
                ):
                    full_question += chunk
                    yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

            {
                "success": True,
                "mode": "interview_question",
                "response": full_question.strip(),
                "interview_type": interview_type,
            }
            yield f"data: [DONE]{sse_end}"

        except Exception as e:
            logger.error(f"Interview question generation error: {e}")
            {"success": False, "mode": "interview", "error": str(e)}
            yield f"data: [DONE]{sse_end}"

    # 3. ë¦¬í¬íŠ¸ ëª¨ë“œ - ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
    elif mode == ChatMode.REPORT:
        try:
            interview_type = request.context.interview_type or "tech"
            interview_type_kr = "ê¸°ìˆ " if interview_type == "tech" else "ì¸ì„±"
            qa_list = request.context.qa_list or []

            if not qa_list:
                yield f"data: [DONE]{sse_end}"
                return

            content = f"{interview_type_kr} ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...{newline}"
            yield f"data: {json.dumps({'chunk': content}, ensure_ascii=False)}{sse_end}"

            # Q&A ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            qa_text = ""
            for i, qa in enumerate(qa_list, 1):
                q = qa.get("question", "")
                a = qa.get("answer", "")
                qa_text += f"ì§ˆë¬¸ {i}: {q}\në‹µë³€ {i}: {a}\n\n"

            # í‰ê°€ ë¦¬í¬íŠ¸ í”„ë¡¬í”„íŠ¸
            report_prompt = f"""
ë‹¤ìŒì€ {interview_type_kr} ë©´ì ‘ Q&A ê¸°ë¡ì…ë‹ˆë‹¤:

{qa_text}

ìœ„ ë©´ì ‘ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ê° ë‹µë³€ì— ëŒ€í•œ ê°œë³„ í‰ê°€ (ì˜í•œ ì , ê°œì„ ì )
2. ì „ì²´ì ì¸ ê°•ì  íŒ¨í„´
3. ì „ì²´ì ì¸ ì•½ì  íŒ¨í„´
4. í–¥í›„ í•™ìŠµ ê°€ì´ë“œ
"""

            full_report = ""

            # vLLM ë˜ëŠ” Gemini ì„ íƒ
            model_choice = (
                request.model.value if hasattr(request.model, "value") else str(request.model)
            )

            if model_choice == "vllm" and rag.vllm:
                logger.info("ğŸ“Š [vLLM] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.vllm.generate_response(
                    user_message=report_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
                ):
                    full_report += chunk
                    yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"
            else:
                logger.info("ğŸ“Š [Gemini] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.llm.generate_response(
                    user_message=report_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
                    user_id=request.user_id,
                ):
                    full_report += chunk
                    yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

            {
                "success": True,
                "mode": "report",
                "response": full_report.strip(),
                "interview_type": interview_type,
                "interview_id": request.interview_id,
            }
            yield f"data: [DONE]{sse_end}"

        except Exception as e:
            logger.error(f"Interview report generation error: {e}")
            {"success": False, "mode": "report", "error": str(e)}
            yield f"data: [DONE]{sse_end}"


@router.post(
    "/chat",
    summary="ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° (ì¼ë°˜/ë©´ì ‘/ë¦¬í¬íŠ¸)",
    description="""
    ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    **ì²˜ë¦¬ ë°©ì‹:** ìŠ¤íŠ¸ë¦¬ë° (SSE)

    **ëª¨ë“œ:**
    - normal: ì¼ë°˜ ëŒ€í™”
    - interview: ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
    - report: ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±

    **ë©´ì ‘ íƒ€ì… (context.interview_type):**
    - behavior: ì¸ì„± ë©´ì ‘
    - tech: ê¸°ìˆ  ë©´ì ‘
    """,
    responses={
        400: {
            "description": "Bad Request",
            "content": {
                "application/json": {
                    "examples": {
                        "invalid_request": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_REQUEST",
                                    "message": "room_idëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤",
                                    "field": "room_id",
                                }
                            }
                        },
                        "invalid_mode": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_MODE",
                                    "message": "modeëŠ” normal ë˜ëŠ” interview ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤",
                                    "field": "context.mode",
                                }
                            }
                        },
                        "invalid_interview_type": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_INTERVIEW_TYPE",
                                    "message": "interview_typeì€ behavior ë˜ëŠ” techë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                                    "field": "context.interview_type",
                                }
                            }
                        },
                    }
                }
            },
        },
        401: {
            "description": "Unauthorized",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {"code": "UNAUTHORIZED", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤"}
                    }
                }
            },
        },
        422: {
            "description": "Unprocessable Entity",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "MISSING_CONTEXT",
                            "message": "ë©´ì ‘ ëª¨ë“œ ì‹œ context.resume_ocr ë˜ëŠ” context.job_posting_ocrì´ í•„ìš”í•©ë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        429: {
            "description": "Too Many Requests",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "RATE_LIMIT_EXCEEDED",
                            "message": "ìš”ì²­ í•œë„ ì´ˆê³¼. 1ë¶„ í›„ ì¬ì‹œë„í•˜ì„¸ìš”",
                        }
                    }
                }
            },
        },
        500: {
            "description": "Internal Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "INTERNAL_ERROR",
                            "message": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        503: {
            "description": "Service Unavailable",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "LLM_UNAVAILABLE",
                            "message": "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
    },
)
async def chat(request: ChatRequest):
    """ì±„íŒ… ì²˜ë¦¬ (ì¼ë°˜/ë©´ì ‘)"""
    return StreamingResponse(
        generate_chat_stream(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        },
    )


# ============================================================================
# API 3: ìº˜ë¦°ë” ì¼ì • íŒŒì‹± (ë™ê¸°)
# ============================================================================


@router.post(
    "/calendar/parse",
    response_model=CalendarParseResponse,
    summary="ìº˜ë¦°ë” ì¼ì • ì •ë³´ íŒŒì‹±",
    description="""
    ìº˜ë¦°ë” ëª¨ë‹¬ì—ì„œ íŒŒì¼/í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (í¼ ìë™ ì±„ìš°ê¸°ìš©).

    **ì²˜ë¦¬ ë°©ì‹:** ë™ê¸° - ê°„ë‹¨í•œ íŒŒì‹± ì‘ì—…

    **Pydantic AI ì‚¬ìš©:** CalendarParseResult

    **ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
    - ëª¨ë‹¬ì—ì„œ ì±„ìš©ê³µê³  íŒŒì¼/í…ìŠ¤íŠ¸ ì²¨ë¶€ â†’ ì¼ì • ì •ë³´ ì¶”ì¶œ
    - Frontendê°€ ëª¨ë‹¬ í¼ì— ìë™ ì±„ì›Œë„£ìŒ
    - ì‚¬ìš©ì í™•ì¸/ìˆ˜ì • â†’ ì €ì¥ â†’ Backendê°€ Google Calendarì— ì¶”ê°€
    """,
)
async def calendar_parse(request: CalendarParseRequest):  # noqa: ARG001
    """ìº˜ë¦°ë” ì¼ì • íŒŒì‹±"""
    # validatorì—ì„œ ì´ë¯¸ ê²€ì¦ë¨

    # Pydantic AIë¡œ êµ¬ì¡°í™”ëœ ê²°ê³¼ ë°˜í™˜
    return CalendarParseResponse(
        success=True,
        company="ì¹´ì¹´ì˜¤",
        position="ë°±ì—”ë“œ ê°œë°œì",
        schedules=[
            {"stage": "ì„œë¥˜ ë§ˆê°", "date": "2026-01-15", "time": None},
            {"stage": "ì½”ë”©í…ŒìŠ¤íŠ¸", "date": "2026-01-20", "time": "14:00"},
            {"stage": "1ì°¨ ë©´ì ‘", "date": "2026-01-25", "time": None},
        ],
        hashtags=["#ì¹´ì¹´ì˜¤", "#ë°±ì—”ë“œ", "#ì‹ ì…"],
    )


# ============================================================================
# API 4: ê²Œì‹œíŒ ì²¨ë¶€íŒŒì¼ ë§ˆìŠ¤í‚¹ (ë¹„ë™ê¸°)
# ============================================================================
# ì´ APIëŠ” app/api/routes/masking.pyë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.
# masking.pyì—ì„œ íŒŒì¼ ê¸°ë°˜ ì €ì¥ì†Œì™€ ì‹¤ì œ Gemini APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
