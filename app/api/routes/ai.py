import asyncio
import json
import logging
import os
import time
import uuid
from datetime import datetime

from fastapi import APIRouter, Header, HTTPException, status
from fastapi.responses import StreamingResponse

from app.prompts import (
    create_tech_followup_prompt,
    create_tech_interview_init_prompt,
    format_conversation_history,
    get_extract_title_prompt,
    get_opening_prompt,
    # ê¸°ìˆ  ë©´ì ‘ 5ë‹¨ê³„ í”„ë¡¬í”„íŠ¸
    get_system_tech_interview,
)
from app.schemas.calendar import CalendarParseRequest, CalendarParseResponse
from app.schemas.chat import (
    ChatMode,
    ChatRequest,
    InterviewQuestionState,
    InterviewSession,
)
from app.schemas.common import AsyncTaskResponse, ErrorCode, TaskStatus, TaskStatusResponse
from app.schemas.text_extract import (
    DocumentExtractResult,
    DocumentInput,
    PageText,
    TextExtractRequest,
    TextExtractResult,
)
from app.services.cloudwatch_service import CloudWatchService
from app.services.llm_service import LLMService
from app.services.rag_service import RAGService
from app.services.vectordb_service import VectorDBService
from app.services.vllm_service import VLLMService
from app.utils.log_sanitizer import safe_info, safe_warning, sanitize_log_input
from app.utils.prompt_guard import RiskLevel, check_prompt_injection
from app.utils.task_store import get_task_store

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/ai",
    tags=["AI APIs (v3.0)"],
    responses={404: {"description": "Not found"}},
)

# í†µí•© ì‘ì—… ì €ì¥ì†Œ (íŒŒì¼ ê¸°ë°˜, ì„œë²„ ì¬ì‹œì‘ ì‹œì—ë„ ìœ ì§€)
# ë°±ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ task_idë¥¼ í‚¤ë¡œ ì‚¬ìš©
task_store = get_task_store()


# ============================================================================
# ë©´ì ‘ ì„¸ì…˜ ìºì‹œ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
# - Spring ë°±ì—”ë“œê°€ ì„¸ì…˜ ìƒíƒœë¥¼ ì „ë‹¬í•˜ì§€ ì•ŠëŠ” ë¬¸ì œì˜ ì„ì‹œ í•´ê²°ì±…
# - ìš´ì˜ í™˜ê²½ì—ì„œëŠ” Redis ê¶Œì¥
# ============================================================================
interview_sessions: dict[str, InterviewSession] = {}


def get_session_key(user_id: int, interview_id: int | None) -> str:
    """ë©´ì ‘ ì„¸ì…˜ ìºì‹œ í‚¤ ìƒì„±"""
    return f"interview:{user_id}:{interview_id or 'default'}"


# Initialize services
llm_service = None
vllm_service = None
vectordb_service = None
rag_service = None


def get_services():
    """Get or initialize AI services"""
    global llm_service, vllm_service, vectordb_service, rag_service

    if llm_service is None:
        api_key = os.getenv("GOOGLE_API_KEY")
        llm_service = LLMService(api_key=api_key)
        vectordb_service = VectorDBService(api_key=api_key)

        # Initialize vLLM service (GCP GPU server)
        gcp_vllm_url = os.getenv("GCP_VLLM_BASE_URL")

        try:
            if gcp_vllm_url:
                logger.info(f"ğŸŒ GCP vLLM ì„œë²„ ì—°ê²°: {gcp_vllm_url}")
                vllm_service = VLLMService()
                logger.info("âœ… vLLM service initialized (GCP GPU server)")
            else:
                # GCP URL ì—†ìœ¼ë©´ OCR ì „ìš© ëª¨ë“œ
                logger.info("ğŸ’° GCP URL ì—†ìŒ - OCR ì „ìš© ëª¨ë“œë¡œ ì´ˆê¸°í™”")
                vllm_service = VLLMService(ocr_only=True)
                logger.info("âœ… vLLM service initialized (OCR-only mode)")
        except Exception as e:
            logger.warning(f"vLLM service initialization failed: {e}")
            vllm_service = None

        rag_service = RAGService(llm_service, vectordb_service, vllm_service)

    return rag_service


def format_analysis_text(
    resume_analysis: dict | None,
    posting_analysis: dict | None,
    summary: str | None,
) -> str:
    """ë¶„ì„ ê²°ê³¼ë¥¼ plain textë¡œ í¬ë§·íŒ… (ë§ˆí¬ë‹¤ìš´ ì—†ì´)

    ë°±ì—”ë“œì—ì„œ ë°”ë¡œ í™”ë©´ì— í‘œì‹œí•  ìˆ˜ ìˆë„ë¡ í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    lines = []

    # íšŒì‚¬/ì§ë¬´
    if summary:
        lines.append(f"ì§€ì› íšŒì‚¬ ë° ì§ë¬´ : {summary}")
        lines.append("")

    # ì´ë ¥ì„œ ë¶„ì„
    if resume_analysis:
        lines.append("ì´ë ¥ì„œ ë¶„ì„")
        lines.append("")
        lines.append("ì¥ì ")
        strengths = resume_analysis.get("strengths", [])
        for i, strength in enumerate(strengths[:5], 1):
            lines.append(f"{i}. {strength}")
        lines.append("")
        lines.append("ë‹¨ì ")
        weaknesses = resume_analysis.get("weaknesses", [])
        for i, weakness in enumerate(weaknesses[:5], 1):
            lines.append(f"{i}. {weakness}")
        lines.append("")

    # ì±„ìš©ê³µê³  ë¶„ì„
    if posting_analysis:
        lines.append("ì±„ìš© ê³µê³  ë¶„ì„")
        lines.append("")
        company = posting_analysis.get("company", "")
        position = posting_analysis.get("position", "")
        lines.append("ê¸°ì—… / í¬ì§€ì…˜")
        lines.append(f"{company} / {position}")
        lines.append("")
        lines.append("í•„ìˆ˜ ì—­ëŸ‰")
        for skill in posting_analysis.get("required_skills", [])[:5]:
            lines.append(f"- {skill}")
        lines.append("")
        lines.append("ìš°ëŒ€ ì‚¬í•­")
        for skill in posting_analysis.get("preferred_skills", [])[:5]:
            lines.append(f"- {skill}")
        lines.append("")

    # ë§¤ì¹­ë„
    if resume_analysis and posting_analysis:
        lines.append("ë§¤ì¹­ë„")
        lines.append("")
        lines.append("ë‚˜ì™€ ì§€ì› ì§ë¬´ì— ë§ëŠ” ì ")
        # matches í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ strengthsì—ì„œ ê°€ì ¸ì˜´
        matches = resume_analysis.get("matches", resume_analysis.get("strengths", [])[:3])
        for match in matches[:3] if matches else []:
            lines.append(f"- {match}")
        lines.append("")
        lines.append("ë‚˜ì™€ ì§€ì› ì§ë¬´ì— ë§ì§€ ì•ŠëŠ” ì ")
        # gaps í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ weaknessesì—ì„œ ê°€ì ¸ì˜´
        gaps = resume_analysis.get("gaps", resume_analysis.get("weaknesses", [])[:3])
        for gap in gaps[:3] if gaps else []:
            lines.append(f"- {gap}")

    return "\n".join(lines)


async def verify_api_key(x_api_key: str | None = Header(None)):
    """API í‚¤ ê²€ì¦"""
    # ì‹¤ì œë¡œëŠ” í™˜ê²½ë³€ìˆ˜ë‚˜ DBì—ì„œ í™•ì¸
    if x_api_key != "your-api-key-here":
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid API key")
    return x_api_key


# ============================================================================
# API 1: í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© (í†µí•©) (ë¹„ë™ê¸°)
# ============================================================================


@router.post(
    "/text/extract",
    response_model=AsyncTaskResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© ì €ì¥ (ì´ë ¥ì„œ + ì±„ìš©ê³µê³ )",
    description="""
    ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë‚´ë¶€ì—ì„œ ì„ë² ë”©ê¹Œì§€ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    **ìš”ì²­ êµ¬ì¡°:**
    - `resume`: ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤ ì…ë ¥ (í•„ìˆ˜)
    - `job_posting`: ì±„ìš©ê³µê³  ì…ë ¥ (í•„ìˆ˜)
    - ê° ë¬¸ì„œëŠ” íŒŒì¼ ì—…ë¡œë“œ(`s3_key` + `file_type`) ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥(`text`) ì¤‘ í•˜ë‚˜ ì„ íƒ

    **ì²˜ë¦¬ ë°©ì‹:** ë¹„ë™ê¸° (task_id ë°˜í™˜ â†’ í´ë§ í•„ìš”)

    **ë‚´ë¶€ ì²˜ë¦¬ íë¦„:**
    1. ì´ë ¥ì„œ ì²˜ë¦¬: íŒŒì¼ì´ë©´ OCR/VLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ, í…ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    2. ì±„ìš©ê³µê³  ì²˜ë¦¬: íŒŒì¼ì´ë©´ OCR/VLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ, í…ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    3. ê° í…ìŠ¤íŠ¸ ì²­í‚¹ (500 tokens, 50 overlap)
    4. Gemini Embedding ìƒì„±
    5. VectorDBì— ì €ì¥ (resume, job_posting ì»¬ë ‰ì…˜)
    6. ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (ì´ë ¥ì„œ/ì±„ìš©ê³µê³  ë¶„ì„)
    7. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ + ë¶„ì„ ê²°ê³¼ ë°˜í™˜
    """,
    responses={
        400: {
            "description": "Bad Request",
            "content": {
                "application/json": {
                    "examples": {
                        "invalid_request": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_REQUEST",
                                    "message": "resumeê³¼ job_posting ëŠ” í•„ìˆ˜ ì…ë ¥í•´ì•¼í•©ë‹ˆë‹¤",
                                }
                            }
                        },
                        "invalid_file_type": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_FILE_TYPE",
                                    "message": "file_typeì€ pdf ë˜ëŠ” imageë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                                    "field": "resume.file_type",
                                }
                            }
                        },
                        "invalid_document": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_DOCUMENT",
                                    "message": "s3_key ë˜ëŠ” text ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤",
                                    "field": "resume",
                                }
                            }
                        },
                    }
                }
            },
        },
        401: {
            "description": "Unauthorized",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {"code": "UNAUTHORIZED", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤"}
                    }
                }
            },
        },
        404: {
            "description": "Not Found",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "FILE_NOT_FOUND",
                            "message": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: users/12/resume/abc123.pdf",
                        }
                    }
                }
            },
        },
        422: {
            "description": "Unprocessable Entity",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "OCR_FAILED",
                            "message": "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        429: {
            "description": "Too Many Requests",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "RATE_LIMIT_EXCEEDED",
                            "message": "ìš”ì²­ í•œë„ ì´ˆê³¼. 1ë¶„ í›„ ì¬ì‹œë„í•˜ì„¸ìš”",
                        }
                    }
                }
            },
        },
        500: {
            "description": "Internal Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "INTERNAL_ERROR",
                            "message": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        503: {
            "description": "Service Unavailable",
            "content": {
                "application/json": {
                    "examples": {
                        "llm_unavailable": {
                            "value": {
                                "detail": {
                                    "code": "LLM_UNAVAILABLE",
                                    "message": "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                                }
                            }
                        },
                        "s3_unavailable": {
                            "value": {
                                "detail": {
                                    "code": "S3_UNAVAILABLE",
                                    "message": "íŒŒì¼ ìŠ¤í† ë¦¬ì§€ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                                }
                            }
                        },
                    }
                }
            },
        },
    },
)
async def text_extract(request: TextExtractRequest):
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© ì €ì¥ (í†µí•©) - ì´ë ¥ì„œ + ì±„ìš©ê³µê³ """
    task_id = request.task_id  # ë°±ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ task_id ì‚¬ìš©

    # ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ì „ì†¡
    try:
        cw = CloudWatchService.get_instance()
        # fire-and-forget (await ì•ˆí•¨, ë°°ê²½ ì‹¤í–‰)
        asyncio.create_task(cw.put_metric("AI_Job_Count", 1, "Count", {"Type": "text_extract"}))
    except Exception:
        pass

    # ë¹„ë™ê¸° ì‘ì—… ì‹œì‘ (í†µí•© task_store ì‚¬ìš©)
    task_key = str(task_id)  # íŒŒì¼ ê¸°ë°˜ ì €ì¥ì†ŒëŠ” ë¬¸ìì—´ í‚¤ ì‚¬ìš©
    task_store.save(
        task_key,
        {
            "type": "text_extract",
            "status": TaskStatus.PROCESSING,
            "created_at": datetime.now(),
            "room_id": request.room_id,
            "request": request.model_dump(),
        },
    )

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
    async def process_text_extract():
        try:
            rag = get_services()

            # ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ê°’: gemini)
            model = request.model if hasattr(request, "model") and request.model else "gemini"
            logger.info("")
            logger.info(f"{'='*80}")
            logger.info("=== ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘ (ì´ë ¥ì„œ + ì±„ìš©ê³µê³ ) ===")
            logger.info(f"{'='*80}")
            logger.info(
                f"ğŸ“Œ OCR ì „ëµ: {'GEMINI (V1 Temporary)' if model == 'auto' else model.upper()}"
            )
            safe_info(logger, "ğŸ“Œ ì‚¬ìš©ì ID: %s", request.user_id)
            logger.info(f"ğŸ“Œ vLLM ì„œë¹„ìŠ¤: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if rag.vllm else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
            logger.info("")

            async def extract_document(
                doc_input: DocumentInput, doc_type: str
            ) -> DocumentExtractResult:
                """ë¬¸ì„œ ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜"""
                logger.info(f"ğŸ“„ [{doc_type.upper()}] ì²˜ë¦¬ ì‹œì‘")

                # íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹
                if doc_input.s3_key:
                    # MIME íƒ€ì…ì„ ë‹¨ìˆœ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (pdf/image)
                    file_type = doc_input.get_file_type_simple() or "pdf"
                    logger.info(f"   â†’ íŒŒì¼ íƒ€ì… (MIME): {doc_input.file_type}")
                    logger.info(f"   â†’ íŒŒì¼ íƒ€ì… (ë‹¨ìˆœ): {file_type}")
                    safe_s3_key = sanitize_log_input(doc_input.s3_key)
                    logger.info("   â†’ S3 í‚¤: %s", safe_s3_key)

                    # OCRService: CLOVA OCR ìš°ì„  â†’ Gemini Fallback (EasyOCRëŠ” GPU ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒìœ¼ë¡œ ë¯¸ì‚¬ìš©)
                    logger.info("   ğŸ” [OCRService] CLOVA OCR ìš°ì„  â†’ Gemini Fallback ì‹œì‘")
                    ocr_result = await rag.ocr.extract_text(
                        file_url=str(doc_input.s3_key),
                        file_type=file_type,
                        user_id=str(request.user_id),
                        fallback_enabled=True,
                    )
                    ocr_engine = ocr_result.get("ocr_engine") or "gemini"
                    fallback_reason = ocr_result.get("fallback_reason")
                    extracted_text = ocr_result.get("extracted_text", "")
                    pages = [PageText(**page) for page in ocr_result.get("pages", [])]

                    if fallback_reason:
                        logger.info(
                            f"   âœ… [{ocr_engine.upper()} OCR] ì¶”ì¶œ ì™„ë£Œ (í´ë°± ì‚¬ìœ : {fallback_reason}): "
                            f"{len(extracted_text)}ì (í˜ì´ì§€: {len(pages)})"
                        )
                    else:
                        logger.info(
                            f"   âœ… [{ocr_engine.upper()} OCR] ì¶”ì¶œ ì™„ë£Œ: "
                            f"{len(extracted_text)}ì (í˜ì´ì§€: {len(pages)})"
                        )

                # í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥
                else:
                    extracted_text = doc_input.text or ""
                    pages = None
                    logger.info(f"   â†’ í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥: {len(extracted_text)} characters")

                # VectorDBì— ì„ë² ë”© ì €ì¥
                if extracted_text:
                    document_id = f"{doc_type}_{uuid.uuid4().hex[:12]}"
                    await rag.vectordb.add_document(
                        document_id=document_id,
                        text=extracted_text,
                        collection_type=doc_type,
                        metadata={
                            "user_id": request.user_id,
                            "file_id": doc_input.file_id,
                            "created_at": datetime.now().isoformat(),
                        },
                    )
                    safe_document_id = sanitize_log_input(document_id)
                    logger.info("   âœ… VectorDB ì €ì¥ ì™„ë£Œ: %s", safe_document_id)

                return DocumentExtractResult(
                    file_id=doc_input.file_id, extracted_text=extracted_text, pages=pages
                )

            # ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³  ê°ê° ì²˜ë¦¬
            resume_result = await extract_document(request.resume, "resume")
            job_posting_result = await extract_document(request.job_posting, "job_posting")

            # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (ëª…ì„¸ì„œ ìš”êµ¬ì‚¬í•­)
            logger.info("")
            logger.info("ğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
            analysis_failed = False
            try:
                analysis_result = await rag.llm.generate_analysis(
                    resume_text=resume_result.extracted_text,
                    posting_text=job_posting_result.extracted_text,
                    user_id=str(request.user_id),
                )
                logger.info("âœ… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                analysis_failed = True
                logger.warning(
                    "âš ï¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ (ì˜¤í”„ë‹ ë©”ì‹œì§€ì— ë¶„ì„ ë‚´ìš©ì´ ë¹„ì–´ ë³´ì¼ ìˆ˜ ìˆìŒ): %s",
                    e,
                    exc_info=True,
                )
                analysis_result = {
                    "resume_analysis": {"strengths": [], "weaknesses": [], "suggestions": []},
                    "posting_analysis": {
                        "company": "ì•Œ ìˆ˜ ì—†ìŒ",
                        "position": "ì•Œ ìˆ˜ ì—†ìŒ",
                        "required_skills": [],
                        "preferred_skills": [],
                    },
                    "matching": {
                        "score": 0,
                        "grade": "F",
                        "matched_skills": [],
                        "missing_skills": [],
                    },
                }

            # ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ (íšŒì‚¬ëª…/ì±„ìš©ì§ë¬´)
            chat_title = ""
            try:
                logger.info("ğŸ“ ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì¤‘...")
                # ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸ (ì• 1000ìë§Œ)
                posting_text = job_posting_result.extracted_text[:1000]
                title_prompt = f"""{get_extract_title_prompt()}

## ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸
{posting_text}
"""
                # Geminië¡œ ì œëª© ì¶”ì¶œ
                title_response = ""
                async for chunk in rag.llm.generate_response(
                    user_message=title_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ì±„ìš©ê³µê³ ì—ì„œ íšŒì‚¬ëª…ê³¼ ì§ë¬´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤.",
                ):
                    title_response += chunk

                chat_title = title_response.strip()
                logger.info(f"âœ… ì±„íŒ…ë°© ì œëª©: {chat_title}")
            except Exception as e:
                logger.error(f"âŒ ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            logger.info("")

            # ê²°ê³¼ ì €ì¥ (ëª…ì„¸ì„œì— ë”°ë¥¸ ì‘ë‹µ êµ¬ì¡°)
            task_data = task_store.get(task_key) or {}
            task_data["status"] = TaskStatus.COMPLETED

            # formatted_text ìƒì„± (ë°±ì—”ë“œì—ì„œ ë°”ë¡œ í‘œì‹œìš©)
            formatted_text = format_analysis_text(
                resume_analysis=analysis_result.get("resume_analysis"),
                posting_analysis=analysis_result.get("posting_analysis"),
                summary=chat_title,
            )
            formatted_text = formatted_text or "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            if analysis_failed:
                formatted_text += (
                    "\n\n(ìƒì„¸ ë¶„ì„ì´ ì¼ì‹œì ìœ¼ë¡œ ë°˜ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                    "ì´ë ¥ì„œÂ·ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸ëŠ” ì €ì¥ë˜ì—ˆìœ¼ë‹ˆ, ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.)"
                )

            # ì˜¤í”„ë‹ ë©”ì‹œì§€ ìƒì„± (Gemini) - ëŒ€í™” ì‹œì‘ìš©
            logger.info("ğŸ¤– ì˜¤í”„ë‹ ë©”ì‹œì§€ ìƒì„± ì‹œì‘...")
            ai_message = ""
            try:
                opening_prompt = get_opening_prompt(formatted_text)
                # Geminië¡œ ì˜¤í”„ë‹ ìƒì„±
                async for chunk in rag.llm.generate_response(
                    user_message=opening_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ë„ì›€ì„ ì£¼ëŠ” ì¹œì ˆí•œ ì·¨ì—… ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                ):
                    ai_message += chunk
                logger.info("âœ… ì˜¤í”„ë‹ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ì˜¤í”„ë‹ ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”ì‹œì§€
                ai_message = f"ì•ˆë…•í•˜ì„¸ìš”! ì§€ì›í•˜ì‹  {chat_title or 'ì§ë¬´'}ì— ëŒ€í•œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ í™•ì¸í•˜ì‹œê³  ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“  ë¬¼ì–´ë´ì£¼ì„¸ìš”!"

            task_data["result"] = TextExtractResult(
                success=True,
                summary=chat_title or None,
                resume_ocr=resume_result.extracted_text,
                job_posting_ocr=job_posting_result.extracted_text,
                resume_analysis=analysis_result.get("resume_analysis"),
                posting_analysis=analysis_result.get("posting_analysis"),
                formatted_text=formatted_text,
                ai_message=ai_message,
            ).model_dump()
            task_store.save(task_key, task_data)

            logger.info("")
            logger.info("âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë¶„ì„ ì™„ë£Œ!")
            logger.info(f"   â†’ ì´ë ¥ì„œ OCR: {len(resume_result.extracted_text)}ì")
            logger.info(f"   â†’ ì±„ìš©ê³µê³  OCR: {len(job_posting_result.extracted_text)}ì")

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            task_data = task_store.get(task_key) or {}
            task_data["status"] = TaskStatus.FAILED
            task_data["error"] = {"code": ErrorCode.PROCESSING_ERROR, "message": str(e)}
            task_store.save(task_key, task_data)

    asyncio.create_task(process_text_extract())

    return AsyncTaskResponse(task_id=task_id, status=TaskStatus.PROCESSING)


# ============================================================================
# ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ
# ============================================================================


@router.get(
    "/task/{task_id}",
    response_model=TaskStatusResponse,
    summary="ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ",
    description="ë¹„ë™ê¸° ì²˜ë¦¬ ì‘ì—…ì˜ ìƒíƒœë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
    responses={
        200: {
            "description": "ì„±ê³µ",
            "content": {
                "application/json": {
                    "examples": {
                        "processing": {
                            "summary": "ì²˜ë¦¬ ì¤‘",
                            "value": {
                                "task_id": 32,
                                "status": "processing",
                                "progress": None,
                                "message": None,
                                "result": None,
                                "error": None,
                            },
                        },
                        "completed": {
                            "summary": "ì™„ë£Œ (text_extract)",
                            "value": {
                                "task_id": 32,
                                "status": "completed",
                                "progress": 100,
                                "message": None,
                                "result": {
                                    "success": True,
                                    "summary": "ì¹´ì¹´ì˜¤ | ë°±ì—”ë“œ ê°œë°œì",
                                    "resume_ocr": "ì´ë¦„: í™ê¸¸ë™\nê²½ë ¥: 3ë…„...",
                                    "job_posting_ocr": "ì¹´ì¹´ì˜¤ ë°±ì—”ë“œ ì±„ìš©\nìê²©ìš”ê±´: Java...",
                                    "resume_analysis": {
                                        "strengths": ["Java/Spring ìˆ™ë ¨ë„", "í”„ë¡œì íŠ¸ ê²½í—˜"],
                                        "weaknesses": ["í´ë¼ìš°ë“œ ê²½í—˜ ë¶€ì¡±"],
                                        "suggestions": ["AWS í•™ìŠµ ê¶Œì¥"],
                                    },
                                    "posting_analysis": {
                                        "company": "ì¹´ì¹´ì˜¤",
                                        "position": "ë°±ì—”ë“œ ê°œë°œì",
                                        "required_skills": ["Java", "Spring", "MySQL"],
                                        "preferred_skills": ["Docker", "Kubernetes"],
                                    },
                                    "formatted_text": "ì§€ì› íšŒì‚¬ ë° ì§ë¬´ : ì¹´ì¹´ì˜¤ | ë°±ì—”ë“œ ê°œë°œì\n\nì´ë ¥ì„œ ë¶„ì„\n\nì¥ì \n1. Java/Spring ìˆ™ë ¨ë„\n2. í”„ë¡œì íŠ¸ ê²½í—˜\n\në‹¨ì \n1. í´ë¼ìš°ë“œ ê²½í—˜ ë¶€ì¡±\n\nì±„ìš© ê³µê³  ë¶„ì„\n\nê¸°ì—… / í¬ì§€ì…˜\nì¹´ì¹´ì˜¤ / ë°±ì—”ë“œ ê°œë°œì\n\ní•„ìˆ˜ ì—­ëŸ‰\n- Java\n- Spring\n- MySQL\n\nìš°ëŒ€ ì‚¬í•­\n- Docker\n- Kubernetes",
                                    "room_id": 23,
                                },
                                "error": None,
                            },
                        },
                        "failed": {
                            "summary": "ì‹¤íŒ¨",
                            "value": {
                                "task_id": 32,
                                "status": "failed",
                                "progress": None,
                                "message": None,
                                "result": None,
                                "error": {
                                    "code": "OCR_FAILED",
                                    "message": "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                                },
                            },
                        },
                    }
                }
            },
        },
        401: {
            "description": "Unauthorized",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {"code": "UNAUTHORIZED", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤"}
                    }
                }
            },
        },
        404: {
            "description": "Not Found",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "TASK_NOT_FOUND",
                            "message": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: 12",
                        }
                    }
                }
            },
        },
    },
)
async def get_task_status(task_id: str):
    """í†µí•© ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ (text_extract, masking ë“±)"""
    task_key = str(task_id)  # ë¬¸ìì—´ í‚¤ë¡œ í†µì¼
    task = task_store.get(task_key)

    if task is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail={
                "code": ErrorCode.TASK_NOT_FOUND.value,
                "message": f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {task_id}",
            },
        )

    # room_idë¥¼ resultì— í¬í•¨
    result = task.get("result")
    if result and "room_id" not in result:
        result["room_id"] = task.get("room_id")

    return TaskStatusResponse(
        task_id=task_id,
        status=task["status"],
        progress=task.get("progress"),
        message=task.get("message"),
        result=result,
        error=task.get("error"),
    )


# ============================================================================
# API 2: ì±„íŒ… (í†µí•©: ëŒ€í™”/ë¶„ì„/ë©´ì ‘) (ìŠ¤íŠ¸ë¦¬ë°)
# ============================================================================


# ============================================================================
# API 2: ì±„íŒ… (í†µí•©: ëŒ€í™”/ë¶„ì„/ë©´ì ‘) (ìŠ¤íŠ¸ë¦¬ë°)
# ============================================================================


async def generate_chat_stream(request: ChatRequest):
    """ì±„íŒ… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""

    # =========================================================================
    # í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ê²€ì‚¬ (API í˜¸ì¶œ ì „ ì‚¬ì „ í•„í„°ë§)
    # =========================================================================
    user_message_raw = request.message or ""
    guard_result = check_prompt_injection(user_message_raw)

    if guard_result.risk_level == RiskLevel.BLOCK:
        # ì°¨ë‹¨: ì•ˆì „í•œ ì‘ë‹µ ë°˜í™˜ (LLM í˜¸ì¶œ ì—†ì´)
        safe_warning(
            logger,
            "ğŸš¨ í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ì°¨ë‹¨: user_id=%s, patterns=%s",
            request.user_id,
            str(guard_result.matched_patterns),
        )
        blocked_response = guard_result.message
        yield f"data: {json.dumps({'content': blocked_response}, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"
        return

    if guard_result.risk_level == RiskLevel.WARNING:
        # ê²½ê³ : ë¡œê¹…ë§Œ í•˜ê³  ê³„ì† ì§„í–‰
        safe_warning(
            logger,
            "âš ï¸ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì…ë ¥ ê°ì§€: user_id=%s, patterns=%s",
            request.user_id,
            str(guard_result.matched_patterns),
        )

    # contextì—ì„œ ëª¨ë“œ ê²°ì • (normal ë˜ëŠ” interview)
    mode = request.context.mode if request.context else ChatMode.NORMAL

    rag = get_services()
    newline = "\n"
    sse_end = "\n\n"

    # ëª¨ë‹ˆí„°ë§ ì‹œì‘
    start_time = time.time()


    # ëª¨ë¸ ì„ íƒ (gemini ë˜ëŠ” vllm)
    model = request.model.value if hasattr(request.model, "value") else str(request.model)

    # ë©”íŠ¸ë¦­ ì°¨ì› ì •ì˜
    dims = {"Model": model, "Mode": str(mode)}

    logger.info("")
    logger.info(f"{'='*80}")
    logger.info("=== ğŸ’¬ ì±„íŒ… ìš”ì²­ ì‹œì‘ ===")
    logger.info(f"{'='*80}")
    safe_info(logger, "ğŸ“Œ ìš”ì²­ ëª¨ë¸: %s", model.upper())
    safe_info(logger, "ğŸ“Œ ì±„íŒ… ëª¨ë“œ: %s", mode)
    safe_info(logger, "ğŸ“Œ ì‚¬ìš©ì ID: %s", request.user_id)
    safe_info(logger, "ğŸ“Œ ì±„íŒ…ë°© ID: %s", request.room_id)
    logger.info(f"ğŸ“Œ vLLM ì„œë¹„ìŠ¤: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if rag.vllm else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
    logger.info("")

    # 1. ì¼ë°˜ ëŒ€í™” (RAG í™œìš©)
    if mode == ChatMode.NORMAL:
        full_response = ""

        try:
            # íˆìŠ¤í† ë¦¬ ì—†ì´ ë‹¨ì¼ ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬ (ëª…ì„¸ì„œ ê¸°ì¤€)
            history_dict = []

            # Determine if this is an analysis request
            user_message = request.message or ""
            is_analysis = any(
                keyword in user_message for keyword in ["ë¶„ì„", "ë§¤ì¹­", "ì í•©", "í‰ê°€", "ë¹„êµ"]
            )

            # ë©´ì ‘ ëª¨ë“œ ì—¬ë¶€ í™•ì¸
            is_followup = (
                request.interview_id is not None and request.context.mode == ChatMode.INTERVIEW
            )

            if is_analysis:
                # ===================================================================
                # ë¶„ì„ ìš”ì²­: vLLMê³¼ Gemini ì™„ì „ ë¶„ë¦¬
                # ===================================================================
                # ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                logger.info("ğŸ” ë¶„ì„ ìš”ì²­ ê°ì§€")
                logger.info("")

                # ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ (íšŒì‚¬ëª…/ì±„ìš©ì§ë¬´)
                chat_title = ""
                try:
                    logger.info("ğŸ“ [0/3] ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì¤‘...")
                    # VectorDBì—ì„œ ì±„ìš©ê³µê³ ë§Œ ê°€ì ¸ì˜¤ê¸°
                    job_posting_docs = await rag.retrieve_all_documents(
                        user_id=request.user_id, context_types=["job_posting"]
                    )

                    if job_posting_docs:
                        # ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸ (ì• 1000ìë§Œ)
                        posting_text = job_posting_docs[:1000]
                        title_prompt = f"""{get_extract_title_prompt()}

## ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸
{posting_text}
"""
                        # Geminië¡œ ì œëª© ì¶”ì¶œ (ë¹ ë¥´ê³  ì •í™•)
                        title_response = ""
                        async for chunk in rag.llm.generate_response(
                            user_message=title_prompt,
                            context=None,
                            history=[],
                            system_prompt="ë‹¹ì‹ ì€ ì±„ìš©ê³µê³ ì—ì„œ íšŒì‚¬ëª…ê³¼ ì§ë¬´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤.",
                        ):
                            title_response += chunk

                        chat_title = title_response.strip()
                        logger.info(f"âœ… [0/3] ì±„íŒ…ë°© ì œëª©: {chat_title}")

                        # ì±„íŒ…ë°© ì œëª©ì„ SSEë¡œ ì „ì†¡
                        yield f"data: {json.dumps({'summary': chat_title}, ensure_ascii=False)}{sse_end}"
                    else:
                        logger.warning("âš ï¸ ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì œëª© ì¶”ì¶œ ìƒëµ")
                except Exception as e:
                    logger.error(f"âŒ ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                logger.info("")

                # ---------------------------------------------------------------
                # vLLM ëª¨ë“œ (ê°€ì„±ë¹„): EasyOCR â†’ VectorDB â†’ Llama ë¶„ì„
                # ---------------------------------------------------------------
                if model == "vllm" and rag.vllm:
                    logger.info("ğŸ’° [vLLM ê°€ì„±ë¹„ ëª¨ë“œ] ë¶„ì„ ì‹œì‘")
                    logger.info("   í”„ë¡œì„¸ìŠ¤: EasyOCR â†’ VectorDB ì €ì¥ â†’ VectorDB ì¡°íšŒ â†’ Llama ë¶„ì„")
                    logger.info("")

                    # 1. VectorDBì—ì„œ OCRë¡œ ì¶”ì¶œëœ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                    logger.info("ğŸ“‚ [1/3] VectorDBì—ì„œ ì—…ë¡œë“œëœ ë¬¸ì„œ ì¡°íšŒ ì¤‘...")
                    full_context = await rag.retrieve_all_documents(
                        user_id=request.user_id, context_types=["resume", "job_posting"]
                    )

                    if not full_context:
                        error_msg = "âŒ ì—…ë¡œë“œëœ ì´ë ¥ì„œ ë˜ëŠ” ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\në¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                        # ì‚¬ìš©ì IDëŠ” ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                        logger.error("âš ï¸ VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                        yield f"data: {json.dumps({'chunk': error_msg}, ensure_ascii=False)}{sse_end}"
                        full_response = error_msg
                    else:
                        logger.info(f"âœ… [1/3] VectorDB ì¡°íšŒ ì™„ë£Œ: {len(full_context)}ì")
                        logger.info("")

                        # 2. Llama ëª¨ë¸ë¡œ ë¶„ì„
                        logger.info("ğŸ¤– [2/3] Llama ëª¨ë¸ ë¶„ì„ ì‹œì‘...")
                        analysis_prompt = f"""ë‹¤ìŒ ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ë¥¼ ë¶„ì„í•˜ì—¬ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{full_context}

[ì¤‘ìš”] ì „ì²´ ì‘ë‹µì€ ë°˜ë“œì‹œ 1500ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

ì•„ë˜ í˜•ì‹ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”:

ì§€ì› íšŒì‚¬ ë° ì§ë¬´ : [íšŒì‚¬ëª…] | [ì§ë¬´ëª…]

ì´ë ¥ì„œ ë¶„ì„

ì¥ì 
1. [êµ¬ì²´ì ì¸ ì¥ì  1]
2. [êµ¬ì²´ì ì¸ ì¥ì  2]
3. [êµ¬ì²´ì ì¸ ì¥ì  3]

ë‹¨ì 
1. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  1]
2. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  2]
3. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  3]

ì±„ìš© ê³µê³  ë¶„ì„

í•„ìˆ˜ ì—­ëŸ‰
- [í•„ìˆ˜ ì—­ëŸ‰ 1]
- [í•„ìˆ˜ ì—­ëŸ‰ 2]
- [í•„ìˆ˜ ì—­ëŸ‰ 3]

ë§¤ì¹­ë„

ë§ëŠ” ì 
- [ë§¤ì¹­ë˜ëŠ” ì—­ëŸ‰ 1]
- [ë§¤ì¹­ë˜ëŠ” ì—­ëŸ‰ 2]

ë³´ì™„í•  ì 
- [ë¶€ì¡±í•œ ì—­ëŸ‰ 1]
- [ë¶€ì¡±í•œ ì—­ëŸ‰ 2]

ì ˆëŒ€ ê¸ˆì§€:
- # ## ### ì œëª© ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€
- ** __ ë³¼ë“œ/ì´íƒ¤ë¦­ ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€
- 1500ì ì´ˆê³¼ ê¸ˆì§€

ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."""

                        async for chunk in rag.vllm.generate_response(
                            user_message=analysis_prompt,
                            context=None,
                            history=[],
                            system_prompt="ë‹¹ì‹ ì€ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•(#, ##, **, ```)ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³  ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.",
                        ):
                            full_response += chunk
                            yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                        logger.info(f"âœ… [3/3] Llama ë¶„ì„ ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")

                # ---------------------------------------------------------------
                # Gemini ëª¨ë“œ (ê³ ì„±ëŠ¥): Gemini Vision APIë¡œ ì§ì ‘ íŒŒì¼ ì½ê³  ë¶„ì„
                # ---------------------------------------------------------------
                else:
                    if model == "vllm" and not rag.vllm:
                        logger.warning("âš ï¸ vLLM ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€ â†’ Geminië¡œ ìë™ ë³€ê²½")

                    logger.info("ğŸš€ [Gemini ê³ ì„±ëŠ¥ ëª¨ë“œ] ë¶„ì„ ì‹œì‘")
                    logger.info("   í”„ë¡œì„¸ìŠ¤: RAG ê²€ìƒ‰ â†’ Gemini ë¶„ì„ (ì›ë˜ ë°©ì‹)")
                    logger.info("")

                    # RAGë¡œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰í•˜ì—¬ Geminië¡œ ë¶„ì„
                    logger.info("ğŸ“‚ [1/2] RAG ê²€ìƒ‰ ì¤‘...")
                    async for chunk in rag.chat_with_rag(
                        user_message=user_message,
                        user_id=request.user_id,
                        history=history_dict,
                        use_rag=True,
                        context_types=["resume", "job_posting"],
                        model="gemini",
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                    logger.info(f"âœ… [2/2] Gemini ë¶„ì„ ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")
            else:
                # ===================================================================
                # ì¼ë°˜ ëŒ€í™”: RAG ê²€ìƒ‰ ì‚¬ìš©
                # ===================================================================
                logger.info("ğŸ’¬ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ")
                logger.info("")

                # ë©´ì ‘ ì„¸ì…˜ì—ì„œ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ë˜ëŠ” í”¼ë“œë°± (interview_idê°€ ìˆê³ , ì´ì „ ì§ˆë¬¸-ë‹µë³€ ìŒì´ ìˆëŠ” ê²½ìš°)
                if is_followup:
                    # ë©´ì ‘ Q&A ê°œìˆ˜ ì¹´ìš´íŠ¸ (assistant=ì§ˆë¬¸, user=ë‹µë³€)
                    interview_qa_count = len([h for h in history_dict if h.get("role") == "user"])

                    logger.info(f"ğŸ“Š í˜„ì¬ ë©´ì ‘ ë‹µë³€ ìˆ˜: {interview_qa_count}ê°œ")

                    # 5ë²ˆì§¸ ë‹µë³€ í›„ â†’ í”¼ë“œë°± ìƒì„±
                    if interview_qa_count >= 5:
                        logger.info("ğŸ¯ [ë©´ì ‘ ì¢…ë£Œ] 5ê°œ ë‹µë³€ ì™„ë£Œ â†’ í”¼ë“œë°± ìƒì„± ì‹œì‘")

                        # ì¢…ë£Œ ë©”ì‹œì§€
                        end_msg = "ë©´ì ‘ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹µë³€ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n\n"
                        yield f"data: {json.dumps({'chunk': end_msg}, ensure_ascii=False)}{sse_end}"
                        full_response += end_msg

                        # Q&A ëª©ë¡ ìƒì„±
                        qa_pairs = []
                        for i in range(0, len(history_dict), 2):
                            if i + 1 < len(history_dict):
                                qa_pairs.append(
                                    {
                                        "question": history_dict[i].get("content", ""),
                                        "answer": history_dict[i + 1].get("content", ""),
                                    }
                                )

                        # í”¼ë“œë°± í”„ë¡¬í”„íŠ¸
                        feedback_prompt = (
                            "ë‹¤ìŒ ë©´ì ‘ Q&Aì— ëŒ€í•´ ê° ë‹µë³€ë§ˆë‹¤ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”:\n\n"
                        )
                        for i, qa in enumerate(qa_pairs[:5], 1):
                            feedback_prompt += (
                                f"ì§ˆë¬¸ {i}: {qa['question']}\në‹µë³€ {i}: {qa['answer']}\n\n"
                            )

                        feedback_prompt += """ê° ë‹µë³€ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í”¼ë“œë°±í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸ 1
[ì§ˆë¬¸ ë‚´ìš©]

ë‹µë³€ 1
[ë‹µë³€ ë‚´ìš©]

í‰ê°€
- ì˜í•œ ì : [êµ¬ì²´ì ìœ¼ë¡œ]
- ê°œì„ ì : [êµ¬ì²´ì ìœ¼ë¡œ]
- ì¶”ì²œ ë‹µë³€: [ë” ë‚˜ì€ ë‹µë³€ ì˜ˆì‹œ]

(ì§ˆë¬¸ 2~5ë„ ê°™ì€ í˜•ì‹ìœ¼ë¡œ)

ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•(#, **, ```)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."""

                        # í”¼ë“œë°± ìƒì„±
                        async for chunk in rag.llm.generate_response(
                            user_message=feedback_prompt,
                            context=None,
                            history=[],
                            system_prompt="ë‹¹ì‹ ì€ ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.",
                            user_id=request.user_id,
                        ):
                            full_response += chunk
                            yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                        logger.info(
                            f"âœ… [ë©´ì ‘ í”¼ë“œë°±] ìƒì„± ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)"
                        )

                    else:
                        # 5ê°œ ë¯¸ë§Œ â†’ ê¼¬ë¦¬ì§ˆë¬¸ ê³„ì† ìƒì„±
                        original_question = history_dict[-2].get("content", "")
                        candidate_answer = history_dict[-1].get("content", "")

                        logger.info("ğŸ” [ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±] ê°ì§€")
                        logger.info(f"   ì›ë³¸ ì§ˆë¬¸: {original_question[:50]}...")
                        logger.info(f"   ë‹µë³€: {candidate_answer[:50]}...")
                        logger.info("")

                        # ê°„ë‹¨í•œ STAR ë¶„ì„
                        star_analysis = {
                            "situation": "unknown",
                            "task": "unknown",
                            "action": "unknown",
                            "result": "unknown",
                        }

                        # ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±
                        async for chunk in rag.generate_followup_question(
                            original_question=original_question,
                            candidate_answer=candidate_answer,
                            star_analysis=star_analysis,
                            model=model,
                            user_id=request.user_id,
                        ):
                            full_response += chunk
                            yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                        logger.info(f"âœ… [ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±] ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")

                else:
                    # ì¼ë°˜ ëŒ€í™” ë˜ëŠ” ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­
                    if (
                        "ë©´ì ‘ ì§ˆë¬¸" in user_message
                        or "ë©´ì ‘ì§ˆë¬¸" in user_message
                        or "ë©´ì ‘" in user_message
                    ):
                        # ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­ ì‹œ portfolio(ë©´ì ‘ ì§ˆë¬¸ ë°ì´í„°)ë§Œ ê²€ìƒ‰
                        context_types = ["portfolio"]
                        logger.info("ğŸ¯ ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­ ê°ì§€ â†’ portfolio ì»¬ë ‰ì…˜ë§Œ ê²€ìƒ‰")
                    else:
                        # ì¼ë°˜ ìš”ì²­ ì‹œ ëª¨ë“  ì»¬ë ‰ì…˜ ê²€ìƒ‰
                        context_types = ["resume", "job_posting", "portfolio"]
                        logger.info("ğŸ“š ì¼ë°˜ ëŒ€í™” â†’ ëª¨ë“  ì»¬ë ‰ì…˜ ê²€ìƒ‰")

                    logger.info("")

                    # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±
                    # ëª¨ë¸ëª…ì€ ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                    logger.info("ğŸ” RAG ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì‹œì‘...")
                    async for chunk in rag.chat_with_rag(
                        user_message=user_message,
                        user_id=request.user_id,
                        history=history_dict,
                        use_rag=True,  # RAG í™œì„±í™”
                        context_types=context_types,
                        model=model,
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                    # ëª¨ë¸ëª…ì€ ë¡œê·¸ì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)
                    logger.info("âœ… ì¼ë°˜ ëŒ€í™” ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: %dì)", len(full_response))

        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            yield f"data: {json.dumps({'chunk': error_msg}, ensure_ascii=False)}{sse_end}"
            full_response = error_msg

        yield f"data: [DONE]{sse_end}"

    # 2. ë©´ì ‘ ëª¨ë“œ - 5ê°œ ì§ˆë¬¸ Ã— ìµœëŒ€ 3 depths ê¼¬ë¦¬ì§ˆë¬¸
    elif mode == ChatMode.INTERVIEW:
        try:
            interview_type = request.context.interview_type or "tech"
            interview_type_kr = "ê¸°ìˆ " if interview_type == "tech" else "ì¸ì„±"

            # ì„¸ì…˜ ìºì‹œ í‚¤ ìƒì„±
            session_key = get_session_key(request.user_id, request.interview_id)
            user_message = request.message or ""

            # ì„¸ì…˜ ìƒíƒœ í™•ì¸ (ìš”ì²­ì— ì—†ìœ¼ë©´ ìºì‹œì—ì„œ ì¡°íšŒ)
            session = request.context.interview_session
            if session is None:
                session = interview_sessions.get(session_key)
                if session:
                    safe_info(
                        logger,
                        "ğŸ“¦ [ë©´ì ‘] ìºì‹œì—ì„œ ì„¸ì…˜ ë³µì›: %s, phase=%s, Q%d/5",
                        session_key,
                        session.phase,
                        session.current_question_id,
                    )

            # vLLM ë˜ëŠ” Gemini ì„ íƒ
            model_choice = (
                request.model.value if hasattr(request.model, "value") else str(request.model)
            )

            # ===================================================================
            # PHASE 1: ì„¸ì…˜ ì´ˆê¸°í™” (ì²« ìš”ì²­ ì‹œ 5ê°œ ì§ˆë¬¸ ì„¸íŠ¸ ìƒì„±)
            # ===================================================================
            if session is None or session.phase == "init":
                logger.info("ğŸ¯ [ë©´ì ‘] ì„¸ì…˜ ì´ˆê¸°í™” - 5ê°œ ì§ˆë¬¸ ì„¸íŠ¸ ìƒì„± ì‹œì‘")

                # RAGë¡œ ì‚¬ìš©ì ì •ë³´ ê²€ìƒ‰
                context = await rag.retrieve_context(
                    query=f"{interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ì„ ìœ„í•œ ì‚¬ìš©ì ì •ë³´",
                    user_id=request.user_id,
                    context_types=["resume", "portfolio", "job_posting"],
                    n_results=3,
                )

                resume_ocr = request.context.resume_ocr if request.context else None
                job_posting_ocr = request.context.job_posting_ocr if request.context else None
                portfolio_text = request.context.portfolio_text if request.context else None

                # 5ê°œ ì§ˆë¬¸ ì„¸íŠ¸ ìƒì„± í”„ë¡¬í”„íŠ¸
                init_prompt = create_tech_interview_init_prompt(
                    resume_text=resume_ocr or context or "ì •ë³´ ì—†ìŒ",
                    job_posting_text=job_posting_ocr or "ì •ë³´ ì—†ìŒ",
                    portfolio_text=portfolio_text or context or "ì •ë³´ ì—†ìŒ",
                )

                # ë°©ì•ˆ 2: ë¹„ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ JSON ìƒì„± (ë¹ ë¦„) â†’ ì§ˆë¬¸ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì¶œë ¥ (íƒ€ì´í•‘ íš¨ê³¼)
                system_prompt = get_system_tech_interview()

                if model_choice == "vllm" and rag.vllm:
                    # vLLMì€ ê¸°ì¡´ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ìœ ì§€ (ë¹„ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›)
                    full_response = ""
                    async for chunk in rag.vllm.generate_response(
                        user_message=init_prompt,
                        context=None,
                        history=[],
                        system_prompt=system_prompt,
                    ):
                        full_response += chunk
                else:
                    # Gemini: ë¹„ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ JSON ë¹ ë¥´ê²Œ ìƒì„±
                    full_response = await rag.llm.generate_response_non_stream(
                        user_message=init_prompt,
                        context=None,
                        system_prompt=system_prompt,
                        user_id=request.user_id,
                    )

                # JSON íŒŒì‹±í•˜ì—¬ ì„¸ì…˜ ìƒì„±
                try:
                    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì œê±° ë° JSON ì¶”ì¶œ
                    import re

                    # ```json ... ``` í˜•ì‹ ì œê±°
                    json_content = re.sub(r"```json\s*", "", full_response)
                    json_content = re.sub(r"```\s*$", "", json_content)
                    json_content = json_content.strip()

                    # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì¤‘ê´„í˜¸ ê¸°ì¤€)
                    json_start = json_content.find("{")
                    json_end = json_content.rfind("}") + 1

                    if json_start != -1 and json_end > json_start:
                        json_str = json_content[json_start:json_end]

                        # ë””ë²„ê¹…ìš© ë¡œê·¸ (ì‹¤ì œ JSON ë¬¸ìì—´ ì¼ë¶€ ì¶œë ¥)
                        logger.info(f"JSON íŒŒì‹± ì‹œë„ (ì²« 200ì): {json_str[:200]}")

                        questions_data = json.loads(json_str)

                        # ì„¸ì…˜ ìƒì„±
                        new_session = InterviewSession(
                            session_id=str(uuid.uuid4()),
                            interview_type=interview_type,
                            questions=[
                                InterviewQuestionState(
                                    id=q["id"],
                                    category=q["category"],
                                    category_name=q["category_name"],
                                    question=q["question"],
                                    intent=q.get("intent", ""),
                                    keywords=q.get("keywords", []),
                                )
                                for q in questions_data.get("questions", [])
                            ],
                            current_question_id=1,
                            phase="questioning",
                        )

                        logger.info("âœ… ë©´ì ‘ ì§ˆë¬¸ ì„¸íŠ¸ ìƒì„± ì™„ë£Œ: %dê°œ", len(new_session.questions))

                        # ì„¸ì…˜ ìºì‹œì— ì €ì¥
                        interview_sessions[session_key] = new_session
                        safe_info(logger, "ğŸ’¾ [ë©´ì ‘] ì„¸ì…˜ ìºì‹œ ì €ì¥: %s", session_key)

                        # ì²« ë²ˆì§¸ ì§ˆë¬¸ ì¶œë ¥ (í—¤ë”: [ê¸°ìˆ ë©´ì ‘ 1/5]) - íƒ€ì´í•‘ íš¨ê³¼
                        first_q = new_session.questions[0] if new_session.questions else None
                        if first_q:
                            question_text = (
                                f"[{interview_type_kr}ë©´ì ‘ 1/5]{newline}{first_q.question}"
                            )
                            # ë°©ì•ˆ 2: ì§ˆë¬¸ì„ í•œ ê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ (íƒ€ì´í•‘ íš¨ê³¼)
                            for char in question_text:
                                yield f"data: {json.dumps({'chunk': char}, ensure_ascii=False)}{sse_end}"
                                await asyncio.sleep(0.015)  # 15ms ë”œë ˆì´ë¡œ íƒ€ì´í•‘ íš¨ê³¼

                            # ì„¸ì…˜ ìƒíƒœ ì „ë‹¬ (ë©”íƒ€ë°ì´í„°ë¡œ)
                            session_meta = {
                                "type": "session_state",
                                "session": new_session.model_dump(),
                            }
                            yield f"data: {json.dumps(session_meta, ensure_ascii=False)}{sse_end}"
                    else:
                        raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

                except (json.JSONDecodeError, ValueError) as e:
                    logger.error(f"ì§ˆë¬¸ ì„¸íŠ¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    logger.error(f"ì›ë³¸ ì‘ë‹µ (ì²« 500ì): {full_response[:500]}")

                    # SSE ì—ëŸ¬ ì´ë²¤íŠ¸ ì „ì†¡ (HTTP 200 ìœ ì§€, payloadì— status=500)
                    error_response = {
                        "type": "error",
                        "error": {
                            "code": "PARSE_FAILED",
                            "status": 500,
                        },
                        "fallback": "ë©´ì ‘ ì§ˆë¬¸ ì„¸íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    }
                    yield f"data: {json.dumps(error_response, ensure_ascii=False)}{sse_end}"

                yield f"data: [DONE]{sse_end}"

            # ===================================================================
            # PHASE 2: ê¼¬ë¦¬ì§ˆë¬¸ ë˜ëŠ” ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± (ë‹µë³€ ìˆ˜ì‹  í›„)
            # ===================================================================
            elif session.phase in ["questioning", "followup"]:
                current_q_id = session.current_question_id
                current_q = next((q for q in session.questions if q.id == current_q_id), None)

                if not current_q:
                    yield f"data: {json.dumps({'chunk': 'ì„¸ì…˜ ì˜¤ë¥˜: í˜„ì¬ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}{sse_end}"
                    yield f"data: [DONE]{sse_end}"
                    return

                # ì§€ì›ì ë‹µë³€ì„ ëŒ€í™” ì´ë ¥ì— ì¶”ê°€
                current_q.conversation.append(
                    {
                        "role": "candidate",
                        "content": user_message,
                    }
                )
                current_q.current_depth += 1

                # ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì—¬ë¶€ íŒë‹¨ (ìµœëŒ€ 3 depths)
                if current_q.current_depth < current_q.max_depth:
                    # ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±
                    followup_prompt = create_tech_followup_prompt(
                        question_id=current_q.id,
                        category_name=current_q.category_name,
                        original_question=current_q.question,
                        conversation_history=format_conversation_history(current_q.conversation),
                        last_answer=user_message,
                        current_depth=current_q.current_depth,
                    )

                    full_response = ""
                    system_prompt = get_system_tech_interview()

                    if model_choice == "vllm" and rag.vllm:
                        async for chunk in rag.vllm.generate_response(
                            user_message=followup_prompt,
                            context=None,
                            history=[],
                            system_prompt=system_prompt,
                        ):
                            full_response += chunk
                    else:
                        async for chunk in rag.llm.generate_response(
                            user_message=followup_prompt,
                            context=None,
                            history=[],
                            system_prompt=system_prompt,
                            user_id=request.user_id,
                        ):
                            full_response += chunk

                    # ê¼¬ë¦¬ì§ˆë¬¸ ì‘ë‹µ íŒŒì‹±
                    try:
                        json_start = full_response.find("{")
                        json_end = full_response.rfind("}") + 1
                        if json_start != -1 and json_end > json_start:
                            followup_data = json.loads(full_response[json_start:json_end])

                            if followup_data.get("should_continue", True) and followup_data.get(
                                "followup"
                            ):
                                # ê¼¬ë¦¬ì§ˆë¬¸ ì¶œë ¥ (í—¤ë”: [ê¸°ìˆ ë©´ì ‘ 2-1/5] í˜•ì‹)
                                followup_q = followup_data["followup"]["question"]
                                current_q.conversation.append(
                                    {
                                        "role": "interviewer",
                                        "content": followup_q,
                                    }
                                )
                                session.phase = "followup"

                                # ê¼¬ë¦¬ì§ˆë¬¸ í—¤ë”: [ê¸°ìˆ ë©´ì ‘ {ì§ˆë¬¸ë²ˆí˜¸}-{ê¼¬ë¦¬ì§ˆë¬¸ë²ˆí˜¸}/5] - íƒ€ì´í•‘ íš¨ê³¼
                                followup_header = f"[{interview_type_kr}ë©´ì ‘ {current_q_id}-{current_q.current_depth}/5]"
                                followup_text = f"{followup_header}{newline}{followup_q}"
                                # í•œ ê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ (íƒ€ì´í•‘ íš¨ê³¼)
                                for char in followup_text:
                                    yield f"data: {json.dumps({'chunk': char}, ensure_ascii=False)}{sse_end}"
                                    await asyncio.sleep(0.015)
                            else:
                                # ë‹¤ìŒ ì£¼ì œë¡œ ì´ë™
                                current_q.is_completed = True
                    except json.JSONDecodeError as e:
                        # ê¼¬ë¦¬ì§ˆë¬¸ íŒŒì‹± ì‹¤íŒ¨ - SSE ì—ëŸ¬ ì „ì†¡ í›„ ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™
                        logger.error(f"ê¼¬ë¦¬ì§ˆë¬¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        error_response = {
                            "type": "error",
                            "error": {
                                "code": "PARSE_FAILED",
                                "status": 500,
                            },
                            "fallback": "ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.",
                        }
                        yield f"data: {json.dumps(error_response, ensure_ascii=False)}{sse_end}"
                        current_q.is_completed = True

                else:
                    # ìµœëŒ€ ê¹Šì´ ë„ë‹¬ - ë‹¤ìŒ ì£¼ì œë¡œ ì´ë™
                    current_q.is_completed = True

                # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™í•´ì•¼ í•˜ëŠ” ê²½ìš°
                if current_q.is_completed:
                    next_q_id = current_q_id + 1
                    if next_q_id <= session.total_questions:
                        next_q = next((q for q in session.questions if q.id == next_q_id), None)
                        if next_q:
                            session.current_question_id = next_q_id
                            session.phase = "questioning"

                            # ë‹¤ìŒ ì§ˆë¬¸ ì¶œë ¥ (í—¤ë”: [ê¸°ìˆ ë©´ì ‘ 2/5] í˜•ì‹) - íƒ€ì´í•‘ íš¨ê³¼
                            question_header = f"[{interview_type_kr}ë©´ì ‘ {next_q_id}/5]"
                            question_text = f"{question_header}{newline}{next_q.question}"
                            # í•œ ê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ (íƒ€ì´í•‘ íš¨ê³¼)
                            for char in question_text:
                                yield f"data: {json.dumps({'chunk': char}, ensure_ascii=False)}{sse_end}"
                                await asyncio.sleep(0.015)

                            next_q.conversation.append(
                                {
                                    "role": "interviewer",
                                    "content": next_q.question,
                                }
                            )
                    else:
                        # ëª¨ë“  ì§ˆë¬¸ ì™„ë£Œ - íƒ€ì´í•‘ íš¨ê³¼
                        session.phase = "completed"
                        complete_msg = f"{newline}{newline}ë©´ì ‘ ê²°ê³¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”."
                        for char in complete_msg:
                            yield f"data: {json.dumps({'chunk': char}, ensure_ascii=False)}{sse_end}"
                            await asyncio.sleep(0.015)

                # ì„¸ì…˜ ìºì‹œ ì—…ë°ì´íŠ¸ (PHASE 2 ë¸”ë¡ ë‚´ë¶€, if current_q.is_completed ì™¸ë¶€)
                interview_sessions[session_key] = session
                safe_info(
                    logger,
                    "ğŸ’¾ [ë©´ì ‘] ì„¸ì…˜ ìºì‹œ ì—…ë°ì´íŠ¸: %s, phase=%s, Q%d/5",
                    session_key,
                    session.phase,
                    session.current_question_id,
                )

                # ë©´ì ‘ ì™„ë£Œ ì‹œ ì„¸ì…˜ ì •ë¦¬
                if session.phase == "completed":
                    interview_sessions.pop(session_key, None)
                    safe_info(logger, "ğŸ—‘ï¸ [ë©´ì ‘] ì™„ë£Œëœ ì„¸ì…˜ ì‚­ì œ: %s", session_key)

                # ì—…ë°ì´íŠ¸ëœ ì„¸ì…˜ ìƒíƒœ ì „ë‹¬
                session_meta = {
                    "type": "session_state",
                    "session": session.model_dump(),
                }
                yield f"data: {json.dumps(session_meta, ensure_ascii=False)}{sse_end}"
                yield f"data: [DONE]{sse_end}"

            # ===================================================================
            # PHASE 3: ë©´ì ‘ ì™„ë£Œ
            # ===================================================================
            elif session.phase == "completed":
                # ì´ë¯¸ ì™„ë£Œëœ ì„¸ì…˜ - ìºì‹œì—ì„œ ì‚­ì œ
                interview_sessions.pop(session_key, None)
                complete_msg = "ë©´ì ‘ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë¦¬í¬íŠ¸ ëª¨ë“œì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”."
                yield f"data: {json.dumps({'chunk': complete_msg}, ensure_ascii=False)}{sse_end}"
                yield f"data: [DONE]{sse_end}"

        except Exception as e:
            logger.error(f"Interview error: {e}", exc_info=True)
            error_msg = f"ë©´ì ‘ ì§„í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            yield f"data: {json.dumps({'chunk': error_msg}, ensure_ascii=False)}{sse_end}"
            yield f"data: [DONE]{sse_end}"

    # 3. ë¦¬í¬íŠ¸ ëª¨ë“œ - ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
    elif mode == ChatMode.REPORT:
        try:
            interview_type = request.context.interview_type or "tech"
            interview_type_kr = "ê¸°ìˆ " if interview_type == "tech" else "ì¸ì„±"
            qa_list = request.context.qa_list or []

            if not qa_list:
                yield f"data: [DONE]{sse_end}"
                return

            content = f"{interview_type_kr} ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...{newline}"
            yield f"data: {json.dumps({'chunk': content}, ensure_ascii=False)}{sse_end}"

            # Q&A ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            qa_text = ""
            for i, qa in enumerate(qa_list, 1):
                q = qa.get("question", "")
                a = qa.get("answer", "")
                qa_text += f"ì§ˆë¬¸ {i}: {q}\në‹µë³€ {i}: {a}\n\n"

            # í‰ê°€ ë¦¬í¬íŠ¸ í”„ë¡¬í”„íŠ¸
            report_prompt = f"""
ë‹¤ìŒì€ {interview_type_kr} ë©´ì ‘ Q&A ê¸°ë¡ì…ë‹ˆë‹¤:

{qa_text}

ìœ„ ë©´ì ‘ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ê° ë‹µë³€ì— ëŒ€í•œ ê°œë³„ í‰ê°€ (ì˜í•œ ì , ê°œì„ ì )
2. ì „ì²´ì ì¸ ê°•ì  íŒ¨í„´
3. ì „ì²´ì ì¸ ì•½ì  íŒ¨í„´
4. í–¥í›„ í•™ìŠµ ê°€ì´ë“œ
"""

            full_report = ""

            # vLLM ë˜ëŠ” Gemini ì„ íƒ
            model_choice = (
                request.model.value if hasattr(request.model, "value") else str(request.model)
            )

            if model_choice == "vllm" and rag.vllm:
                logger.info("ğŸ“Š [vLLM] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.vllm.generate_response(
                    user_message=report_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
                ):
                    full_report += chunk
                    yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"
            else:
                logger.info("ğŸ“Š [Gemini] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.llm.generate_response(
                    user_message=report_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
                    user_id=request.user_id,
                ):
                    full_report += chunk
                    yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

            {
                "success": True,
                "mode": "report",
                "response": full_report.strip(),
                "interview_type": interview_type,
                "interview_id": request.interview_id,
            }
            yield f"data: [DONE]{sse_end}"

        except Exception as e:
            logger.error(f"Interview report generation error: {e}")
            {"success": False, "mode": "report", "error": str(e)}
            yield f"data: [DONE]{sse_end}"

    # Latency ì¸¡ì • ì¢…ë£Œ ë° ì „ì†¡
    try:
        duration = (time.time() - start_time) * 1000
        cw = CloudWatchService.get_instance()
        asyncio.create_task(cw.put_metric("AI_Chat_Latency", duration, "Milliseconds", dims))
    except Exception as e:
        logger.error(f"Failed to record latency metric: {e}")


@router.post(
    "/chat",
    summary="ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° (ì¼ë°˜/ë©´ì ‘/ë¦¬í¬íŠ¸)",
    description="""
    ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    **ì²˜ë¦¬ ë°©ì‹:** ìŠ¤íŠ¸ë¦¬ë° (SSE)

    **ëª¨ë“œ:**
    - normal: ì¼ë°˜ ëŒ€í™”
    - interview: ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
    - report: ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±

    **ë©´ì ‘ íƒ€ì… (context.interview_type):**
    - behavior: ì¸ì„± ë©´ì ‘
    - tech: ê¸°ìˆ  ë©´ì ‘
    """,
    responses={
        400: {
            "description": "Bad Request",
            "content": {
                "application/json": {
                    "examples": {
                        "invalid_request": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_REQUEST",
                                    "message": "room_idëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤",
                                    "field": "room_id",
                                }
                            }
                        },
                        "invalid_mode": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_MODE",
                                    "message": "modeëŠ” normal ë˜ëŠ” interview ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤",
                                    "field": "context.mode",
                                }
                            }
                        },
                        "invalid_interview_type": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_INTERVIEW_TYPE",
                                    "message": "interview_typeì€ behavior ë˜ëŠ” techë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                                    "field": "context.interview_type",
                                }
                            }
                        },
                    }
                }
            },
        },
        401: {
            "description": "Unauthorized",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {"code": "UNAUTHORIZED", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤"}
                    }
                }
            },
        },
        422: {
            "description": "Unprocessable Entity",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "MISSING_CONTEXT",
                            "message": "ë©´ì ‘ ëª¨ë“œ ì‹œ context.resume_ocr ë˜ëŠ” context.job_posting_ocrì´ í•„ìš”í•©ë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        429: {
            "description": "Too Many Requests",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "RATE_LIMIT_EXCEEDED",
                            "message": "ìš”ì²­ í•œë„ ì´ˆê³¼. 1ë¶„ í›„ ì¬ì‹œë„í•˜ì„¸ìš”",
                        }
                    }
                }
            },
        },
        500: {
            "description": "Internal Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "INTERNAL_ERROR",
                            "message": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        503: {
            "description": "Service Unavailable",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "LLM_UNAVAILABLE",
                            "message": "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
    },
)
async def chat(request: ChatRequest):
    """ì±„íŒ… ì²˜ë¦¬ (ì¼ë°˜/ë©´ì ‘)"""
    return StreamingResponse(
        generate_chat_stream(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        },
    )


# ============================================================================
# API 3: ìº˜ë¦°ë” ì¼ì • íŒŒì‹± (ë™ê¸°)
# ============================================================================


@router.post(
    "/calendar/parse",
    response_model=CalendarParseResponse,
    summary="ìº˜ë¦°ë” ì¼ì • ì •ë³´ íŒŒì‹±",
    description="""
    ìº˜ë¦°ë” ëª¨ë‹¬ì—ì„œ íŒŒì¼/í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (í¼ ìë™ ì±„ìš°ê¸°ìš©).

    **ì²˜ë¦¬ ë°©ì‹:** ë™ê¸° - ê°„ë‹¨í•œ íŒŒì‹± ì‘ì—…

    **Pydantic AI ì‚¬ìš©:** CalendarParseResult

    **ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
    - ëª¨ë‹¬ì—ì„œ ì±„ìš©ê³µê³  íŒŒì¼/í…ìŠ¤íŠ¸ ì²¨ë¶€ â†’ ì¼ì • ì •ë³´ ì¶”ì¶œ
    - Frontendê°€ ëª¨ë‹¬ í¼ì— ìë™ ì±„ì›Œë„£ìŒ
    - ì‚¬ìš©ì í™•ì¸/ìˆ˜ì • â†’ ì €ì¥ â†’ Backendê°€ Google Calendarì— ì¶”ê°€
    """,
)
async def calendar_parse(request: CalendarParseRequest):  # noqa: ARG001
    """ìº˜ë¦°ë” ì¼ì • íŒŒì‹±"""
    # validatorì—ì„œ ì´ë¯¸ ê²€ì¦ë¨

    # Pydantic AIë¡œ êµ¬ì¡°í™”ëœ ê²°ê³¼ ë°˜í™˜
    return CalendarParseResponse(
        success=True,
        company="ì¹´ì¹´ì˜¤",
        position="ë°±ì—”ë“œ ê°œë°œì",
        schedules=[
            {"stage": "ì„œë¥˜ ë§ˆê°", "date": "2026-01-15", "time": None},
            {"stage": "ì½”ë”©í…ŒìŠ¤íŠ¸", "date": "2026-01-20", "time": "14:00"},
            {"stage": "1ì°¨ ë©´ì ‘", "date": "2026-01-25", "time": None},
        ],
        hashtags=["#ì¹´ì¹´ì˜¤", "#ë°±ì—”ë“œ", "#ì‹ ì…"],
    )


# ============================================================================
# API 4: ê²Œì‹œíŒ ì²¨ë¶€íŒŒì¼ ë§ˆìŠ¤í‚¹ (ë¹„ë™ê¸°)
# ============================================================================
# ì´ APIëŠ” app/api/routes/masking.pyë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.
# masking.pyì—ì„œ íŒŒì¼ ê¸°ë°˜ ì €ì¥ì†Œì™€ ì‹¤ì œ Gemini APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
