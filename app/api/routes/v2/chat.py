"""
v2 ì±„íŒ… API (í†µí•©: ëŒ€í™”/ë¶„ì„/ë©´ì ‘/ë¦¬í¬íŠ¸)

POST /ai/chat - ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° (SSE)
"""

import asyncio
import json
import logging
import re
import time
import uuid

from fastapi import APIRouter, Depends
from fastapi.responses import StreamingResponse

from app.api.routes.v2._helpers import get_services, get_session_key
from app.api.routes.v2._sse_errors import sse_error_event
from app.config.dependencies import get_session_store
from app.prompts import (
    create_tech_followup_prompt,
    create_tech_interview_init_prompt,
    format_conversation_history,
    get_extract_title_prompt,
    get_system_tech_interview,
)
from app.schemas.chat import (
    ChatMode,
    ChatRequest,
    InterviewQuestionState,
    InterviewSession,
)
from app.services.cloudwatch_service import CloudWatchService
from app.utils.log_sanitizer import safe_info, safe_warning
from app.utils.prompt_guard import RiskLevel, check_prompt_injection

logger = logging.getLogger(__name__)

router = APIRouter()


async def generate_chat_stream(
    request: ChatRequest,
    session_store,
):
    """ì±„íŒ… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (session_store: DI from get_session_store)"""

    # =========================================================================
    # í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ê²€ì‚¬
    # =========================================================================
    user_message_raw = request.message or ""
    guard_result = check_prompt_injection(user_message_raw)

    if guard_result.risk_level == RiskLevel.BLOCK:
        safe_warning(
            logger,
            "ğŸš¨ í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ì°¨ë‹¨: user_id=%s, patterns=%s",
            request.user_id,
            str(guard_result.matched_patterns),
        )
        yield sse_error_event(
            code="PROMPT_BLOCKED",
            status=400,
            message="í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ì´ ê°ì§€ë˜ì–´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
            fallback=guard_result.message,
        )
        yield "data: [DONE]\n\n"
        return

    if guard_result.risk_level == RiskLevel.WARNING:
        safe_warning(
            logger,
            "âš ï¸ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì…ë ¥ ê°ì§€: user_id=%s, patterns=%s",
            request.user_id,
            str(guard_result.matched_patterns),
        )

    mode = request.context.mode if request.context else ChatMode.NORMAL

    rag = get_services()
    newline = "\n"
    sse_end = "\n\n"

    # ëª¨ë‹ˆí„°ë§ ì‹œì‘
    start_time = time.time()

    model = request.model.value if hasattr(request.model, "value") else str(request.model)
    dims = {"Model": model, "Mode": str(mode)}

    logger.info("")
    logger.info(f"{'='*80}")
    logger.info("=== ğŸ’¬ ì±„íŒ… ìš”ì²­ ì‹œì‘ ===")
    logger.info(f"{'='*80}")
    safe_info(logger, "ğŸ“Œ ìš”ì²­ ëª¨ë¸: %s", model.upper())
    safe_info(logger, "ğŸ“Œ ì±„íŒ… ëª¨ë“œ: %s", mode)
    safe_info(logger, "ğŸ“Œ ì‚¬ìš©ì ID: %s", request.user_id)
    safe_info(logger, "ğŸ“Œ ì±„íŒ…ë°© ID: %s", request.room_id)
    logger.info(f"ğŸ“Œ vLLM ì„œë¹„ìŠ¤: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if rag.vllm else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
    logger.info("")

    # =========================================================================
    # 1. ì¼ë°˜ ëŒ€í™” (RAG í™œìš©)
    # =========================================================================
    if mode == ChatMode.NORMAL:
        full_response = ""

        try:
            history_dict = []
            user_message = request.message or ""
            is_analysis = any(
                keyword in user_message for keyword in ["ë¶„ì„", "ë§¤ì¹­", "ì í•©", "í‰ê°€", "ë¹„êµ"]
            )
            is_followup = (
                request.interview_id is not None and request.context.mode == ChatMode.INTERVIEW
            )

            if is_analysis:
                logger.info("ğŸ” ë¶„ì„ ìš”ì²­ ê°ì§€")
                logger.info("")

                # ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ
                chat_title = ""
                try:
                    logger.info("ğŸ“ [0/3] ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì¤‘...")
                    job_posting_docs = await rag.retrieve_all_documents(
                        user_id=request.user_id, context_types=["job_posting"]
                    )

                    if job_posting_docs:
                        posting_text = job_posting_docs[:1000]
                        title_prompt = f"""{get_extract_title_prompt()}

## ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸
{posting_text}
"""
                        title_response = ""
                        async for chunk in rag.llm.generate_response(
                            user_message=title_prompt,
                            context=None,
                            history=[],
                            system_prompt="ë‹¹ì‹ ì€ ì±„ìš©ê³µê³ ì—ì„œ íšŒì‚¬ëª…ê³¼ ì§ë¬´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤.",
                        ):
                            title_response += chunk

                        chat_title = title_response.strip()
                        logger.info(f"âœ… [0/3] ì±„íŒ…ë°© ì œëª©: {chat_title}")
                        yield f"data: {json.dumps({'summary': chat_title}, ensure_ascii=False)}{sse_end}"
                    else:
                        logger.warning("âš ï¸ ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì œëª© ì¶”ì¶œ ìƒëµ")
                except Exception as e:
                    logger.error(f"âŒ ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.info("")

                # vLLM ëª¨ë“œ
                if model == "vllm" and rag.vllm:
                    logger.info("ğŸ’° [vLLM ê°€ì„±ë¹„ ëª¨ë“œ] ë¶„ì„ ì‹œì‘")
                    logger.info("   í”„ë¡œì„¸ìŠ¤: EasyOCR â†’ VectorDB ì €ì¥ â†’ VectorDB ì¡°íšŒ â†’ Llama ë¶„ì„")
                    logger.info("")

                    logger.info("ğŸ“‚ [1/3] VectorDBì—ì„œ ì—…ë¡œë“œëœ ë¬¸ì„œ ì¡°íšŒ ì¤‘...")
                    full_context = await rag.retrieve_all_documents(
                        user_id=request.user_id, context_types=["resume", "job_posting"]
                    )

                    if not full_context:
                        logger.error("âš ï¸ VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                        yield sse_error_event(
                            code="VECTORDB_ERROR",
                            status=404,
                            message="VectorDBì— ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.",
                            fallback="ì—…ë¡œë“œëœ ì´ë ¥ì„œ ë˜ëŠ” ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
                        )
                        full_response = ""
                    else:
                        logger.info(f"âœ… [1/3] VectorDB ì¡°íšŒ ì™„ë£Œ: {len(full_context)}ì")
                        logger.info("")

                        logger.info("ğŸ¤– [2/3] Llama ëª¨ë¸ ë¶„ì„ ì‹œì‘...")
                        analysis_prompt = f"""ë‹¤ìŒ ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ë¥¼ ë¶„ì„í•˜ì—¬ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{full_context}

[ì¤‘ìš”] ì „ì²´ ì‘ë‹µì€ ë°˜ë“œì‹œ 1500ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

ì•„ë˜ í˜•ì‹ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”:

ì§€ì› íšŒì‚¬ ë° ì§ë¬´ : [íšŒì‚¬ëª…] | [ì§ë¬´ëª…]

ì´ë ¥ì„œ ë¶„ì„

ì¥ì 
1. [êµ¬ì²´ì ì¸ ì¥ì  1]
2. [êµ¬ì²´ì ì¸ ì¥ì  2]
3. [êµ¬ì²´ì ì¸ ì¥ì  3]

ë‹¨ì 
1. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  1]
2. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  2]
3. [êµ¬ì²´ì ì¸ ë‹¨ì  ë˜ëŠ” ë³´ì™„ì  3]

ì±„ìš© ê³µê³  ë¶„ì„

í•„ìˆ˜ ì—­ëŸ‰
- [í•„ìˆ˜ ì—­ëŸ‰ 1]
- [í•„ìˆ˜ ì—­ëŸ‰ 2]
- [í•„ìˆ˜ ì—­ëŸ‰ 3]

ë§¤ì¹­ë„

ë§ëŠ” ì 
- [ë§¤ì¹­ë˜ëŠ” ì—­ëŸ‰ 1]
- [ë§¤ì¹­ë˜ëŠ” ì—­ëŸ‰ 2]

ë³´ì™„í•  ì 
- [ë¶€ì¡±í•œ ì—­ëŸ‰ 1]
- [ë¶€ì¡±í•œ ì—­ëŸ‰ 2]

ì ˆëŒ€ ê¸ˆì§€:
- # ## ### ì œëª© ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€
- ** __ ë³¼ë“œ/ì´íƒ¤ë¦­ ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€
- 1500ì ì´ˆê³¼ ê¸ˆì§€

ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."""

                        async for chunk in rag.vllm.generate_response(
                            user_message=analysis_prompt,
                            context=None,
                            history=[],
                            system_prompt="ë‹¹ì‹ ì€ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•(#, ##, **, ```)ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³  ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.",
                        ):
                            full_response += chunk
                            yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                        logger.info(f"âœ… [3/3] Llama ë¶„ì„ ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")

                # Gemini ëª¨ë“œ
                else:
                    if model == "vllm" and not rag.vllm:
                        logger.warning("âš ï¸ vLLM ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€ â†’ Geminië¡œ ìë™ ë³€ê²½")

                    logger.info("ğŸš€ [Gemini ê³ ì„±ëŠ¥ ëª¨ë“œ] ë¶„ì„ ì‹œì‘")
                    logger.info("   í”„ë¡œì„¸ìŠ¤: RAG ê²€ìƒ‰ â†’ Gemini ë¶„ì„ (ì›ë˜ ë°©ì‹)")
                    logger.info("")

                    logger.info("ğŸ“‚ [1/2] RAG ê²€ìƒ‰ ì¤‘...")
                    async for chunk in rag.chat_with_rag(
                        user_message=user_message,
                        user_id=request.user_id,
                        history=history_dict,
                        use_rag=True,
                        context_types=["resume", "job_posting"],
                        model="gemini",
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                    logger.info(f"âœ… [2/2] Gemini ë¶„ì„ ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")
            else:
                # ì¼ë°˜ ëŒ€í™”
                logger.info("ğŸ’¬ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ")
                logger.info("")

                if is_followup:
                    interview_qa_count = len([h for h in history_dict if h.get("role") == "user"])
                    logger.info(f"ğŸ“Š í˜„ì¬ ë©´ì ‘ ë‹µë³€ ìˆ˜: {interview_qa_count}ê°œ")

                    if interview_qa_count >= 5:
                        logger.info("ğŸ¯ [ë©´ì ‘ ì¢…ë£Œ] 5ê°œ ë‹µë³€ ì™„ë£Œ â†’ í”¼ë“œë°± ìƒì„± ì‹œì‘")

                        end_msg = "ë©´ì ‘ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹µë³€ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n\n"
                        yield f"data: {json.dumps({'chunk': end_msg}, ensure_ascii=False)}{sse_end}"
                        full_response += end_msg

                        qa_pairs = []
                        for i in range(0, len(history_dict), 2):
                            if i + 1 < len(history_dict):
                                qa_pairs.append(
                                    {
                                        "question": history_dict[i].get("content", ""),
                                        "answer": history_dict[i + 1].get("content", ""),
                                    }
                                )

                        feedback_prompt = (
                            "ë‹¤ìŒ ë©´ì ‘ Q&Aì— ëŒ€í•´ ê° ë‹µë³€ë§ˆë‹¤ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”:\n\n"
                        )
                        for i, qa in enumerate(qa_pairs[:5], 1):
                            feedback_prompt += (
                                f"ì§ˆë¬¸ {i}: {qa['question']}\në‹µë³€ {i}: {qa['answer']}\n\n"
                            )

                        feedback_prompt += """ê° ë‹µë³€ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í”¼ë“œë°±í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸ 1
[ì§ˆë¬¸ ë‚´ìš©]

ë‹µë³€ 1
[ë‹µë³€ ë‚´ìš©]

í‰ê°€
- ì˜í•œ ì : [êµ¬ì²´ì ìœ¼ë¡œ]
- ê°œì„ ì : [êµ¬ì²´ì ìœ¼ë¡œ]
- ì¶”ì²œ ë‹µë³€: [ë” ë‚˜ì€ ë‹µë³€ ì˜ˆì‹œ]

(ì§ˆë¬¸ 2~5ë„ ê°™ì€ í˜•ì‹ìœ¼ë¡œ)

ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•(#, **, ```)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."""

                        async for chunk in rag.llm.generate_response(
                            user_message=feedback_prompt,
                            context=None,
                            history=[],
                            system_prompt="ë‹¹ì‹ ì€ ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.",
                            user_id=request.user_id,
                        ):
                            full_response += chunk
                            yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                        logger.info(
                            f"âœ… [ë©´ì ‘ í”¼ë“œë°±] ìƒì„± ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)"
                        )

                    else:
                        original_question = history_dict[-2].get("content", "")
                        candidate_answer = history_dict[-1].get("content", "")

                        logger.info("ğŸ” [ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±] ê°ì§€")
                        logger.info(f"   ì›ë³¸ ì§ˆë¬¸: {original_question[:50]}...")
                        logger.info(f"   ë‹µë³€: {candidate_answer[:50]}...")
                        logger.info("")

                        star_analysis = {
                            "situation": "unknown",
                            "task": "unknown",
                            "action": "unknown",
                            "result": "unknown",
                        }

                        async for chunk in rag.generate_followup_question(
                            original_question=original_question,
                            candidate_answer=candidate_answer,
                            star_analysis=star_analysis,
                            model=model,
                            user_id=request.user_id,
                        ):
                            full_response += chunk
                            yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                        logger.info(f"âœ… [ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±] ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì)")

                else:
                    if (
                        "ë©´ì ‘ ì§ˆë¬¸" in user_message
                        or "ë©´ì ‘ì§ˆë¬¸" in user_message
                        or "ë©´ì ‘" in user_message
                    ):
                        context_types = ["portfolio"]
                        logger.info("ğŸ¯ ë©´ì ‘ ì§ˆë¬¸ ìš”ì²­ ê°ì§€ â†’ portfolio ì»¬ë ‰ì…˜ë§Œ ê²€ìƒ‰")
                    else:
                        context_types = ["resume", "job_posting", "portfolio"]
                        logger.info("ğŸ“š ì¼ë°˜ ëŒ€í™” â†’ ëª¨ë“  ì»¬ë ‰ì…˜ ê²€ìƒ‰")

                    logger.info("")

                    logger.info("ğŸ” RAG ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì‹œì‘...")
                    async for chunk in rag.chat_with_rag(
                        user_message=user_message,
                        user_id=request.user_id,
                        history=history_dict,
                        use_rag=True,
                        context_types=context_types,
                        model=model,
                    ):
                        full_response += chunk
                        yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

                    logger.info("âœ… ì¼ë°˜ ëŒ€í™” ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: %dì)", len(full_response))

        except Exception as e:
            logger.error("ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: %s", str(e), exc_info=True)
            yield sse_error_event(
                code="INTERNAL_ERROR",
                status=500,
                message=str(e),
                fallback="ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            )
            full_response = ""

        yield f"data: [DONE]{sse_end}"

    # =========================================================================
    # 2. ë©´ì ‘ ëª¨ë“œ - 5ê°œ ì§ˆë¬¸ Ã— ìµœëŒ€ 3 depths ê¼¬ë¦¬ì§ˆë¬¸
    # =========================================================================
    elif mode == ChatMode.INTERVIEW:
        try:
            interview_type = request.context.interview_type or "tech"
            interview_type_kr = "ê¸°ìˆ " if interview_type == "tech" else "ì¸ì„±"

            session_key = get_session_key(request.user_id, request.interview_id)
            user_message = request.message or ""

            session = request.context.interview_session
            if session is None:
                session_data = await session_store.get(session_key)
                session = InterviewSession.model_validate(session_data) if session_data else None
                if session:
                    safe_info(
                        logger,
                        "ğŸ“¦ [ë©´ì ‘] ìºì‹œì—ì„œ ì„¸ì…˜ ë³µì›: %s, phase=%s, Q%d/5",
                        session_key,
                        session.phase,
                        session.current_question_id,
                    )

            model_choice = (
                request.model.value if hasattr(request.model, "value") else str(request.model)
            )

            # PHASE 1: ì„¸ì…˜ ì´ˆê¸°í™”
            if session is None or session.phase == "init":
                logger.info("ğŸ¯ [ë©´ì ‘] ì„¸ì…˜ ì´ˆê¸°í™” - 5ê°œ ì§ˆë¬¸ ì„¸íŠ¸ ìƒì„± ì‹œì‘")

                context = await rag.retrieve_context(
                    query=f"{interview_type_kr} ë©´ì ‘ ì§ˆë¬¸ì„ ìœ„í•œ ì‚¬ìš©ì ì •ë³´",
                    user_id=request.user_id,
                    context_types=["resume", "portfolio", "job_posting"],
                    n_results=3,
                )

                resume_ocr = request.context.resume_ocr if request.context else None
                job_posting_ocr = request.context.job_posting_ocr if request.context else None
                portfolio_text = request.context.portfolio_text if request.context else None

                init_prompt = create_tech_interview_init_prompt(
                    resume_text=resume_ocr or context or "ì •ë³´ ì—†ìŒ",
                    job_posting_text=job_posting_ocr or "ì •ë³´ ì—†ìŒ",
                    portfolio_text=portfolio_text or context or "ì •ë³´ ì—†ìŒ",
                )

                system_prompt = get_system_tech_interview()

                if model_choice == "vllm" and rag.vllm:
                    full_response = ""
                    async for chunk in rag.vllm.generate_response(
                        user_message=init_prompt,
                        context=None,
                        history=[],
                        system_prompt=system_prompt,
                    ):
                        full_response += chunk
                else:
                    full_response = await rag.llm.generate_response_non_stream(
                        user_message=init_prompt,
                        context=None,
                        system_prompt=system_prompt,
                        user_id=request.user_id,
                    )

                # JSON íŒŒì‹±í•˜ì—¬ ì„¸ì…˜ ìƒì„±
                try:
                    json_content = re.sub(r"```json\s*", "", full_response)
                    json_content = re.sub(r"```\s*$", "", json_content)
                    json_content = json_content.strip()

                    json_start = json_content.find("{")
                    json_end = json_content.rfind("}") + 1

                    if json_start != -1 and json_end > json_start:
                        json_str = json_content[json_start:json_end]
                        logger.info(f"JSON íŒŒì‹± ì‹œë„ (ì²« 200ì): {json_str[:200]}")

                        questions_data = json.loads(json_str)

                        new_session = InterviewSession(
                            session_id=str(uuid.uuid4()),
                            interview_type=interview_type,
                            questions=[
                                InterviewQuestionState(
                                    id=q["id"],
                                    category=q["category"],
                                    category_name=q["category_name"],
                                    question=q["question"],
                                    intent=q.get("intent", ""),
                                    keywords=q.get("keywords", []),
                                )
                                for q in questions_data.get("questions", [])
                            ],
                            current_question_id=1,
                            phase="questioning",
                        )

                        logger.info("âœ… ë©´ì ‘ ì§ˆë¬¸ ì„¸íŠ¸ ìƒì„± ì™„ë£Œ: %dê°œ", len(new_session.questions))

                        await session_store.set(session_key, new_session.model_dump())
                        safe_info(logger, "ğŸ’¾ [ë©´ì ‘] ì„¸ì…˜ ì €ì¥: %s", session_key)

                        first_q = new_session.questions[0] if new_session.questions else None
                        if first_q:
                            question_text = (
                                f"[{interview_type_kr}ë©´ì ‘ 1/5]{newline}{first_q.question}"
                            )
                            for char in question_text:
                                yield f"data: {json.dumps({'chunk': char}, ensure_ascii=False)}{sse_end}"
                                await asyncio.sleep(0.015)

                            session_meta = {
                                "type": "session_state",
                                "session": new_session.model_dump(),
                            }
                            yield f"data: {json.dumps(session_meta, ensure_ascii=False)}{sse_end}"
                    else:
                        raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

                except (json.JSONDecodeError, ValueError) as e:
                    logger.error(f"ì§ˆë¬¸ ì„¸íŠ¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    logger.error(f"ì›ë³¸ ì‘ë‹µ (ì²« 500ì): {full_response[:500]}")

                    yield sse_error_event(
                        code="PARSE_FAILED",
                        status=500,
                        message=f"ì§ˆë¬¸ ì„¸íŠ¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}",
                        fallback="ë©´ì ‘ ì§ˆë¬¸ ì„¸íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    )

                yield f"data: [DONE]{sse_end}"

            # PHASE 2: ê¼¬ë¦¬ì§ˆë¬¸ ë˜ëŠ” ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±
            elif session.phase in ["questioning", "followup"]:
                current_q_id = session.current_question_id
                current_q = next((q for q in session.questions if q.id == current_q_id), None)

                if not current_q:
                    yield sse_error_event(
                        code="SESSION_NOT_FOUND",
                        status=404,
                        message=f"ë©´ì ‘ ì„¸ì…˜ì—ì„œ ì§ˆë¬¸ ID {current_q_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        fallback="ì„¸ì…˜ ì˜¤ë¥˜: í˜„ì¬ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    )
                    yield f"data: [DONE]{sse_end}"
                    return

                current_q.conversation.append(
                    {
                        "role": "candidate",
                        "content": user_message,
                    }
                )
                current_q.current_depth += 1

                if current_q.current_depth < current_q.max_depth:
                    followup_prompt = create_tech_followup_prompt(
                        question_id=current_q.id,
                        category_name=current_q.category_name,
                        original_question=current_q.question,
                        conversation_history=format_conversation_history(current_q.conversation),
                        last_answer=user_message,
                        current_depth=current_q.current_depth,
                    )

                    full_response = ""
                    system_prompt = get_system_tech_interview()

                    if model_choice == "vllm" and rag.vllm:
                        async for chunk in rag.vllm.generate_response(
                            user_message=followup_prompt,
                            context=None,
                            history=[],
                            system_prompt=system_prompt,
                        ):
                            full_response += chunk
                    else:
                        async for chunk in rag.llm.generate_response(
                            user_message=followup_prompt,
                            context=None,
                            history=[],
                            system_prompt=system_prompt,
                            user_id=request.user_id,
                        ):
                            full_response += chunk

                    try:
                        json_start = full_response.find("{")
                        json_end = full_response.rfind("}") + 1
                        if json_start != -1 and json_end > json_start:
                            followup_data = json.loads(full_response[json_start:json_end])

                            if followup_data.get("should_continue", True) and followup_data.get(
                                "followup"
                            ):
                                followup_q = followup_data["followup"]["question"]
                                current_q.conversation.append(
                                    {
                                        "role": "interviewer",
                                        "content": followup_q,
                                    }
                                )
                                session.phase = "followup"

                                followup_header = f"[{interview_type_kr}ë©´ì ‘ {current_q_id}-{current_q.current_depth}/5]"
                                followup_text = f"{followup_header}{newline}{followup_q}"
                                for char in followup_text:
                                    yield f"data: {json.dumps({'chunk': char}, ensure_ascii=False)}{sse_end}"
                                    await asyncio.sleep(0.015)
                            else:
                                current_q.is_completed = True
                    except json.JSONDecodeError as e:
                        logger.error(f"ê¼¬ë¦¬ì§ˆë¬¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        yield sse_error_event(
                            code="PARSE_FAILED",
                            status=500,
                            message=f"ê¼¬ë¦¬ì§ˆë¬¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}",
                            fallback="ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.",
                        )
                        current_q.is_completed = True

                else:
                    current_q.is_completed = True

                if current_q.is_completed:
                    next_q_id = current_q_id + 1
                    if next_q_id <= session.total_questions:
                        next_q = next((q for q in session.questions if q.id == next_q_id), None)
                        if next_q:
                            session.current_question_id = next_q_id
                            session.phase = "questioning"

                            question_header = f"[{interview_type_kr}ë©´ì ‘ {next_q_id}/5]"
                            question_text = f"{question_header}{newline}{next_q.question}"
                            for char in question_text:
                                yield f"data: {json.dumps({'chunk': char}, ensure_ascii=False)}{sse_end}"
                                await asyncio.sleep(0.015)

                            next_q.conversation.append(
                                {
                                    "role": "interviewer",
                                    "content": next_q.question,
                                }
                            )
                    else:
                        session.phase = "completed"
                        complete_msg = f"{newline}{newline}ë©´ì ‘ ê²°ê³¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”."
                        for char in complete_msg:
                            yield f"data: {json.dumps({'chunk': char}, ensure_ascii=False)}{sse_end}"
                            await asyncio.sleep(0.015)

                await session_store.set(session_key, session.model_dump())
                safe_info(
                    logger,
                    "ğŸ’¾ [ë©´ì ‘] ì„¸ì…˜ ì—…ë°ì´íŠ¸: %s, phase=%s, Q%d/5",
                    session_key,
                    session.phase,
                    session.current_question_id,
                )

                if session.phase == "completed":
                    await session_store.delete(session_key)
                    safe_info(logger, "ğŸ—‘ï¸ [ë©´ì ‘] ì™„ë£Œëœ ì„¸ì…˜ ì‚­ì œ: %s", session_key)

                session_meta = {
                    "type": "session_state",
                    "session": session.model_dump(),
                }
                yield f"data: {json.dumps(session_meta, ensure_ascii=False)}{sse_end}"
                yield f"data: [DONE]{sse_end}"

            # PHASE 3: ë©´ì ‘ ì™„ë£Œ
            elif session.phase == "completed":
                await session_store.delete(session_key)
                complete_msg = "ë©´ì ‘ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë¦¬í¬íŠ¸ ëª¨ë“œì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”."
                yield f"data: {json.dumps({'chunk': complete_msg}, ensure_ascii=False)}{sse_end}"
                yield f"data: [DONE]{sse_end}"

        except Exception as e:
            logger.error(f"Interview error: {e}", exc_info=True)
            yield sse_error_event(
                code="INTERNAL_ERROR",
                status=500,
                message=str(e),
                fallback="ë©´ì ‘ ì§„í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            )
            yield f"data: [DONE]{sse_end}"

    # =========================================================================
    # 3. ë¦¬í¬íŠ¸ ëª¨ë“œ - ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
    # =========================================================================
    elif mode == ChatMode.REPORT:
        try:
            interview_type = request.context.interview_type or "tech"
            interview_type_kr = "ê¸°ìˆ " if interview_type == "tech" else "ì¸ì„±"
            qa_list = request.context.qa_list or []

            if not qa_list:
                yield f"data: [DONE]{sse_end}"
                return

            content = f"{interview_type_kr} ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...{newline}"
            yield f"data: {json.dumps({'chunk': content}, ensure_ascii=False)}{sse_end}"

            qa_text = ""
            for i, qa in enumerate(qa_list, 1):
                q = qa.get("question", "")
                a = qa.get("answer", "")
                qa_text += f"ì§ˆë¬¸ {i}: {q}\në‹µë³€ {i}: {a}\n\n"

            report_prompt = f"""
ë‹¤ìŒì€ {interview_type_kr} ë©´ì ‘ Q&A ê¸°ë¡ì…ë‹ˆë‹¤:

{qa_text}

ìœ„ ë©´ì ‘ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ê° ë‹µë³€ì— ëŒ€í•œ ê°œë³„ í‰ê°€ (ì˜í•œ ì , ê°œì„ ì )
2. ì „ì²´ì ì¸ ê°•ì  íŒ¨í„´
3. ì „ì²´ì ì¸ ì•½ì  íŒ¨í„´
4. í–¥í›„ í•™ìŠµ ê°€ì´ë“œ
"""

            full_report = ""

            model_choice = (
                request.model.value if hasattr(request.model, "value") else str(request.model)
            )

            if model_choice == "vllm" and rag.vllm:
                logger.info("ğŸ“Š [vLLM] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.vllm.generate_response(
                    user_message=report_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
                ):
                    full_report += chunk
                    yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"
            else:
                logger.info("ğŸ“Š [Gemini] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
                async for chunk in rag.llm.generate_response(
                    user_message=report_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
                    user_id=request.user_id,
                ):
                    full_report += chunk
                    yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

            yield f"data: [DONE]{sse_end}"

        except Exception as e:
            logger.error(f"Interview report generation error: {e}", exc_info=True)
            yield sse_error_event(
                code="LLM_ERROR",
                status=500,
                message=str(e),
                fallback="ë©´ì ‘ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            )
            yield f"data: [DONE]{sse_end}"

    # Latency ì¸¡ì • ì¢…ë£Œ ë° ì „ì†¡
    try:
        duration = (time.time() - start_time) * 1000
        cw = CloudWatchService.get_instance()
        asyncio.create_task(cw.put_metric("AI_Chat_Latency", duration, "Milliseconds", dims))
    except Exception as e:
        logger.error(f"Failed to record latency metric: {e}")


@router.post(
    "/chat",
    summary="ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° (ì¼ë°˜/ë©´ì ‘/ë¦¬í¬íŠ¸)",
    description="""
    ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    **ì²˜ë¦¬ ë°©ì‹:** ìŠ¤íŠ¸ë¦¬ë° (SSE)

    **ëª¨ë“œ:**
    - normal: ì¼ë°˜ ëŒ€í™”
    - interview: ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
    - report: ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±

    **ë©´ì ‘ íƒ€ì… (context.interview_type):**
    - behavior: ì¸ì„± ë©´ì ‘
    - tech: ê¸°ìˆ  ë©´ì ‘
    """,
    responses={
        400: {
            "description": "Bad Request",
            "content": {
                "application/json": {
                    "examples": {
                        "invalid_request": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_REQUEST",
                                    "message": "room_idëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤",
                                    "field": "room_id",
                                }
                            }
                        },
                        "invalid_mode": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_MODE",
                                    "message": "modeëŠ” normal ë˜ëŠ” interview ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤",
                                    "field": "context.mode",
                                }
                            }
                        },
                    }
                }
            },
        },
        500: {
            "description": "Internal Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "INTERNAL_ERROR",
                            "message": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        503: {
            "description": "Service Unavailable",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "LLM_UNAVAILABLE",
                            "message": "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
    },
)
async def chat(
    request: ChatRequest,
    session_store=Depends(get_session_store),
):
    """ì±„íŒ… ì²˜ë¦¬ (ì¼ë°˜/ë©´ì ‘)"""
    return StreamingResponse(
        generate_chat_stream(request, session_store),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )
