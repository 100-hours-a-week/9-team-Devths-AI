"""
ë©´ì ‘ ë‹µë³€ í‰ê°€ API ì—”ë“œí¬ì¸íŠ¸ (í†µí•©).

POST /ai/evaluation/analyze - ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± (SSE ìŠ¤íŠ¸ë¦¬ë°)
  - retry=false: Gemini ë‹¨ë… ë¶„ì„
  - retry=true:  GeminiÃ—GPT-4o í† ë¡  í›„ ìµœì¢… ë¦¬í¬íŠ¸
"""

import json
import logging
import re

from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse

from app.api.routes.v2._helpers import get_services
from app.api.routes.v2._sse_errors import sse_error_event
from app.config.dependencies import (
    get_debate_service,
    get_evaluation_analyzer,
)
from app.domain.evaluation.analyzer import InterviewAnalyzer
from app.domain.evaluation.debate_graph import DebateService
from app.schemas.evaluation import (
    AnalyzeInterviewRequest,
)

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/evaluation")


def _sanitize_log_value(value: str, max_length: int = 50) -> str:
    """Log Injection ë°©ì§€ë¥¼ ìœ„í•´ ë¡œê·¸ ê°’ì„ ì •ì œí•©ë‹ˆë‹¤."""
    sanitized = re.sub(r"[\r\n\t]", "", str(value))
    if len(sanitized) > max_length:
        sanitized = sanitized[:max_length] + "..."
    return sanitized


# ============================================
# Gemini ë‹¨ë… ë¶„ì„ ìŠ¤íŠ¸ë¦¬ë° (retry=false)
# ============================================


async def _generate_analyze_stream(request: AnalyzeInterviewRequest):
    """ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ SSE ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (Gemini ë‹¨ë…)."""
    sse_end = "\n\n"
    qa_list = request.context or []

    if not qa_list:
        yield sse_error_event(
            code="EMPTY_CONTEXT",
            status=400,
            message="í‰ê°€í•  Q&A ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
            fallback="ë©´ì ‘ ë¬¸ë‹µ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. contextì— Q&A ëª©ë¡ì„ ì „ë‹¬í•´ì£¼ì„¸ìš”.",
        )
        yield f"data: [DONE]{sse_end}"
        return

    rag = get_services()

    content = "ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\n"
    yield f"data: {json.dumps({'chunk': content}, ensure_ascii=False)}{sse_end}"

    # Q&A í…ìŠ¤íŠ¸ êµ¬ì„±
    qa_text = ""
    for i, qa in enumerate(qa_list, 1):
        q = qa.get("question", "")
        a = qa.get("answer", "")
        qa_text += f"ì§ˆë¬¸ {i}: {q}\në‹µë³€ {i}: {a}\n\n"

    report_prompt = f"""
ë‹¤ìŒì€ ë©´ì ‘ Q&A ê¸°ë¡ì…ë‹ˆë‹¤:

{qa_text}

ìœ„ ë©´ì ‘ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ê° ë‹µë³€ì— ëŒ€í•œ ê°œë³„ í‰ê°€ (ì˜í•œ ì , ê°œì„ ì )
2. ì „ì²´ì ì¸ ê°•ì  íŒ¨í„´
3. ì „ì²´ì ì¸ ì•½ì  íŒ¨í„´
4. í–¥í›„ í•™ìŠµ ê°€ì´ë“œ
"""

    full_report = ""
    model_choice = (
        request.model.value if hasattr(request.model, "value") else str(request.model)
    )

    try:
        if model_choice == "vllm" and rag.vllm:
            logger.info("ğŸ“Š [vLLM] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
            async for chunk in rag.vllm.generate_response(
                user_message=report_prompt,
                context=None,
                history=[],
                system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
            ):
                full_report += chunk
                yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"
        else:
            logger.info("ğŸ“Š [Gemini] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
            async for chunk in rag.llm.generate_response(
                user_message=report_prompt,
                context=None,
                history=[],
                system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
                user_id=request.user_id,
            ):
                full_report += chunk
                yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

        logger.info("âœ… ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: %dì)", len(full_report))

    except Exception as e:
        logger.error("ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: %s", str(e), exc_info=True)
        yield sse_error_event(
            code="LLM_ERROR",
            status=500,
            message=str(e),
            fallback="ë©´ì ‘ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        )

    yield f"data: [DONE]{sse_end}"


# ============================================
# GeminiÃ—GPT-4o í† ë¡  ìŠ¤íŠ¸ë¦¬ë° (retry=true)
# ============================================


async def _generate_debate_stream(
    request: AnalyzeInterviewRequest,
    analyzer: InterviewAnalyzer,
    debate_service: DebateService,
):
    """GeminiÃ—GPT-4o í† ë¡  í›„ ìµœì¢… ë¦¬í¬íŠ¸ SSE ìŠ¤íŠ¸ë¦¬ë° ìƒì„±."""
    sse_end = "\n\n"
    qa_list = request.context or []

    if not qa_list:
        yield sse_error_event(
            code="EMPTY_CONTEXT",
            status=400,
            message="í‰ê°€í•  Q&A ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
            fallback="ë©´ì ‘ ë¬¸ë‹µ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. contextì— Q&A ëª©ë¡ì„ ì „ë‹¬í•´ì£¼ì„¸ìš”.",
        )
        yield f"data: [DONE]{sse_end}"
        return

    # Q&A ë³€í™˜
    qa_pairs = [
        {
            "question": qa.get("question", ""),
            "answer": qa.get("answer", ""),
            "category": qa.get("category", ""),
        }
        for qa in qa_list
    ]

    try:
        # 1ë‹¨ê³„: Gemini êµ¬ì¡°í™” ë¶„ì„
        progress = "ğŸ” Gemini ë¶„ì„ ì¤‘...\n"
        yield f"data: {json.dumps({'chunk': progress}, ensure_ascii=False)}{sse_end}"

        logger.info("ğŸ“Š [Debate] 1ë‹¨ê³„: Gemini êµ¬ì¡°í™” ë¶„ì„ ì‹œì‘")
        gemini_result = await analyzer.analyze(qa_pairs=qa_pairs)
        gemini_dict = gemini_result.to_dict()
        logger.info("âœ… [Debate] 1ë‹¨ê³„ ì™„ë£Œ: overall_score=%d", gemini_result.overall_score)

        # 2ë‹¨ê³„: GeminiÃ—GPT-4o í† ë¡ 
        progress = "ğŸ¤– GPT-4o ì‹¬ì¸µ ë¶„ì„ ë° í† ë¡  ì¤‘...\n"
        yield f"data: {json.dumps({'chunk': progress}, ensure_ascii=False)}{sse_end}"

        logger.info("ğŸ“Š [Debate] 2ë‹¨ê³„: GeminiÃ—GPT-4o í† ë¡  ì‹œì‘")
        debate_result = await debate_service.run_debate(
            qa_pairs=qa_pairs,
            gemini_analysis=gemini_dict,
        )
        logger.info(
            "âœ… [Debate] 2ë‹¨ê³„ ì™„ë£Œ: consensus=%s", debate_result.consensus_method
        )

        # 3ë‹¨ê³„: ìµœì¢… ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ë¡œ SSE ìŠ¤íŠ¸ë¦¬ë°
        progress = f"\nğŸ“‹ í† ë¡  ì™„ë£Œ (í•©ì˜ ë°©ë²•: {debate_result.consensus_method})\n\n"
        yield f"data: {json.dumps({'chunk': progress}, ensure_ascii=False)}{sse_end}"

        final = debate_result.final_analysis

        # ê° ì§ˆë¬¸ë³„ í‰ê°€ ì¶œë ¥
        for i, q in enumerate(final.questions, 1):
            report_text = f"â”â”â” ì§ˆë¬¸ {i} â”â”â”\n"
            report_text += f"ì§ˆë¬¸: {q.question}\n"
            report_text += f"ë‹µë³€: {q.user_answer}\n"
            report_text += f"íŒì •: {q.verdict} (ì ìˆ˜: {q.score}/5)\n"
            report_text += f"í‰ê°€: {q.reasoning}\n"
            if q.recommended_answer:
                report_text += f"ì¶”ì²œ ë‹µë³€: {q.recommended_answer}\n"
            report_text += "\n"
            yield f"data: {json.dumps({'chunk': report_text}, ensure_ascii=False)}{sse_end}"

        # ì¢…í•© í”¼ë“œë°±
        summary_text = "â”â”â” ì¢…í•© í‰ê°€ â”â”â”\n"
        summary_text += f"ì¢…í•© ì ìˆ˜: {final.overall_score}/5\n"
        summary_text += f"ì¢…í•© í”¼ë“œë°±: {final.overall_feedback}\n\n"

        if final.strengths:
            summary_text += "ê°•ì :\n"
            for s in final.strengths:
                summary_text += f"  - {s}\n"
            summary_text += "\n"

        if final.improvements:
            summary_text += "ê°œì„ ì :\n"
            for imp in final.improvements:
                summary_text += f"  - {imp}\n"
            summary_text += "\n"

        yield f"data: {json.dumps({'chunk': summary_text}, ensure_ascii=False)}{sse_end}"

        logger.info("âœ… [Debate] ìµœì¢… ë¦¬í¬íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")

    except Exception as e:
        logger.error("Debate ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: %s", str(e), exc_info=True)
        yield sse_error_event(
            code="LLM_ERROR",
            status=500,
            message=str(e),
            fallback="ì‹¬ì¸µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        )

    yield f"data: [DONE]{sse_end}"


# ============================================
# í†µí•© ì—”ë“œí¬ì¸íŠ¸
# ============================================


@router.post(
    "/analyze",
    summary="ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±",
    description="""
    ë©´ì ‘ ì¢…ë£Œ ì‹œ Q&A ë°ì´í„°ë¥¼ ë°›ì•„ SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    **retry=false (ê¸°ë³¸):** Gemini ë‹¨ë… ë¶„ì„ â†’ SSE ë¦¬í¬íŠ¸
    **retry=true (ë‹µë³€ ë‹¤ì‹œ ë°›ê¸°):** GeminiÃ—GPT-4o í† ë¡  â†’ SSE ë¦¬í¬íŠ¸
    """,
)
async def analyze_interview(
    request: AnalyzeInterviewRequest,
    analyzer: InterviewAnalyzer = Depends(get_evaluation_analyzer),
    debate_service: DebateService | None = Depends(get_debate_service),
):
    """ë©´ì ‘ Q&A ê¸°ë°˜ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    logger.info(
        "Analyze interview: session=%s, user=%s, questions=%d, retry=%s",
        _sanitize_log_value(str(request.session_id)),
        _sanitize_log_value(str(request.user_id)),
        len(request.context),
        request.retry,
    )

    # retry=true â†’ GeminiÃ—GPT-4o í† ë¡ 
    if request.retry:
        if not debate_service:
            # debate ì„œë¹„ìŠ¤ ë¯¸ì‚¬ìš© ì‹œ Gemini ë‹¨ë…ìœ¼ë¡œ í´ë°±
            logger.warning("Debate service unavailable, falling back to Gemini-only")
            return StreamingResponse(
                _generate_analyze_stream(request),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no",
                },
            )

        return StreamingResponse(
            _generate_debate_stream(request, analyzer, debate_service),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )

    # retry=false â†’ Gemini ë‹¨ë… ë¶„ì„
    return StreamingResponse(
        _generate_analyze_stream(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )
