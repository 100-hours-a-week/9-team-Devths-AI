"""
ë©´ì ‘ ë‹µë³€ í‰ê°€ API ì—”ë“œí¬ì¸íŠ¸ (í†µí•©).

POST /ai/evaluation/analyze - ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± (SSE ìŠ¤íŠ¸ë¦¬ë°)
  - retry=false: Gemini ë‹¨ë… ë¶„ì„
  - retry=true:  GeminiÃ—GPT-4o í† ë¡  í›„ ìµœì¢… ë¦¬í¬íŠ¸
"""

import json
import logging
import re

from fastapi import APIRouter, Depends
from fastapi.responses import StreamingResponse

from app.api.routes.v2._helpers import get_services
from app.api.routes.v2._sse_errors import sse_error_event
from app.config.dependencies import (
    get_debate_service,
    get_evaluation_analyzer,
)
from app.domain.evaluation.analyzer import InterviewAnalyzer
from app.domain.evaluation.debate_graph import DebateService
from app.schemas.evaluation import (
    AnalyzeInterviewRequest,
)

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/evaluation")


def _sanitize_log_value(value: str, max_length: int = 50) -> str:
    """Log Injection ë°©ì§€ë¥¼ ìœ„í•´ ë¡œê·¸ ê°’ì„ ì •ì œí•©ë‹ˆë‹¤."""
    sanitized = re.sub(r"[\r\n\t]", "", str(value))
    if len(sanitized) > max_length:
        sanitized = sanitized[:max_length] + "..."
    return sanitized


def _validate_qa_list(qa_list: list[dict]) -> str | None:
    """Q&A ë¦¬ìŠ¤íŠ¸ì˜ í˜•ì‹ì„ ê²€ì¦í•©ë‹ˆë‹¤.

    Returns:
        ì—ëŸ¬ ë©”ì‹œì§€ (ì •ìƒì´ë©´ None)
    """
    for i, qa in enumerate(qa_list):
        if "question" not in qa or "answer" not in qa:
            return f"context[{i}]ì— question, answer í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤."
    return None


# ============================================
# Gemini ë‹¨ë… ë¶„ì„ ìŠ¤íŠ¸ë¦¬ë° (retry=false)
# ============================================


async def _generate_analyze_stream(request: AnalyzeInterviewRequest):
    """ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ SSE ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (Gemini ë‹¨ë…)."""
    sse_end = "\n\n"
    qa_list = request.context or []

    # context ë¹ˆ ë°°ì—´ ê²€ì‚¬
    if not qa_list:
        yield sse_error_event(
            code="EMPTY_CONTEXT",
            status=400,
            message="í‰ê°€í•  Q&A ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
            fallback="ë©´ì ‘ ë¬¸ë‹µ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. contextì— Q&A ëª©ë¡ì„ ì „ë‹¬í•´ì£¼ì„¸ìš”.",
        )
        yield f"data: [DONE]{sse_end}"
        return

    # context Q&A í˜•ì‹ ê²€ì¦
    validation_error = _validate_qa_list(qa_list)
    if validation_error:
        yield sse_error_event(
            code="INVALID_CONTEXT",
            status=400,
            message=validation_error,
            fallback="Q&A ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ê° í•­ëª©ì— questionê³¼ answerë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.",
        )
        yield f"data: [DONE]{sse_end}"
        return

    # RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    try:
        rag = get_services()
    except Exception as e:
        logger.error("RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: %s", str(e), exc_info=True)
        yield sse_error_event(
            code="LLM_UNAVAILABLE",
            status=503,
            message=str(e),
            fallback="AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        )
        yield f"data: [DONE]{sse_end}"
        return

    content = "ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\n"
    yield f"data: {json.dumps({'chunk': content}, ensure_ascii=False)}{sse_end}"

    # Q&A í…ìŠ¤íŠ¸ êµ¬ì„±
    qa_text = ""
    for i, qa in enumerate(qa_list, 1):
        q = qa.get("question", "")
        a = qa.get("answer", "")
        qa_text += f"ì§ˆë¬¸ {i}: {q}\në‹µë³€ {i}: {a}\n\n"

    report_prompt = f"""
ë‹¤ìŒì€ ë©´ì ‘ Q&A ê¸°ë¡ì…ë‹ˆë‹¤:

{qa_text}

ìœ„ ë©´ì ‘ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ê° ë‹µë³€ì— ëŒ€í•œ ê°œë³„ í‰ê°€ (ì˜í•œ ì , ê°œì„ ì )
2. ì „ì²´ì ì¸ ê°•ì  íŒ¨í„´
3. ì „ì²´ì ì¸ ì•½ì  íŒ¨í„´
4. í–¥í›„ í•™ìŠµ ê°€ì´ë“œ
"""

    full_report = ""
    model_choice = (
        request.model.value if hasattr(request.model, "value") else str(request.model)
    )

    try:
        if model_choice == "vllm" and rag.vllm:
            logger.info("ğŸ“Š [vLLM] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
            async for chunk in rag.vllm.generate_response(
                user_message=report_prompt,
                context=None,
                history=[],
                system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
            ):
                full_report += chunk
                yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"
        else:
            logger.info("ğŸ“Š [Gemini] ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
            async for chunk in rag.llm.generate_response(
                user_message=report_prompt,
                context=None,
                history=[],
                system_prompt="ë‹¹ì‹ ì€ ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
                user_id=request.user_id,
            ):
                full_report += chunk
                yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}{sse_end}"

        logger.info("âœ… ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: %dì)", len(full_report))

    except ConnectionError as e:
        logger.error("LLM ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨: %s", str(e), exc_info=True)
        yield sse_error_event(
            code="LLM_UNAVAILABLE",
            status=503,
            message=str(e),
            fallback="AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        )
    except Exception as e:
        logger.error("ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: %s", str(e), exc_info=True)
        yield sse_error_event(
            code="LLM_ERROR",
            status=500,
            message=str(e),
            fallback="ë©´ì ‘ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        )

    yield f"data: [DONE]{sse_end}"


# ============================================
# GeminiÃ—GPT-4o í† ë¡  ìŠ¤íŠ¸ë¦¬ë° (retry=true)
# ============================================


def _format_analysis_as_text(analysis) -> str:
    """InterviewAnalysis ê°ì²´ë¥¼ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    lines = []

    for i, q in enumerate(analysis.questions, 1):
        lines.append(f"â”â”â” ì§ˆë¬¸ {i} â”â”â”")
        lines.append(f"ì§ˆë¬¸: {q.question}")
        lines.append(f"ë‹µë³€: {q.user_answer}")
        lines.append(f"íŒì •: {q.verdict} (ì ìˆ˜: {q.score}/5)")
        lines.append(f"í‰ê°€: {q.reasoning}")
        if q.recommended_answer:
            lines.append(f"ì¶”ì²œ ë‹µë³€: {q.recommended_answer}")
        lines.append("")

    lines.append("â”â”â” ì¢…í•© í‰ê°€ â”â”â”")
    lines.append(f"ì¢…í•© ì ìˆ˜: {analysis.overall_score}/5")
    lines.append(f"ì¢…í•© í”¼ë“œë°±: {analysis.overall_feedback}")
    lines.append("")

    if analysis.strengths:
        lines.append("ê°•ì :")
        for s in analysis.strengths:
            lines.append(f"  - {s}")
        lines.append("")

    if analysis.improvements:
        lines.append("ê°œì„ ì :")
        for imp in analysis.improvements:
            lines.append(f"  - {imp}")
        lines.append("")

    return "\n".join(lines)


async def _generate_debate_stream(
    request: AnalyzeInterviewRequest,
    analyzer: InterviewAnalyzer,
    debate_service: DebateService,
):
    """GeminiÃ—GPT-4o í† ë¡  í›„ ìµœì¢… ë¦¬í¬íŠ¸ SSE ìŠ¤íŠ¸ë¦¬ë° ìƒì„±."""
    sse_end = "\n\n"
    qa_list = request.context or []

    # context ë¹ˆ ë°°ì—´ ê²€ì‚¬
    if not qa_list:
        yield sse_error_event(
            code="EMPTY_CONTEXT",
            status=400,
            message="í‰ê°€í•  Q&A ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
            fallback="ë©´ì ‘ ë¬¸ë‹µ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. contextì— Q&A ëª©ë¡ì„ ì „ë‹¬í•´ì£¼ì„¸ìš”.",
        )
        yield f"data: [DONE]{sse_end}"
        return

    # context Q&A í˜•ì‹ ê²€ì¦
    validation_error = _validate_qa_list(qa_list)
    if validation_error:
        yield sse_error_event(
            code="INVALID_CONTEXT",
            status=400,
            message=validation_error,
            fallback="Q&A ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ê° í•­ëª©ì— questionê³¼ answerë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.",
        )
        yield f"data: [DONE]{sse_end}"
        return

    # Q&A ë³€í™˜
    qa_pairs = [
        {
            "question": qa.get("question", ""),
            "answer": qa.get("answer", ""),
            "category": qa.get("category", ""),
        }
        for qa in qa_list
    ]

    gemini_result = None

    # â”€â”€ 1ë‹¨ê³„: Gemini êµ¬ì¡°í™” ë¶„ì„ â”€â”€
    try:
        progress = "ğŸ” Gemini ë¶„ì„ ì¤‘...\n"
        yield f"data: {json.dumps({'chunk': progress}, ensure_ascii=False)}{sse_end}"

        logger.info("ğŸ“Š [Debate] 1ë‹¨ê³„: Gemini êµ¬ì¡°í™” ë¶„ì„ ì‹œì‘")
        gemini_result = await analyzer.analyze(qa_pairs=qa_pairs)
        gemini_dict = gemini_result.to_dict()
        logger.info("âœ… [Debate] 1ë‹¨ê³„ ì™„ë£Œ: overall_score=%d", gemini_result.overall_score)

    except Exception as e:
        logger.error("Gemini ë¶„ì„ ì‹¤íŒ¨: %s", str(e), exc_info=True)
        yield sse_error_event(
            code="LLM_ERROR",
            status=500,
            message=str(e),
            fallback="Gemini ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        )
        yield f"data: [DONE]{sse_end}"
        return

    # â”€â”€ 2ë‹¨ê³„: GeminiÃ—GPT-4o í† ë¡  â”€â”€
    try:
        progress = "ğŸ¤– GPT-4o ì‹¬ì¸µ ë¶„ì„ ë° í† ë¡  ì¤‘...\n"
        yield f"data: {json.dumps({'chunk': progress}, ensure_ascii=False)}{sse_end}"

        logger.info("ğŸ“Š [Debate] 2ë‹¨ê³„: GeminiÃ—GPT-4o í† ë¡  ì‹œì‘")
        debate_result = await debate_service.run_debate(
            qa_pairs=qa_pairs,
            gemini_analysis=gemini_dict,
        )
        logger.info(
            "âœ… [Debate] 2ë‹¨ê³„ ì™„ë£Œ: consensus=%s", debate_result.consensus_method
        )

        # 3ë‹¨ê³„: ìµœì¢… ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ë¡œ SSE ìŠ¤íŠ¸ë¦¬ë°
        progress = f"\nğŸ“‹ í† ë¡  ì™„ë£Œ (í•©ì˜ ë°©ë²•: {debate_result.consensus_method})\n\n"
        yield f"data: {json.dumps({'chunk': progress}, ensure_ascii=False)}{sse_end}"

        final = debate_result.final_analysis
        report_text = _format_analysis_as_text(final)
        yield f"data: {json.dumps({'chunk': report_text}, ensure_ascii=False)}{sse_end}"

        logger.info("âœ… [Debate] ìµœì¢… ë¦¬í¬íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")

    except Exception as e:
        logger.error("Debate í† ë¡  ì‹¤íŒ¨: %s", str(e), exc_info=True)

        # í† ë¡  ì‹¤íŒ¨ ì‹œ Gemini 1ë‹¨ê³„ ê²°ê³¼ë¼ë„ ë°˜í™˜
        yield sse_error_event(
            code="DEBATE_ERROR",
            status=500,
            message=str(e),
            fallback="ì‹¬ì¸µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. Gemini ë‹¨ë… ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
        )

        if gemini_result and gemini_result.questions:
            fallback_text = "\n[Gemini ë‹¨ë… ë¶„ì„ ê²°ê³¼]\n\n"
            fallback_text += _format_analysis_as_text(gemini_result)
            yield f"data: {json.dumps({'chunk': fallback_text}, ensure_ascii=False)}{sse_end}"

    yield f"data: [DONE]{sse_end}"


# ============================================
# í†µí•© ì—”ë“œí¬ì¸íŠ¸
# ============================================


@router.post(
    "/analyze",
    summary="ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±",
    description="""
    ë©´ì ‘ ì¢…ë£Œ ì‹œ Q&A ë°ì´í„°ë¥¼ ë°›ì•„ SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    **retry=false (ê¸°ë³¸):** Gemini ë‹¨ë… ë¶„ì„ â†’ SSE ë¦¬í¬íŠ¸
    **retry=true (ë‹µë³€ ë‹¤ì‹œ ë°›ê¸°):** GeminiÃ—GPT-4o í† ë¡  â†’ SSE ë¦¬í¬íŠ¸
    """,
    responses={
        400: {
            "description": "Bad Request",
            "content": {
                "application/json": {
                    "examples": {
                        "empty_context": {
                            "value": {
                                "detail": {
                                    "code": "EMPTY_CONTEXT",
                                    "message": "í‰ê°€í•  Q&A ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                                }
                            }
                        },
                        "invalid_context": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_CONTEXT",
                                    "message": "contextì˜ ê° í•­ëª©ì— question, answer í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                                }
                            }
                        },
                    }
                }
            },
        },
        422: {
            "description": "Validation Error",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "VALIDATION_ERROR",
                            "message": "í•„ìˆ˜ í•„ë“œ ëˆ„ë½ (room_id, user_id, session_id, context)",
                        }
                    }
                }
            },
        },
        500: {
            "description": "Internal Server Error",
            "content": {
                "application/json": {
                    "examples": {
                        "llm_error": {
                            "value": {
                                "detail": {
                                    "code": "LLM_ERROR",
                                    "message": "LLM ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤.",
                                }
                            }
                        },
                        "debate_error": {
                            "value": {
                                "detail": {
                                    "code": "DEBATE_ERROR",
                                    "message": "ì‹¬ì¸µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                                }
                            }
                        },
                        "stream_error": {
                            "value": {
                                "detail": {
                                    "code": "STREAM_ERROR",
                                    "message": "ìŠ¤íŠ¸ë¦¬ë° ì—°ê²°ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
                                }
                            }
                        },
                    }
                }
            },
        },
        503: {
            "description": "Service Unavailable",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "LLM_UNAVAILABLE",
                            "message": "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        }
                    }
                }
            },
        },
    },
)
async def analyze_interview(
    request: AnalyzeInterviewRequest,
    analyzer: InterviewAnalyzer = Depends(get_evaluation_analyzer),
    debate_service: DebateService | None = Depends(get_debate_service),
):
    """ë©´ì ‘ Q&A ê¸°ë°˜ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    logger.info(
        "Analyze interview: session=%s, user=%s, questions=%d, retry=%s",
        _sanitize_log_value(str(request.session_id)),
        _sanitize_log_value(str(request.user_id)),
        len(request.context),
        request.retry,
    )

    sse_headers = {
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    # retry=true â†’ GeminiÃ—GPT-4o í† ë¡ 
    if request.retry:
        if not debate_service:
            logger.warning("Debate service unavailable, falling back to Gemini-only")
            return StreamingResponse(
                _generate_analyze_stream(request),
                media_type="text/event-stream",
                headers=sse_headers,
            )

        return StreamingResponse(
            _generate_debate_stream(request, analyzer, debate_service),
            media_type="text/event-stream",
            headers=sse_headers,
        )

    # retry=false â†’ Gemini ë‹¨ë… ë¶„ì„
    return StreamingResponse(
        _generate_analyze_stream(request),
        media_type="text/event-stream",
        headers=sse_headers,
    )
