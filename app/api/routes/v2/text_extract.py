"""
v2 í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© API

POST /ai/text/extract - ì´ë ¥ì„œ + ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¶„ì„
"""

import asyncio
import logging
import uuid
from datetime import datetime

from fastapi import APIRouter, Depends, status

from app.api.routes.v2._helpers import format_analysis_text, get_services
from app.config.dependencies import get_legacy_task_storage
from app.prompts import get_extract_title_prompt, get_opening_prompt
from app.schemas.common import AsyncTaskResponse, ErrorCode, TaskStatus
from app.schemas.text_extract import (
    DocumentExtractResult,
    DocumentInput,
    PageText,
    TextExtractRequest,
    TextExtractResult,
)
from app.services.cloudwatch_service import CloudWatchService
from app.utils.log_sanitizer import safe_info, sanitize_log_input

logger = logging.getLogger(__name__)

router = APIRouter()


@router.post(
    "/text/extract",
    response_model=AsyncTaskResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© ì €ì¥ (ì´ë ¥ì„œ + ì±„ìš©ê³µê³ )",
    description="""
    ì´ë ¥ì„œì™€ ì±„ìš©ê³µê³ ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë‚´ë¶€ì—ì„œ ì„ë² ë”©ê¹Œì§€ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    **ìš”ì²­ êµ¬ì¡°:**
    - `resume`: ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤ ì…ë ¥ (í•„ìˆ˜)
    - `job_posting`: ì±„ìš©ê³µê³  ì…ë ¥ (í•„ìˆ˜)
    - ê° ë¬¸ì„œëŠ” íŒŒì¼ ì—…ë¡œë“œ(`s3_key` + `file_type`) ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥(`text`) ì¤‘ í•˜ë‚˜ ì„ íƒ

    **ì²˜ë¦¬ ë°©ì‹:** ë¹„ë™ê¸° (task_id ë°˜í™˜ â†’ í´ë§ í•„ìš”)

    **ë‚´ë¶€ ì²˜ë¦¬ íë¦„:**
    1. ì´ë ¥ì„œ ì²˜ë¦¬: íŒŒì¼ì´ë©´ OCR/VLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ, í…ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    2. ì±„ìš©ê³µê³  ì²˜ë¦¬: íŒŒì¼ì´ë©´ OCR/VLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ, í…ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    3. ê° í…ìŠ¤íŠ¸ ì²­í‚¹ (500 tokens, 50 overlap)
    4. Gemini Embedding ìƒì„±
    5. VectorDBì— ì €ì¥ (resume, job_posting ì»¬ë ‰ì…˜)
    6. ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (ì´ë ¥ì„œ/ì±„ìš©ê³µê³  ë¶„ì„)
    7. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ + ë¶„ì„ ê²°ê³¼ ë°˜í™˜
    """,
    responses={
        400: {
            "description": "Bad Request",
            "content": {
                "application/json": {
                    "examples": {
                        "invalid_request": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_REQUEST",
                                    "message": "resumeê³¼ job_posting ëŠ” í•„ìˆ˜ ì…ë ¥í•´ì•¼í•©ë‹ˆë‹¤",
                                }
                            }
                        },
                        "invalid_file_type": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_FILE_TYPE",
                                    "message": "file_typeì€ pdf ë˜ëŠ” imageë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                                    "field": "resume.file_type",
                                }
                            }
                        },
                        "invalid_document": {
                            "value": {
                                "detail": {
                                    "code": "INVALID_DOCUMENT",
                                    "message": "s3_key ë˜ëŠ” text ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤",
                                    "field": "resume",
                                }
                            }
                        },
                    }
                }
            },
        },
        401: {
            "description": "Unauthorized",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {"code": "UNAUTHORIZED", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤"}
                    }
                }
            },
        },
        404: {
            "description": "Not Found",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "FILE_NOT_FOUND",
                            "message": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: users/12/resume/abc123.pdf",
                        }
                    }
                }
            },
        },
        422: {
            "description": "Unprocessable Entity",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "OCR_FAILED",
                            "message": "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        429: {
            "description": "Too Many Requests",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "RATE_LIMIT_EXCEEDED",
                            "message": "ìš”ì²­ í•œë„ ì´ˆê³¼. 1ë¶„ í›„ ì¬ì‹œë„í•˜ì„¸ìš”",
                        }
                    }
                }
            },
        },
        500: {
            "description": "Internal Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "detail": {
                            "code": "INTERNAL_ERROR",
                            "message": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                        }
                    }
                }
            },
        },
        503: {
            "description": "Service Unavailable",
            "content": {
                "application/json": {
                    "examples": {
                        "llm_unavailable": {
                            "value": {
                                "detail": {
                                    "code": "LLM_UNAVAILABLE",
                                    "message": "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                                }
                            }
                        },
                        "s3_unavailable": {
                            "value": {
                                "detail": {
                                    "code": "S3_UNAVAILABLE",
                                    "message": "íŒŒì¼ ìŠ¤í† ë¦¬ì§€ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                                }
                            }
                        },
                    }
                }
            },
        },
    },
)
async def text_extract(
    request: TextExtractRequest,
    task_storage=Depends(get_legacy_task_storage),
):
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì„ë² ë”© ì €ì¥ (í†µí•©) - ì´ë ¥ì„œ + ì±„ìš©ê³µê³ """
    task_id = request.task_id

    # ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ì „ì†¡
    try:
        cw = CloudWatchService.get_instance()
        asyncio.create_task(cw.put_metric("AI_Job_Count", 1, "Count", {"Type": "text_extract"}))
    except Exception:
        pass

    # ë¹„ë™ê¸° ì‘ì—… ì‹œì‘
    task_key = str(task_id)
    task_storage.save(
        task_key,
        {
            "type": "text_extract",
            "status": TaskStatus.PROCESSING,
            "created_at": datetime.now(),
            "room_id": request.room_id,
            "request": request.model_dump(),
        },
    )

    async def process_text_extract(store):
        try:
            rag = get_services()

            model = request.model if hasattr(request, "model") and request.model else "gemini"
            logger.info("")
            logger.info(f"{'='*80}")
            logger.info("=== ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘ (ì´ë ¥ì„œ + ì±„ìš©ê³µê³ ) ===")
            logger.info(f"{'='*80}")
            logger.info(
                f"ğŸ“Œ OCR ì „ëµ: {'GEMINI (V1 Temporary)' if model == 'auto' else model.upper()}"
            )
            safe_info(logger, "ğŸ“Œ ì‚¬ìš©ì ID: %s", request.user_id)
            logger.info(f"ğŸ“Œ vLLM ì„œë¹„ìŠ¤: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if rag.vllm else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
            logger.info("")

            async def extract_document(
                doc_input: DocumentInput, doc_type: str
            ) -> DocumentExtractResult:
                """ë¬¸ì„œ ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜"""
                logger.info(f"ğŸ“„ [{doc_type.upper()}] ì²˜ë¦¬ ì‹œì‘")

                if doc_input.s3_key:
                    file_type = doc_input.get_file_type_simple() or "pdf"
                    logger.info(f"   â†’ íŒŒì¼ íƒ€ì… (MIME): {doc_input.file_type}")
                    logger.info(f"   â†’ íŒŒì¼ íƒ€ì… (ë‹¨ìˆœ): {file_type}")
                    safe_s3_key = sanitize_log_input(doc_input.s3_key)
                    logger.info("   â†’ S3 í‚¤: %s", safe_s3_key)

                    logger.info("   ğŸ” [OCRService] CLOVA OCR ìš°ì„  â†’ Gemini Fallback ì‹œì‘")
                    ocr_result = await rag.ocr.extract_text(
                        file_url=str(doc_input.s3_key),
                        file_type=file_type,
                        user_id=str(request.user_id),
                        fallback_enabled=True,
                    )
                    ocr_engine = ocr_result.get("ocr_engine") or "gemini"
                    fallback_reason = ocr_result.get("fallback_reason")
                    extracted_text = ocr_result.get("extracted_text", "")
                    pages = [PageText(**page) for page in ocr_result.get("pages", [])]

                    if fallback_reason:
                        logger.info(
                            f"   âœ… [{ocr_engine.upper()} OCR] ì¶”ì¶œ ì™„ë£Œ (í´ë°± ì‚¬ìœ : {fallback_reason}): "
                            f"{len(extracted_text)}ì (í˜ì´ì§€: {len(pages)})"
                        )
                    else:
                        logger.info(
                            f"   âœ… [{ocr_engine.upper()} OCR] ì¶”ì¶œ ì™„ë£Œ: "
                            f"{len(extracted_text)}ì (í˜ì´ì§€: {len(pages)})"
                        )
                else:
                    extracted_text = doc_input.text or ""
                    pages = None
                    logger.info(f"   â†’ í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥: {len(extracted_text)} characters")

                if extracted_text:
                    document_id = f"{doc_type}_{uuid.uuid4().hex[:12]}"
                    await rag.vectordb.add_document(
                        document_id=document_id,
                        text=extracted_text,
                        collection_type=doc_type,
                        metadata={
                            "user_id": request.user_id,
                            "file_id": doc_input.file_id,
                            "created_at": datetime.now().isoformat(),
                        },
                    )
                    safe_document_id = sanitize_log_input(document_id)
                    logger.info("   âœ… VectorDB ì €ì¥ ì™„ë£Œ: %s", safe_document_id)

                return DocumentExtractResult(
                    file_id=doc_input.file_id, extracted_text=extracted_text, pages=pages
                )

            resume_result = await extract_document(request.resume, "resume")
            job_posting_result = await extract_document(request.job_posting, "job_posting")

            # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
            logger.info("")
            logger.info("ğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
            analysis_failed = False
            try:
                analysis_result = await rag.llm.generate_analysis(
                    resume_text=resume_result.extracted_text,
                    posting_text=job_posting_result.extracted_text,
                    user_id=str(request.user_id),
                )
                logger.info("âœ… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                analysis_failed = True
                logger.warning(
                    "âš ï¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ (ì˜¤í”„ë‹ ë©”ì‹œì§€ì— ë¶„ì„ ë‚´ìš©ì´ ë¹„ì–´ ë³´ì¼ ìˆ˜ ìˆìŒ): %s",
                    e,
                    exc_info=True,
                )
                analysis_result = {
                    "resume_analysis": {"strengths": [], "weaknesses": [], "suggestions": []},
                    "posting_analysis": {
                        "company": "ì•Œ ìˆ˜ ì—†ìŒ",
                        "position": "ì•Œ ìˆ˜ ì—†ìŒ",
                        "required_skills": [],
                        "preferred_skills": [],
                    },
                    "matching": {
                        "score": 0,
                        "grade": "F",
                        "matched_skills": [],
                        "missing_skills": [],
                    },
                }

            # ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ
            chat_title = ""
            try:
                logger.info("ğŸ“ ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì¤‘...")
                posting_text = job_posting_result.extracted_text[:1000]
                title_prompt = f"""{get_extract_title_prompt()}

## ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸
{posting_text}
"""
                title_response = ""
                async for chunk in rag.llm.generate_response(
                    user_message=title_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ì±„ìš©ê³µê³ ì—ì„œ íšŒì‚¬ëª…ê³¼ ì§ë¬´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤.",
                ):
                    title_response += chunk

                chat_title = title_response.strip()
                logger.info(f"âœ… ì±„íŒ…ë°© ì œëª©: {chat_title}")
            except Exception as e:
                logger.error(f"âŒ ì±„íŒ…ë°© ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            logger.info("")

            task_data = store.get(task_key) or {}
            task_data["status"] = TaskStatus.COMPLETED

            formatted_text = format_analysis_text(
                resume_analysis=analysis_result.get("resume_analysis"),
                posting_analysis=analysis_result.get("posting_analysis"),
                summary=chat_title,
            )
            formatted_text = formatted_text or "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            if analysis_failed:
                formatted_text += (
                    "\n\n(ìƒì„¸ ë¶„ì„ì´ ì¼ì‹œì ìœ¼ë¡œ ë°˜ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                    "ì´ë ¥ì„œÂ·ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸ëŠ” ì €ì¥ë˜ì—ˆìœ¼ë‹ˆ, ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.)"
                )

            # ì˜¤í”„ë‹ ë©”ì‹œì§€ ìƒì„±
            logger.info("ğŸ¤– ì˜¤í”„ë‹ ë©”ì‹œì§€ ìƒì„± ì‹œì‘...")
            ai_message = ""
            try:
                opening_prompt = get_opening_prompt(formatted_text)
                async for chunk in rag.llm.generate_response(
                    user_message=opening_prompt,
                    context=None,
                    history=[],
                    system_prompt="ë‹¹ì‹ ì€ ë„ì›€ì„ ì£¼ëŠ” ì¹œì ˆí•œ ì·¨ì—… ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                ):
                    ai_message += chunk
                logger.info("âœ… ì˜¤í”„ë‹ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ì˜¤í”„ë‹ ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨: {e}")
                ai_message = f"ì•ˆë…•í•˜ì„¸ìš”! ì§€ì›í•˜ì‹  {chat_title or 'ì§ë¬´'}ì— ëŒ€í•œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ í™•ì¸í•˜ì‹œê³  ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“  ë¬¼ì–´ë´ì£¼ì„¸ìš”!"

            task_data["result"] = TextExtractResult(
                success=True,
                summary=chat_title or None,
                resume_ocr=resume_result.extracted_text,
                job_posting_ocr=job_posting_result.extracted_text,
                resume_analysis=analysis_result.get("resume_analysis"),
                posting_analysis=analysis_result.get("posting_analysis"),
                formatted_text=formatted_text,
                ai_message=ai_message,
            ).model_dump()
            store.save(task_key, task_data)

            logger.info("")
            logger.info("âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë¶„ì„ ì™„ë£Œ!")
            logger.info(f"   â†’ ì´ë ¥ì„œ OCR: {len(resume_result.extracted_text)}ì")
            logger.info(f"   â†’ ì±„ìš©ê³µê³  OCR: {len(job_posting_result.extracted_text)}ì")

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            task_data = store.get(task_key) or {}
            task_data["status"] = TaskStatus.FAILED
            task_data["error"] = {"code": ErrorCode.PROCESSING_ERROR, "message": str(e)}
            store.save(task_key, task_data)

    asyncio.create_task(process_text_extract(task_storage))

    return AsyncTaskResponse(task_id=task_id, status=TaskStatus.PROCESSING)
